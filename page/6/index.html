<!DOCTYPE html>
<html lang="zh-CH">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"بحث...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="记录点滴成长">
<meta property="og:type" content="website">
<meta property="og:title" content="Think World">
<meta property="og:url" content="http://example.com/page/6/index.html">
<meta property="og:site_name" content="Think World">
<meta property="og:description" content="记录点滴成长">
<meta property="og:locale" content="zh_CH">
<meta property="article:author" content="Sherry Wang">
<meta property="article:tag" content="computer vision, DeepLearning ,MachineLearning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CH","comments":"","permalink":"","path":"page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Think World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="تشغيل شريط التصفح" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Think World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">从个人成长角度来说，从经历中学点什么总是重要的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          المحتويات
        </li>
        <li class="sidebar-nav-overview">
          عام
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sherry Wang"
      src="/images/aa.webp">
  <p class="site-author-name" itemprop="name">Sherry Wang</p>
  <div class="site-description" itemprop="description">记录点滴成长</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">المقالات</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">التصنيفات</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">الوسوم</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/25/%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/25/%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/" class="post-title-link" itemprop="url">合并与分割</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-08-25 00:06:15" itemprop="dateCreated datePublished" datetime="2020-08-25T00:06:15+08:00">2020-08-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2021-07-25 09:17:08" itemprop="dateModified" datetime="2021-07-25T09:17:08+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="合并与分割"><a href="#合并与分割" class="headerlink" title="合并与分割"></a>合并与分割</h2><h3 id="1-cat"><a href="#1-cat" class="headerlink" title="1. cat"></a>1. cat</h3><p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_06-36.png" alt="2020-08-25_06-36"></p>
<p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_06-38.png" alt="2020-08-25_06-38"></p>
<p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_06-38_1.png" alt="2020-08-25_06-38_1"></p>
<h3 id="2-stack"><a href="#2-stack" class="headerlink" title="2. stack"></a>2. stack</h3><p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_06-35.png" alt="2020-08-25_06-35"></p>
<h3 id="cat-v-s-stack"><a href="#cat-v-s-stack" class="headerlink" title="cat v.s. stack"></a>cat v.s. stack</h3><p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_07-04.png" alt="2020-08-25_07-04"></p>
<h3 id="3-split"><a href="#3-split" class="headerlink" title="3. split"></a>3. split</h3><h4 id="by-sub-len"><a href="#by-sub-len" class="headerlink" title="by sub-len"></a>by sub-len</h4><p>​    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(sub_size ,dim =dd)	<span class="comment">#拆分成若干个size都为sub_size的tensor</span></span><br><span class="line"></span><br><span class="line">torch.chunk([sub.size1,sub_size2.....],dim=d) <span class="comment">#拆分成各分布为size1,size2...的tensor</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="4-chunk"><a href="#4-chunk" class="headerlink" title="4. chunk"></a>4. chunk</h3><h4 id="by-sub-num"><a href="#by-sub-num" class="headerlink" title="by sub-num"></a>by sub-num</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(sub_num ,dim =dd)</span><br></pre></td></tr></table></figure>



<p>汇总代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">a1 = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">a2 = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">b = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">b2= torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">b3 = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c = torch.stack([a,b],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([a,b],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([a1,d],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([a2,d],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([b2,d],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([b3,d],dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(d.shape)</span><br><span class="line"></span><br><span class="line">n,m = d.split([<span class="number">2</span>*<span class="number">32</span>,<span class="number">4</span>*<span class="number">32</span>],dim=<span class="number">0</span>) <span class="comment">#指定每个sub-tensor的size,使用list表示，所有size的和必须等于拆分前的tensor的size</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(n.shape)</span><br><span class="line"><span class="built_in">print</span>(m.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nn,mm = d.split(<span class="number">3</span>*<span class="number">32</span>,dim=<span class="number">0</span>) <span class="comment">#如果所有长度都固定，就用一个数来表示每个sub-tensor的长度，sub-tensor的个数可以自动计算得到</span></span><br><span class="line"><span class="comment"># nn,mm = d.split(3,dim=0) #报错，如果每个sub-tensor第一维的size＝３，会返回64个sub-tensor</span></span><br><span class="line"><span class="built_in">print</span>(nn.shape)</span><br><span class="line"><span class="built_in">print</span>(mm.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nnn ,mmm = d.chunk(<span class="number">2</span>,dim=<span class="number">0</span>)<span class="comment">#拆分成两个tensor,每个tensor的长度相同为３*32</span></span><br><span class="line"><span class="built_in">print</span>(nnn.shape)</span><br><span class="line"><span class="built_in">print</span>(mmm.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/24/Broadcasting/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/24/Broadcasting/" class="post-title-link" itemprop="url">Broadcasting</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>
      

      <time title="أُنشأ: 2020-08-24 23:43:31 / عُدل: 16:05:46" itemprop="dateCreated datePublished" datetime="2020-08-24T23:43:31+08:00">2020-08-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ، 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="1-Broadcasting的基本准则"><a href="#1-Broadcasting的基本准则" class="headerlink" title="1.Broadcasting的基本准则"></a>1.Broadcasting的基本准则</h3><p><img src="Broadcasting/2020-08-25_00-03.png" alt="2020-08-25_00-03"></p>
<p>例子</p>
<p><img src="Broadcasting/2020-08-24_23-52.png" alt="2020-08-24_23-52"></p>
<h3 id="2-Broadcasting引入的背景"><a href="#2-Broadcasting引入的背景" class="headerlink" title="2.Broadcasting引入的背景"></a>2.Broadcasting引入的背景</h3><pre><code>1. 实际计算需求：可以允许不同shape(但又满足某种标准)的tensor可以进行计算
 2. 同时这种原则满足数学上的运算，又无需手动操作，又不会消耗很多的内存（对比repeat)
</code></pre>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/24/%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/24/%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/" class="post-title-link" itemprop="url">维度转换</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-08-24 16:42:48" itemprop="dateCreated datePublished" datetime="2020-08-24T16:42:48+08:00">2020-08-24</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2021-07-25 09:09:49" itemprop="dateModified" datetime="2021-07-25T09:09:49+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p><strong>数据没有被改变，改变的只有数据的理解方式</strong></p>
<h3 id="1-view-reshape"><a href="#1-view-reshape" class="headerlink" title="1. view / reshape"></a>1. view / reshape</h3><p>1.view reshape ,两个方法的用法完全相同<br>要求转换前后tensor的numel()相同即可，prod(a.size()) ==prod(b.size())</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="comment">#把每张照片打成一维数据,这个常做为－卷积网络的输入</span></span><br><span class="line">b = a.view(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="comment">#把只是关注照片中每行的特点</span></span><br><span class="line">c = a.view(<span class="number">4</span>*<span class="number">1</span>*<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#不关心像素点是来自哪里，只是关注图片中不同像素点的不同</span></span><br><span class="line">d = a.view(<span class="number">4</span>*<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(d.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据的存储／维度顺序非常重要，但是这里view 和reshape函数在转换时把这一信息丢掉了，无额外记录之前的形状，无法恢复</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([4, 1, 28, 28])</span></span><br><span class="line"><span class="string">torch.Size([4, 784])</span></span><br><span class="line"><span class="string">torch.Size([112, 28])</span></span><br><span class="line"><span class="string">torch.Size([4, 28, 28])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="2-squeeze-v-s-unsqueeze-维度"><a href="#2-squeeze-v-s-unsqueeze-维度" class="headerlink" title="2.squeeze v.s. unsqueeze-维度"></a>2.squeeze v.s. unsqueeze-维度</h3><h4 id="2-1-unsqueeze"><a href="#2-1-unsqueeze" class="headerlink" title="2.1 unsqueeze()"></a>2.1 unsqueeze()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#unsqueeze( input , dim ) -&gt;Tensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#####参数说明：</span></span><br><span class="line"><span class="comment">#input (Tensor) – the input tensor.</span></span><br><span class="line"><span class="comment"># dim (int) – the index at which to insert the singleton dimension,dim的取值范围[- input.dim() -1 , input.dim()+1 ]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果dim为负数，则　dim = dim + input.dim()+1 将其转换成正数</span></span><br><span class="line"><span class="comment">#如果dim为整数，则　新增的维度添加到　dim的前面</span></span><br><span class="line"><span class="comment">#维度上的元素个数只有一个，所以数据规模没有改变，改变的只是数据的含义</span></span><br></pre></td></tr></table></figure>

<p><strong>图示</strong></p>
<p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/1.jpg" alt="1"></p>
<p>可以看出来维度变换之后，数据的理解方式不同，要好好体会</p>
<p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_19-08.png" alt="2020-08-24_19-08"></p>
<p><strong>数据处理的实例应用</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">b = torch.rand(<span class="number">32</span>)</span><br><span class="line">f = torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">14</span>,<span class="number">14</span>)</span><br><span class="line">b = b.unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="comment">#方便之后 f+b的计算。b 即bias，相当于给每个channel上的所有像素增加一个偏置</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output : torch.Size([1, 32, 1, 1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">##常用的：　就是如果向后面插就使用　unsqueeze(-1)多次插入，就多次写</span></span><br><span class="line"><span class="comment">#向开头插入就，直接调用 unsqueeze(0)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="2-2-squeeze"><a href="#2-2-squeeze" class="headerlink" title="2.2 squeeze"></a>2.2 squeeze</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#torch.squeeze(input , dim =None ,Out = None) -&gt;tensor</span></span><br><span class="line"><span class="comment">#当不传参数时，会将input所有元素只有一个的维度给去掉</span></span><br><span class="line"><span class="comment">#传参数dim，在指定维度上且维度只有一个元素时，将挤压掉该维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#b.shape = torch.Size([1,32,1,1])</span></span><br><span class="line"><span class="built_in">print</span>(b.squeeze(-<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(b.squeeze(<span class="number">1</span>).shape) <span class="comment">#无效</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([1, 32, 1])</span></span><br><span class="line"><span class="string">torch.Size([1, 32, 1, 1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-Expand-repeat-行"><a href="#3-Expand-repeat-行" class="headerlink" title="3.Expand /repeat-行"></a>3.Expand /repeat-行</h3><p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_19-29.png" alt="2020-08-24_19-29"></p>
<h4 id="3-1-expand"><a href="#3-1-expand" class="headerlink" title="3.1 expand"></a>3.1 expand</h4><p>参数是广播的目标tensor的shape</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">14</span>,<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">b = torch.rand(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">b = b.unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">b1 = b.expand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">14</span>,<span class="number">14</span>)<span class="comment">#之后b2即可以与a进行运算</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b1.shape)</span><br><span class="line"><span class="comment"># b2 = b.expand(4,33,14,14) 会报错 The expanded size of the tensor (33) must match the existing size (32) at non-singleton dimension 1</span></span><br><span class="line"></span><br><span class="line">b2 = b.expand(-<span class="number">1</span>,-<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) <span class="comment">#-1表示不想对该维度进行修改</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b2.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br></pre></td></tr></table></figure>

<h4 id="3-2-repeat－－不建议使用"><a href="#3-2-repeat－－不建议使用" class="headerlink" title="3.2 repeat－－不建议使用"></a>3.2 repeat－－不建议使用</h4><p>进行repeat之后，可能会开辟新的空间去保存repeat的结果，会降低效率</p>
<p><em>参数传递是每个维度上要重复的次数，需要自己计算</em></p>
<p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_19-43.png" alt="2020-08-24_19-43"></p>
<h3 id="4-转置操作-t（只能适用于2D操作）"><a href="#4-转置操作-t（只能适用于2D操作）" class="headerlink" title="4.转置操作　.t（只能适用于2D操作）"></a>4.转置操作　.t（只能适用于2D操作）</h3><p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_19-44.png" alt="2020-08-24_19-44"></p>
<h3 id="5-Transpose"><a href="#5-Transpose" class="headerlink" title="5.Transpose()"></a>5.Transpose()</h3><p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_20-18.png" alt="2020-08-24_20-18"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转换效果图</span></span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = torch.transpose(a,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">c = torch.transpose(a,<span class="number">1</span>,<span class="number">2</span>).contiguous().view(<span class="number">2</span>,<span class="number">2</span>*<span class="number">3</span>).view(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">d = torch.transpose(a,<span class="number">1</span>,<span class="number">2</span>).contiguous().view(<span class="number">2</span>,<span class="number">2</span>*<span class="number">3</span>).view(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[-0.2507, -0.4298],</span></span><br><span class="line"><span class="string">         [-2.8421,  0.9166],</span></span><br><span class="line"><span class="string">         [ 3.2584,  1.1366]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.8887, -0.5606],</span></span><br><span class="line"><span class="string">         [ 1.3798, -0.5037],</span></span><br><span class="line"><span class="string">         [ 0.9862,  0.8550]]])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([[[-0.2507, -2.8421,  3.2584],</span></span><br><span class="line"><span class="string">         [-0.4298,  0.9166,  1.1366]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.8887,  1.3798,  0.9862],</span></span><br><span class="line"><span class="string">         [-0.5606, -0.5037,  0.8550]]])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([[[-0.2507, -2.8421],</span></span><br><span class="line"><span class="string">         [ 3.2584, -0.4298],</span></span><br><span class="line"><span class="string">         [ 0.9166,  1.1366]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.8887,  1.3798],</span></span><br><span class="line"><span class="string">         [ 0.9862, -0.5606],</span></span><br><span class="line"><span class="string">         [-0.5037,  0.8550]]])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([[[-0.2507, -0.4298],</span></span><br><span class="line"><span class="string">         [-2.8421,  0.9166],</span></span><br><span class="line"><span class="string">         [ 3.2584,  1.1366]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.8887, -0.5606],</span></span><br><span class="line"><span class="string">         [ 1.3798, -0.5037],</span></span><br><span class="line"><span class="string">         [ 0.9862,  0.8550]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="6-permute"><a href="#6-permute" class="headerlink" title="6.permute"></a>6.permute</h3><p>permute也会打乱内存的顺序，需要调用coutigious函数</p>
<p>permute底层是调用多次transpose()实现的</p>
<p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_20-24.png" alt="2020-08-24_20-24"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上面遮挡住的命令是</span></span><br><span class="line">b.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).shape</span><br><span class="line"><span class="comment">#out : torch.Size([4,28,32,3])</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/24/pytroch%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/24/pytroch%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87/" class="post-title-link" itemprop="url">pytroch索引与切片</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-08-24 07:33:55" itemprop="dateCreated datePublished" datetime="2020-08-24T07:33:55+08:00">2020-08-24</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2021-07-25 08:51:19" itemprop="dateModified" datetime="2021-07-25T08:51:19+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="1-pytorch风格的索引－维度选择"><a href="#1-pytorch风格的索引－维度选择" class="headerlink" title="1.pytorch风格的索引－维度选择"></a>1.pytorch风格的索引－维度选择</h3><p>跟据Tensor的shape,从前往后索引，依次在每个维度上进行索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>])</span><br><span class="line"><span class="built_in">print</span>( a[<span class="number">0</span>].shape)<span class="comment">#和Ｃ＋＋等高级问题类似，a[0]表示选择第一章图片</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>,<span class="number">0</span>].shape)<span class="comment">#选择第一章图片的第一个通道</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>])<span class="comment">#选择某个像素点，</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="string">torch.Size([28, 28])</span></span><br><span class="line"><span class="string">tensor(0.1076) #是维度为０的元素</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="2-python风格的索引-特定维度上"><a href="#2-python风格的索引-特定维度上" class="headerlink" title="2.python风格的索引-特定维度上"></a>2.python风格的索引-特定维度上</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">基本结构：</span></span><br><span class="line"><span class="string">eg : a[0,0,0:28:2,0]</span></span><br><span class="line"><span class="string">#0:28:2－－在某个维度上，start: end[:offset]  其中start:end:offset 表示从start开始，到end结束，其中不包括end，每次步长为offset;当省略offset时，表示offset=1</span></span><br><span class="line"><span class="string"># 0:28其实等价与 0:28:1这种结构</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">-1:表示倒数第１个元素，-2表示倒数第２个元素</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 譬如：4张图片，每张三个通道，每个通道28行28列的像素</span></span><br><span class="line">a = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在第一个维度上取后0和1，等同于取第一、第二张图片</span></span><br><span class="line"><span class="built_in">print</span>(a[:<span class="number">2</span>].shape)  </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取0，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第一个通道</span></span><br><span class="line"><span class="built_in">print</span>(a[:<span class="number">2</span>, :<span class="number">1</span>, :, :].shape)  </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取1,2，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第二个通道与第三个通道</span></span><br><span class="line"><span class="built_in">print</span>(a[:<span class="number">2</span>, <span class="number">1</span>:, :, :].shape) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取1,2，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第二个通道与第三个通道</span></span><br><span class="line"><span class="built_in">print</span>(a[:<span class="number">2</span>, -<span class="number">2</span>:, :, :].shape)  </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 使用step隔行采样</span></span><br><span class="line"><span class="comment"># 在第一、第二维度取所有元素，在第三、第四维度步长为２采样</span></span><br><span class="line"><span class="comment"># 等同于所有图片所有通道的行列每个一行或者一列采样</span></span><br><span class="line"><span class="comment"># 注意：下面的代码不包括28</span></span><br><span class="line"><span class="built_in">print</span>(a[:, :, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>].shape) </span><br><span class="line"><span class="built_in">print</span>(a[:, :, ::<span class="number">2</span>, ::<span class="number">2</span>].shape)  <span class="comment"># 等同于上面语句</span></span><br></pre></td></tr></table></figure>



<h3 id="3-选择特定的元素"><a href="#3-选择特定的元素" class="headerlink" title="3. 选择特定的元素"></a>3. 选择特定的元素</h3><h4 id="3-1-index-select"><a href="#3-1-index-select" class="headerlink" title="3.1 index_select"></a>3.1 index_select</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.index_select(input, dim, index, out=None) → Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">input (Tensor) – the input tensor.</span></span><br><span class="line"><span class="string">dim (int) – the dimension in which we index</span></span><br><span class="line"><span class="string">index (LongTensor) – the 1-D tensor containing the indices to index；是list转换成的一维tensor</span></span><br><span class="line"><span class="string">out (Tensor, optional) – the output tensor.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>((a.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">2</span>]))).shape)</span><br><span class="line"><span class="comment">#选取第一个维度上的　索引为0,2的tensor</span></span><br><span class="line"><span class="comment">#第二个参数是将list [0,2]转换成 tensor</span></span><br><span class="line"><span class="built_in">print</span>((a.index_select(<span class="number">2</span>,torch.arange(<span class="number">8</span>))).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#选择第二维度上的前８个元素</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([4, 3, 28, 28])</span></span><br><span class="line"><span class="string">torch.Size([2, 3, 28, 28])</span></span><br><span class="line"><span class="string">torch.Size([4, 3, 8, 28])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>索引效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>((a.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">2</span>]))))</span><br><span class="line"><span class="built_in">print</span>((a.index_select(<span class="number">1</span>,torch.arange(<span class="number">2</span>))))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.9071, 0.5350, 0.4018],</span></span><br><span class="line"><span class="string">        [0.9565, 0.9739, 0.7234],</span></span><br><span class="line"><span class="string">        [0.1984, 0.3562, 0.8078]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0.9071, 0.5350, 0.4018],</span></span><br><span class="line"><span class="string">        [0.1984, 0.3562, 0.8078]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0.9071, 0.5350],</span></span><br><span class="line"><span class="string">        [0.9565, 0.9739],</span></span><br><span class="line"><span class="string">        [0.1984, 0.3562]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-2-masked-select"><a href="#3-2-masked-select" class="headerlink" title="3.2 masked_select()"></a>3.2 masked_select()</h4><h6 id="方式一"><a href="#方式一" class="headerlink" title="方式一"></a>方式一</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">函数说明</span></span><br><span class="line"><span class="string">torch.masked_select( input, mask ,out = None ) -&gt; 张量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据掩码张量mask中的二元值(0,1)，取输入张量中的指定项( mask为一个 ByteTensor)，将取值返回到一个新的1D张量－－－是打平的张量; 张量 mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。</span></span><br><span class="line"><span class="string">注意： 返回的张量不与原始张量共享内存空间。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">mask2 = a.ge(<span class="number">0.3</span>)</span><br><span class="line">mask3 = a.le(<span class="number">0.7</span>)</span><br><span class="line"><span class="built_in">print</span>(mask2)</span><br><span class="line"><span class="built_in">print</span>(a[mask2])</span><br><span class="line"><span class="built_in">print</span>(a[mask3])</span><br><span class="line"><span class="built_in">print</span>(torch.masked_select(a,mask2))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[-0.7769, -0.0803, -0.4235, -0.3562],</span></span><br><span class="line"><span class="string">        [-0.4744,  1.2078,  0.6371, -0.6981],</span></span><br><span class="line"><span class="string">        [-1.1653, -0.3432, -2.3189,  0.1708]])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([[ True, False,  True,  True],</span></span><br><span class="line"><span class="string">        [False, False, False, False],</span></span><br><span class="line"><span class="string">        [ True, False, False, False]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([1.2078, 0.6371])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([-0.7769, -0.0803, -0.4235, -0.3562, -0.4744,  0.6371, -0.6981, -1.1653,</span></span><br><span class="line"><span class="string">        -0.3432, -2.3189,  0.1708])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([1.2078, 0.6371])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h6 id="方式二-不使用a-ge-a-le-方法"><a href="#方式二-不使用a-ge-a-le-方法" class="headerlink" title="方式二　不使用a.ge() \ a.le()方法"></a>方式二　不使用a.ge() \ a.le()方法</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">mask1 = torch.ByteTensor((a&gt;<span class="number">0.5</span>).byte())　</span><br><span class="line"><span class="built_in">print</span>(mask1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[mask1])</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">输出：</span></span><br><span class="line"><span class="string">tensor([[-1.8973, -0.2158,  1.0196, -0.2119],</span></span><br><span class="line"><span class="string">        [-0.2365,  0.4743, -1.2473, -0.6554],</span></span><br><span class="line"><span class="string">        [ 0.3040, -0.0906, -0.8517, -0.2679]])</span></span><br><span class="line"><span class="string">tensor([[0, 0, 1, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0]], dtype=torch.uint8)</span></span><br><span class="line"><span class="string">tensor([1.0196])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">提醒一下：不是特别建议使用这种方式</span></span><br><span class="line"><span class="string"> UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"> 感觉自己对python和pytorch基本类型的转换有些模糊　－－－torch.bool　与　torch.uint8</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-2-torch-take"><a href="#3-2-torch-take" class="headerlink" title="3.2 torch.take()"></a>3.2 torch.take()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">使用打平的index进行索引</span></span><br><span class="line"><span class="string">torch.take(input, index) -&gt;Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">index(Long Tensor) 把input Tensor看作一维Tensor对每个元素的索引</span></span><br><span class="line"><span class="string">输出:一个一维Tensor</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = torch.take(a, torch.tensor([<span class="number">1</span>,<span class="number">5</span>,<span class="number">7</span>]))</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">输出：</span></span><br><span class="line"><span class="string">tensor([[-0.3353,  1.2220,  0.7055,  0.4678],</span></span><br><span class="line"><span class="string">        [-0.4759,  0.5173,  1.1912, -0.8545],</span></span><br><span class="line"><span class="string">        [-1.2037,  0.5052,  0.0388, -0.3160]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([ 1.2220,  0.5173, -0.8545])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/" class="post-title-link" itemprop="url">分类问题的引入</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-08-23 15:54:03" itemprop="dateCreated datePublished" datetime="2020-08-23T15:54:03+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2020-08-24 08:45:00" itemprop="dateModified" datetime="2020-08-24T08:45:00+08:00">2020-08-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ، 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h2><p><strong>MNIST　数据集</strong></p>
<p>由不同风格的手写数字组成(0-9)</p>
<p>每个数字都有7000张，每张图片都是28*28的灰度图片</p>
<p>训练时：将训练集和测试集分为60k Vs 10k</p>
<h3 id="1-No-deeping-learning"><a href="#1-No-deeping-learning" class="headerlink" title="1. No deeping learning"></a>1. No deeping learning</h3><p>X:[1….28*28]</p>
<p>每个点都是０－１，表示该像素点的灰度值</p>
<p>关键点：参数的维度定义，以及每层转换的含义</p>
<p><img src="%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/2020-08-22_16-28.png" alt="2020-08-22_16-28"></p>
<p><img src="%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/2020-08-22_16-55.png" alt="2020-08-22_16-55"></p>
<p><img src="%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/2020-08-22_16-57.png" alt="2020-08-22_16-57"></p>
<p>2.代码实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-encodng: utf-8-*-</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">@File: minist.py.py</span></span><br><span class="line"><span class="string">@Contact: 2257925767@qq.com</span></span><br><span class="line"><span class="string">@Author:wangyu</span></span><br><span class="line"><span class="string">@Version:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@Desciption:</span></span><br><span class="line"><span class="string">        手写数字识别的核心代码</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        还存在一些问题没有解决－－－自己得到的数据比老师的代码迭代的次数要少很多</span></span><br><span class="line"><span class="string">    env: </span></span><br><span class="line"><span class="string">        pytorch 1.3.1</span></span><br><span class="line"><span class="string">@DateTime: 2020/8/22下午5:04 </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F <span class="comment">#常见的激活函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> plot_image, plot_curve, one_hot</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 . load dataset,采用向量并行</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">512</span></span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    torchvision.datasets.MNIST(<span class="string">&quot;mnist_data&quot;</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>,</span><br><span class="line">                               transform= torchvision.transforms.Compose([torchvision.transforms.ToTensor(),<span class="comment">#将numby格式数据转成pytorch</span></span><br><span class="line">                                                                            torchvision.transforms.Normalize(</span><br><span class="line">                                                                                (<span class="number">0.1307</span>,),(<span class="number">0.3081</span>,)), <span class="comment">#对像素点的灰度值进行正则化，会提高优化效率</span></span><br><span class="line">                                                                          ])),batch_size = batch_size,shuffle = <span class="literal">True</span>)<span class="comment">#随机打散</span></span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    torchvision.datasets.MNIST(<span class="string">&quot;mnist_data/&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,</span><br><span class="line">                               transform= torchvision.transforms.Compose([torchvision.transforms.ToTensor(),<span class="comment">#将numby格式数据转成pytorch</span></span><br><span class="line">                                                                            torchvision.transforms.Normalize(</span><br><span class="line">                                                                                (<span class="number">0.1307</span>,),(<span class="number">0.3081</span>,)), <span class="comment">#对像素点的灰度值进行正则化，会提高优化效率</span></span><br><span class="line">                                                                          ])),batch_size = batch_size,shuffle = <span class="literal">False</span>)<span class="comment">#随机打散</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x,y = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"><span class="built_in">print</span>(x.shape,y.shape,x.<span class="built_in">min</span>(),x.<span class="built_in">max</span>())</span><br><span class="line">plot_image(x,y,<span class="string">&#x27;image_test&#x27;</span>) <span class="comment">#检查数据集</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span><span class="comment">#初始化函数:搭建网络结构</span></span><br><span class="line">        <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#wx+b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">256</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>,<span class="number">64</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">64</span>,<span class="number">10</span>)<span class="comment">#28*28 =&gt; 256 =&gt;64这个是随机确定的,最后一层的输出是由分类的种类数决定的</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span><span class="comment">#网络的计算过程</span></span><br><span class="line">        <span class="comment"># x :[batch_size,1,28,28] 1:表示只有一个通道</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#h1= relu(xw1+b1)</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="comment">#h1=relu(h1w2+b2)</span></span><br><span class="line">        x=F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment">#h3=h2w3+b3 --这里没有使用激活函数，只是简单输出</span></span><br><span class="line">        x=self.fc3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()<span class="comment">#实例化一个net</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义优化器</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(),lr=<span class="number">0.01</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">train_loss=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_idx , (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader): <span class="comment">#迭代一次数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.调整数据的尺寸，构建网络，通过网络计算预测值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#x: [b,1,28,28], y:[512]</span></span><br><span class="line">        <span class="comment"># [b,1,28,28] =&gt; [b,784]</span></span><br><span class="line">        x =x.view(x.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>) <span class="comment">#size[0]表示batch_size</span></span><br><span class="line">        <span class="comment">#=&gt;[b,10]</span></span><br><span class="line">        out = net(x) <span class="comment">#经过网络计算出来的值</span></span><br><span class="line">        y_onehot = one_hot(y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.定义梯度</span></span><br><span class="line">        <span class="comment">#loss = mse_loss(out,y_onehot) 欧式距离</span></span><br><span class="line">        loss = F.mse_loss(out,y_onehot)</span><br><span class="line"><span class="comment">#3.梯度清０</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.梯度计算过程</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.参数更新</span></span><br><span class="line">        <span class="comment">#w&#x27;= w-lr*grad</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_loss.append(loss.item())<span class="comment">#loss: tensor =&gt; numpy</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(epoch,batch_idx,loss.item())</span><br><span class="line"></span><br><span class="line">plot_curve(train_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#get optimal[w1,b2,w2,b2,w3,b3]</span></span><br><span class="line"></span><br><span class="line">total_correct = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> test_loader:</span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">    out = net(x)</span><br><span class="line">    <span class="comment">#out = net(x)</span></span><br><span class="line">    pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    correct = pred.eq(y).<span class="built_in">sum</span>().<span class="built_in">float</span>().item()</span><br><span class="line">    total_correct+= correct</span><br><span class="line"></span><br><span class="line">total_num = <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">acc=total_correct/total_num</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test acc:&#x27;</span>,acc)</span><br><span class="line"></span><br><span class="line">x,y = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line">out = net(x.view(x.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">plot_image(x,pred,<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-encodng: utf-8-*-</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">@File: util.py.py</span></span><br><span class="line"><span class="string">@Contact: 2257925767@qq.com</span></span><br><span class="line"><span class="string">@Author:wangyu</span></span><br><span class="line"><span class="string">@Version:</span></span><br><span class="line"><span class="string">        手写数字识别的工具文件</span></span><br><span class="line"><span class="string">@Desciption:</span></span><br><span class="line"><span class="string">    env: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@DateTime: 2020/8/22下午5:04 </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span>    matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_curve</span>(<span class="params">data</span>):</span><span class="comment">#计算训练曲线</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(data)), data, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;value&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;step&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;value&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span>(<span class="params">img, label, name</span>):</span><span class="comment">#展现出识别结果</span></span><br><span class="line"></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        plt.subplot(<span class="number">2</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        plt.imshow(img[i][<span class="number">0</span>]*<span class="number">0.3081</span>+<span class="number">0.1307</span>, cmap=<span class="string">&#x27;gray&#x27;</span>, interpolation=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(name, label[i].item()))</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span>(<span class="params">label, depth=<span class="number">10</span></span>):</span><span class="comment">#完成one-hot编码</span></span><br><span class="line">    out = torch.zeros(label.size(<span class="number">0</span>), depth)</span><br><span class="line">    idx = torch.LongTensor(label).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    out.scatter_(dim=<span class="number">1</span>, index=idx, value=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">简单的回归问题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-08-23 15:53:54" itemprop="dateCreated datePublished" datetime="2020-08-23T15:53:54+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2021-07-25 08:59:29" itemprop="dateModified" datetime="2021-07-25T08:59:29+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="简单的回归问题"><a href="#简单的回归问题" class="headerlink" title="简单的回归问题"></a>简单的回归问题</h2><h3 id="１．从简单到复杂"><a href="#１．从简单到复杂" class="headerlink" title="１．从简单到复杂"></a>１．从简单到复杂</h3><p>==梯度下降算法==：梯度是深度学习的核心</p>
<p>１.简单的小例子</p>
<p><img src="%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/2020-08-22_06-44.png" alt="2020-08-22_06-44"></p>
<p>２.问题的迭代</p>
<p>求解$y = wx+b$　二元一次方程$w, b$的值</p>
<p>　　i.中学阶段：　求解二元一次方程的方法－－&gt;消元法(利用Closed Form Solution精确求得ｗ,b的解)</p>
<p>​        ii.引入噪音(noise：），模拟现实情况，我们的目标并不是为了得到一个精确解，而是得到一个从经验上精度可行的近似解即可，</p>
<p>解决方法：需要更多的样本点＋之后采用梯度下降算法求解</p>
<p>​        iii.首先构造一个函数(均方差)，因为梯度下降算法是求解极值的算法</p>
<p>​<br>$$<br>loss = (y-wx-b)^2<br>$$<br>$loss$方程值最小所对应的$w,b$ 可以近似认为是二元一次方程的解</p>
<p>​        注:在实际问题中，首先根据样本分布的情况，选择它可能对应的方程（二元一次，二元二次….)</p>
<p>​        V:优化过程（Convex Optimization-凸优化问题)</p>
<p><img src="%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/2020-08-22_06-57.png" alt="2020-08-22_06-57"></p>
<p>​        针对于一个样本来讲<br>$$<br>对于loss函数，变量ｗ,b；按照梯度下降的算法求loss的极值\<br>            找到loss最小时，对应的w,b\<br>            通过梯度迭代更新\<br>initial : w=0,b=0</p>
<p>\<br>b = b+ learningRate* \partial w\<br>w = w+ learningRate*\partial b</p>
<p>\<br>\partial w = 2(y-wx-b)*(-x)<br>\<br>\partial b= 2(y-wx-b)(-1)<br>$$</p>
<p>之后编程实现的是在Ｎ个样本上的问题</p>
<p><img src="%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/2020-08-22_06-59.png" alt="2020-08-22_06-59"></p>
<p>​        </p>
<h3 id="2-问题类型"><a href="#2-问题类型" class="headerlink" title="2.问题类型"></a>2.问题类型</h3><p><img src="%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/2020-08-22_07-01.png" alt="2020-08-22_07-01"></p>
<h3 id="3-二元一次方程-编程实现"><a href="#3-二元一次方程-编程实现" class="headerlink" title="3. 二元一次方程　编程实现"></a>3. 二元一次方程　编程实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computer_error_for_line_given_points</span>(<span class="params">b,w,points</span>):</span> <span class="comment">##计算错误率</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(points)):</span><br><span class="line">        x=points[i,<span class="number">0</span>]</span><br><span class="line">        y=points[i,<span class="number">1</span>]</span><br><span class="line">        totalError += (y-(w*x+b))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError/(<span class="built_in">float</span>)(<span class="built_in">len</span>(points))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_gradient</span>(<span class="params">b_current,w_current,points,learningRate</span>):</span><span class="comment">##在所有的节点上进行，一次梯度下降，更新参数</span></span><br><span class="line">    b_gradient=<span class="number">0</span></span><br><span class="line">    w_gradient=<span class="number">0</span></span><br><span class="line">    N= <span class="built_in">float</span>(<span class="built_in">len</span>(points))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(points)):</span><br><span class="line">        x=points[i,<span class="number">0</span>]</span><br><span class="line">        y=points[i,<span class="number">1</span>]</span><br><span class="line">        b_gradient += -(<span class="number">2</span>/N) *(y-((w_current*x)+b_current))</span><br><span class="line">        w_gradient += -(<span class="number">2</span>/N)*x*(y-((w_current*x)+b_current))</span><br><span class="line">    new_b = b_current -(learningRate*b_gradient)</span><br><span class="line">    new_w = w_current -(learningRate*w_gradient)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>[new_b,new_w]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span>(<span class="params">points,starting_b,starting_w,learning_rate,num_iterator</span>):</span></span><br><span class="line"></span><br><span class="line">    b= starting_b</span><br><span class="line">    w= starting_w</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterator):</span><br><span class="line">        b,w = step_gradient(b,w,np.array(points),learning_rate)<span class="comment">#这里应该是没有选择最小的loss只是把最后迭代的结果返回</span></span><br><span class="line">    <span class="keyword">return</span> [b,w]</span><br><span class="line"></span><br><span class="line"><span class="comment">#这里没有数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    points = np.genfromtxt(<span class="string">&quot;/home/doriswang/workplace/coding/pytorch_learning/venv/include/2.1/data.csv&quot;</span>,delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    <span class="comment"># print(points)</span></span><br><span class="line">    learning_rate =<span class="number">0.0001</span></span><br><span class="line">    initial_b =<span class="number">0</span></span><br><span class="line">    initial_w =<span class="number">0</span></span><br><span class="line">    num_iterations = <span class="number">1000</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting gradient descent at b=&#123;0&#125; , w=&#123;1&#125; ,error =&#123;2&#125;&quot;</span>.<span class="built_in">format</span>(initial_b,initial_w,computer_error_for_line_given_points(initial_b,initial_w,points)))</span><br><span class="line">    [b,w]=gradient_descent_runner(points,initial_b,initial_w,learning_rate,num_iterations)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After &#123;0&#125; iterations b=&#123;1&#125;,w=&#123;2&#125;,error=&#123;3&#125;&quot;</span>.<span class="built_in">format</span>(num_iterations,b,w,computer_error_for_line_given_points(b,w,points)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/%E5%88%9D%E8%AF%86pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/%E5%88%9D%E8%AF%86pytorch/" class="post-title-link" itemprop="url">初始pytorch</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-08-23 15:53:10" itemprop="dateCreated datePublished" datetime="2020-08-23T15:53:10+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2020-08-28 14:36:18" itemprop="dateModified" datetime="2020-08-28T14:36:18+08:00">2020-08-28</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ، 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="初试深度学习"><a href="#初试深度学习" class="headerlink" title="初试深度学习"></a>初试深度学习</h2><h3 id="1-深度学习框架"><a href="#1-深度学习框架" class="headerlink" title="1.深度学习框架"></a>1.深度学习框架</h3><p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_19-40.png" alt="2020-08-21_19-40"></p>
<p>==pytorch &amp;&amp; tensorflow的本质区别在于动态图优先还是静态图优先==</p>
<p>1.动态优先图–pytorch</p>
<p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_19-53.png" alt="2020-08-21_19-53"></p>
<p>2.静态图的方式–tensorflow</p>
<p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_19-59.png" alt="2020-08-21_19-59"></p>
<p>1.首先先建立一个计算图（框架）</p>
<p>2.向计算图传递参数来运行计算图</p>
<p>在计算图运行过程中，我们不能干预，调试或者动态改变比较麻烦</p>
<h4 id="综合评价"><a href="#综合评价" class="headerlink" title="综合评价"></a>综合评价</h4><p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_20-04.png" alt="2020-08-21_20-04"></p>
<h3 id="２-pytorch的生态"><a href="#２-pytorch的生态" class="headerlink" title="２.pytorch的生态"></a>２.pytorch的生态</h3><p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_20-10.png" alt="2020-08-21_20-10"></p>
<h3 id="３-pytorch的优势"><a href="#３-pytorch的优势" class="headerlink" title="３.pytorch的优势"></a>３.pytorch的优势</h3><p>１.使用GPU进行加速</p>
<p>２.自动求导</p>
<p>３.常用网络层</p>
<p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_20-19.png" alt="2020-08-21_20-19"></p>
<h2 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h2><p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_20-26.png" alt="2020-08-21_20-26"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/" class="post-title-link" itemprop="url">pytorch基础语法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-08-23 12:16:28" itemprop="dateCreated datePublished" datetime="2020-08-23T12:16:28+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2021-07-25 08:51:37" itemprop="dateModified" datetime="2021-07-25T08:51:37+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-基本数据类型"><a href="#1-基本数据类型" class="headerlink" title="1 基本数据类型"></a>1 基本数据类型</h2><h3 id="1-1-python中数据类型与torch中数据类型的对比"><a href="#1-1-python中数据类型与torch中数据类型的对比" class="headerlink" title="1.1 python中数据类型与torch中数据类型的对比"></a>1.1 python中数据类型与torch中数据类型的对比</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_18-08.png" alt="2020-08-23_18-08"></p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_18-12.png" alt="2020-08-23_18-12"></p>
<p>《下图中常用的数据类型使用红色方框表示出来》</p>
<h3 id="1-2-pytorch中String的表示方法"><a href="#1-2-pytorch中String的表示方法" class="headerlink" title="1.2 pytorch中String的表示方法"></a>1.2 pytorch中String的表示方法</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_18-10.png" alt="2020-08-23_18-10"></p>
<h3 id="1-3-pytorch中CPU和GPU数据类型的区别"><a href="#1-3-pytorch中CPU和GPU数据类型的区别" class="headerlink" title="1.3　pytorch中CPU和GPU数据类型的区别"></a>1.3　pytorch中CPU和GPU数据类型的区别</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_17-07.png" alt="2020-08-23_17-07"></p>
<p>如果在GPU上面，则需要将其数据类型转换:<br>方法：``data=data.cuda()`　，调用此函数会返回一个ＧＰＵ上的一个应用</p>
<h3 id="1-4-torch中数据类型的判断"><a href="#1-4-torch中数据类型的判断" class="headerlink" title="1.4 torch中数据类型的判断"></a>1.4 torch中数据类型的判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch中的数据类型</span></span><br><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">3</span>)<span class="comment">#随机初始化一个两行三列的数据</span></span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">type</span>()) <span class="comment">#查看数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(a,torch.FloatTensor)) <span class="comment">#数据类型合法性检验</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(a,torch.IntTensor))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.FloatTensor</span></span><br><span class="line"><span class="string">&lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="1-5-Tensor的形状"><a href="#1-5-Tensor的形状" class="headerlink" title="1.5 Tensor的形状"></a>1.5 Tensor的形状</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_18-57.png" alt="2020-08-23_18-57"></p>
<p><code>data.shape</code>, <code>data.size()</code>,<code>data.dim()</code>,<code>data.numel()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dim =  size(data.shape) ,表示数据的深度／维度</span></span><br><span class="line"><span class="comment"># size=shape，表示数据的形状，记录每一维度的长度</span></span><br><span class="line"><span class="comment"># numel:表示tensor占用内存的长度</span></span><br><span class="line">d4 = torch.FloatTensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(d4.size())</span><br><span class="line"><span class="built_in">print</span>(d4.shape)</span><br><span class="line"><span class="built_in">print</span>(d4.dim())</span><br><span class="line"><span class="built_in">print</span>(d4.size(<span class="number">0</span>)) <span class="comment">#可以进行索引，某维度的长度，为了更好地与python进行交互，一般直接将size和shape转换成list</span></span><br><span class="line"><span class="built_in">print</span>(d4.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([2, 3])</span></span><br><span class="line"><span class="string">torch.Size([2, 3])</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-00.png" alt="2020-08-23_19-00"></p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-19.png" alt="2020-08-23_19-19"></p>
<h3 id="1-6-Dim-0的数据"><a href="#1-6-Dim-0的数据" class="headerlink" title="1.6 Dim 0的数据"></a>1.6 Dim 0的数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape) <span class="comment">#1.0/tensor(1.)是0维的是标量，但是[1.]是１维长度为１的Tensor</span></span><br><span class="line">                			<span class="comment">#标量的应用：计算的loss函数都是标量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(a.shape))</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([]),这里表示size中的‘list&#x27;长度为０</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">torch.Size([])</span></span><br><span class="line"><span class="string">tensor(2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>标量的用途：　1.常见是在计算loss函数的结果是用标量表示的</p>
<h3 id="1-7-Dim-1的数据"><a href="#1-7-Dim-1的数据" class="headerlink" title="1.7 Dim 1的数据"></a>1.7 Dim 1的数据</h3><p>常用于  1. Bias: wx+b 中的 b         2. Linear Input 例如，手写数字识别中的28*28 可以看作长度为784的一维数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##Dim 1/ rank 1</span></span><br><span class="line"><span class="comment">#其实在tensor 0.3 之前没有dim=0的Tensor，实际上维度为１，size=1的数即可表示标量,不过0.4之后把他们区分开了</span></span><br><span class="line"></span><br><span class="line">a1=torch.tensor([<span class="number">1.1</span>])<span class="comment">#dim=1,size=1</span></span><br><span class="line">a2=torch.tensor([<span class="number">1.1</span>,<span class="number">2.2</span>]) <span class="comment"># dim=1,size=2</span></span><br><span class="line"></span><br><span class="line">a3=torch.FloatTensor(<span class="number">1</span>) <span class="comment">#这里的参数指定的是Dim１的长度，随机初始化</span></span><br><span class="line"><span class="built_in">print</span>(a1.size())</span><br><span class="line"><span class="built_in">print</span>(a2.size())</span><br><span class="line"><span class="built_in">print</span>(a3.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([1])</span></span><br><span class="line"><span class="string">torch.Size([2])</span></span><br><span class="line"><span class="string">torch.Size([1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="1-8-Dim２的数据"><a href="#1-8-Dim２的数据" class="headerlink" title="1.8 Dim２的数据"></a>1.8 Dim２的数据</h3><p>常用场景：　1. Linear Input batch [ batch_size , features_num]</p>
<p>a=torch.tensor([4,784])<br>其中4是指数据图片的数目，而784是指每一张图片的特征维度<br>适用于普通的机器学习数据</p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-12_1.png" alt="2020-08-23_19-12_1"></p>
<h3 id="1-9-Dim-3的数据"><a href="#1-9-Dim-3的数据" class="headerlink" title="1.9 Dim 3的数据"></a>1.9 Dim 3的数据</h3><p><strong>RNN Input Batch</strong>　[batch_size, sentence_num, word_one_sentence]</p>
<p>例如，对于RNN神经网络进行语音识别与处理时[10,20,100]表示:每个单词包含100个特征，一句话一共有10个单词，而每次输20句话</p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-12.png" alt="2020-08-23_19-12"></p>
<h3 id="1-10-Dim-4-的数据"><a href="#1-10-Dim-4-的数据" class="headerlink" title="1.10 Dim 4 的数据"></a>1.10 Dim 4 的数据</h3><p><strong>CNN input Batch</strong> [batch_size, channel_num, height, weight ]　</p>
<p>通道数，图片的高度，图片的宽度</p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-19_1.png" alt="2020-08-23_19-19_1"></p>
<h2 id="2-创建Tensor"><a href="#2-创建Tensor" class="headerlink" title="2.创建Tensor"></a>2.创建Tensor</h2><h3 id="1-Import-from-numpy"><a href="#1-Import-from-numpy" class="headerlink" title="1. Import from numpy"></a>1. Import from numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([<span class="number">2</span>,<span class="number">3.3</span>]) <span class="comment"># 数据实际是double类型</span></span><br><span class="line"><span class="built_in">print</span>(torch.from_numpy(a))</span><br><span class="line"></span><br><span class="line">a = np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.from_numpy(a))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([2.0000, 3.3000], dtype=torch.float64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]], dtype=torch.float64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="2-Import-from-list"><a href="#2-Import-from-list" class="headerlink" title="2. Import from list"></a>2. Import from list</h3><p><strong>关于参数传递的小问题</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor / FloatTensor /IntTensor　等既可以传递数值型，也可以传递shape相关的类型的数据</span><br><span class="line">tensor只能传递数据list作为参数</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">2.</span>,<span class="number">3.2</span>])</span><br><span class="line">b = torch.tensor([[<span class="number">2.</span>,<span class="number">2.3</span>],[<span class="number">1.</span>,<span class="number">2.34</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">a2 = torch.FloatTensor([<span class="number">2.</span>,<span class="number">3.2</span>])<span class="comment">##不建议用该方法，tensor可以 代替</span></span><br><span class="line">a3 = torch.FloatTensor(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#建议传递参数为shape参数</span></span><br><span class="line"><span class="built_in">print</span>(a2)</span><br><span class="line"><span class="built_in">print</span>(a3)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;        </span></span><br><span class="line"><span class="string">tensor([2.0000, 3.2000])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[2.0000, 2.3000],</span></span><br><span class="line"><span class="string">        [1.0000, 2.3400]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([2.0000, 3.2000])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[[ 2.0281e+15,  4.5682e-41,  2.5162e-26,  4.5682e-41],</span></span><br><span class="line"><span class="string">         [ 2.0260e+15,  4.5682e-41,  2.5162e-26,  4.5682e-41],</span></span><br><span class="line"><span class="string">         [ 2.0260e+15,  4.5682e-41,  2.5157e-26,  4.5682e-41]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 2.0284e+15,  4.5682e-41,  2.6360e-26,  4.5682e-41],</span></span><br><span class="line"><span class="string">         [ 3.6957e+15,  4.5682e-41,  2.6361e-26,  4.5682e-41],</span></span><br><span class="line"><span class="string">         [ 2.0284e+15,  4.5682e-41, -2.9008e+29,  3.0639e-41]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-未初始化的数据"><a href="#3-未初始化的数据" class="headerlink" title="3. 未初始化的数据"></a>3. 未初始化的数据</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_20-25.png" alt="2020-08-23_20-25"></p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_20-29.png" alt="2020-08-23_20-29"></p>
<h3 id="4-随机初始化"><a href="#4-随机初始化" class="headerlink" title="4.随机初始化"></a>4.随机初始化</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#相关函数</span><br><span class="line">torch.rand(Tensor.shape)　#生成一个[0,1)之间的随机数</span><br><span class="line">torch.randlike(Tensor tt)</span><br><span class="line"></span><br><span class="line">torch.randint(  int low , int high , [d1,d2....])#均匀采样，只采整数值</span><br><span class="line">torch.randint_like(Tensor tt ,int low, int high) </span><br></pre></td></tr></table></figure>

<p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">a= torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># a1 = torch.rand()</span></span><br><span class="line"></span><br><span class="line">b = torch.rand_like(a)<span class="comment">#会生成与ａ尺寸大小相同的随机数的tensor</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">c = torch.randint(<span class="number">1</span>,<span class="number">99</span>,[<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line">d = torch.randint_like(c,<span class="number">1</span>,<span class="number">10</span>) <span class="comment">#使用randint一定要指出数据范围</span></span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.9843, 0.0908, 0.6296],</span></span><br><span class="line"><span class="string">        [0.5270, 0.4654, 0.3570],</span></span><br><span class="line"><span class="string">        [0.9419, 0.1018, 0.1724]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0.3775, 0.3013, 0.3237],</span></span><br><span class="line"><span class="string">        [0.6003, 0.1313, 0.1082],</span></span><br><span class="line"><span class="string">        [0.7574, 0.4202, 0.0610]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[20, 93, 85],</span></span><br><span class="line"><span class="string">        [20, 81, 95],</span></span><br><span class="line"><span class="string">        [22, 10, 88]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[8, 9, 8],</span></span><br><span class="line"><span class="string">        [2, 7, 4],</span></span><br><span class="line"><span class="string">        [1, 4, 9]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="5-生成正态分布"><a href="#5-生成正态分布" class="headerlink" title="5.生成正态分布"></a>5.生成正态分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#相关函数说明</span></span><br><span class="line">tensor.normal(mean,std,*,generator=<span class="literal">None</span>,out = <span class="literal">None</span>) -&gt;Tensor</span><br><span class="line"><span class="comment">#参数说明:该方法只能生成一维,需要进行形状变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1. mean, std可以都为tensor，此时可以为每个元素指定分布来源</span></span><br><span class="line"><span class="comment">#2. mean,std可以一个为float/tensor，此时最后生成的Tensor的尺寸mean/std(数据类型为tensor)决定，此时所有数据共享均值或者方差</span></span><br><span class="line"><span class="comment">#3. mean,std都为float时，要求指明参数，此时所有数据都来源一个分布</span></span><br><span class="line"></span><br><span class="line">tensor,randn(*size , out = <span class="literal">None</span>, dtype = <span class="literal">None</span>,....)<span class="comment">#参数列表没有列全</span></span><br><span class="line"><span class="comment">#从平均值为0，方差为1的正态分布中返回一个填充有随机数的张量（也称为标准正态分布</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">a1 = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#从平均值为0，方差为1的正态分布中返回一个填充有随机数的张量，形状由参数决定（也称为标准正态分布）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#关于torch.normal：比较灵活，掌握一种方式就行</span></span><br><span class="line"><span class="comment">#方式１：保证mean和std的尺寸相同</span></span><br><span class="line">a = torch.normal( mean = torch.arange(<span class="number">1.</span>,<span class="number">11.</span>),std = torch.arange(<span class="number">1</span>,<span class="number">0</span>,-<span class="number">0.1</span>))</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment">#方式２：</span></span><br><span class="line">b = torch.normal(mean = torch.full([<span class="number">10</span>],<span class="number">0</span>),std = <span class="number">0.5</span>) <span class="comment">#这种方式也是可以实现所有数据来自一个分布</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">c = torch.normal(mean = <span class="number">10</span>, std = torch.arange(<span class="number">5</span>,<span class="number">0</span>,-<span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment">#方式３</span></span><br><span class="line">d = torch.normal(<span class="number">10</span>,<span class="number">0.1</span>,size=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-0.6377,  0.0554, -0.7616],</span></span><br><span class="line"><span class="string">        [ 0.1244, -0.1076,  0.2659]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([-1.5609,  0.1449,  2.7791,  3.4830,  4.6098,  5.9147,  7.2475,  8.0593,</span></span><br><span class="line"><span class="string">         9.3134, 10.0676])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([ 0.8977, -0.6111, -0.2214, -0.0326,  0.4336, -0.2131, -0.0297, -0.0359,</span></span><br><span class="line"><span class="string">         0.0294, -0.3355])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([11.8549, 12.7589, 13.7979, 12.9034,  8.1115, 15.1006, 13.7763, 12.9549,10.0838, 10.1662])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[10.0064,  9.8803,  9.9584],</span></span><br><span class="line"><span class="string">        [10.1267, 10.2988,  9.8402]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="6-生成元素相同的tensor"><a href="#6-生成元素相同的tensor" class="headerlink" title="6.生成元素相同的tensor"></a>6.生成元素相同的tensor</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-13.png" alt="2020-08-24_00-13"></p>
<h3 id="7-生成等差数列的tensor"><a href="#7-生成等差数列的tensor" class="headerlink" title="7.生成等差数列的tensor"></a>7.生成等差数列的tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#相关函数说明</span></span><br><span class="line">torch.arange( start ,end , offset )</span><br><span class="line"><span class="comment">#torch.range(start,end, offset) #闭区间</span></span><br><span class="line">torch.linspace(start, end, steps=<span class="number">100</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Return </span></span><br><span class="line"><span class="string">start (float) – the starting value for the set of points</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">end (float) – the ending value for the set of points</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">steps (int) – number of points to sample between start and end. Default: 100.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">torch.logspace(torch.logspace(start, end, steps=<span class="number">100</span>, base=<span class="number">10.0</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>) → Tensor</span><br><span class="line">               </span><br><span class="line"> <span class="string">&#x27;&#x27;&#x27;  </span></span><br><span class="line"><span class="string">    Returns a one-dimensional tensor of steps points logarithmically spaced with base base between base^&#123;start&#125;  and  base^&#123;end&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-14.png" alt="2020-08-24_00-14"></p>
<p>???=＝如何设置base==</p>
<h3 id="8-其他生成函数-Ones-zeros-eye"><a href="#8-其他生成函数-Ones-zeros-eye" class="headerlink" title="8.其他生成函数(Ones, zeros,eye)"></a>8.其他生成函数(Ones, zeros,eye)</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-23.png" alt="2020-08-24_00-23"></p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-23_1.png" alt="2020-08-24_00-23_1"></p>
<h3 id="9-randperm-random-shuffle"><a href="#9-randperm-random-shuffle" class="headerlink" title="9.randperm(random.shuffle)"></a>9.randperm(random.shuffle)</h3><p>??</p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-26.png" alt="2020-08-24_00-26"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/git%E5%91%BD%E4%BB%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/git%E5%91%BD%E4%BB%A4/" class="post-title-link" itemprop="url">git命令</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-08-23 08:07:42" itemprop="dateCreated datePublished" datetime="2020-08-23T08:07:42+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2021-07-25 09:27:22" itemprop="dateModified" datetime="2021-07-25T09:27:22+08:00">2021-07-25</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="git学习和博客搭建的相关资源"><a href="#git学习和博客搭建的相关资源" class="headerlink" title="git学习和博客搭建的相关资源"></a>git学习和博客搭建的相关资源</h2><h3 id="1-hexo搭建博客的相关参考博客"><a href="#1-hexo搭建博客的相关参考博客" class="headerlink" title="1. hexo搭建博客的相关参考博客"></a>1. hexo搭建博客的相关参考博客</h3><p>​    a.如何在本地库增加新的博客</p>
<p><a target="_blank" rel="noopener" href="https://winney07.github.io/2018/08/02/%E5%9C%A8Hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%8F%91%E5%B8%83%E6%96%87%E7%AB%A0/">https://winney07.github.io/2018/08/02/%E5%9C%A8Hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%8F%91%E5%B8%83%E6%96%87%E7%AB%A0/</a></p>
<p> b.如何搭建博客</p>
<p>​    <a target="_blank" rel="noopener" href="http://www.wzqj.top/2019/02/26/linux%E4%B8%8B%E4%BD%BF%E7%94%A8-Github-Pages-Hexo-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">http://www.wzqj.top/2019/02/26/linux%E4%B8%8B%E4%BD%BF%E7%94%A8-Github-Pages-Hexo-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000017986794">https://segmentfault.com/a/1190000017986794</a></p>
<p>c.hexo的常见命令</p>
<p><a target="_blank" rel="noopener" href="https://vel.life/Hexo-%E5%8D%9A%E6%96%87%E7%BC%96%E8%BE%91%E6%89%8B%E5%86%8C/">https://vel.life/Hexo-%E5%8D%9A%E6%96%87%E7%BC%96%E8%BE%91%E6%89%8B%E5%86%8C/</a></p>
<p><strong>托管中心</strong><code>维护远程库</code></p>
<ul>
<li><strong>内网：可以自己搭建一个GitLab服务器</strong></li>
<li><strong>外网：可以使用码云、Github</strong></li>
</ul>
<p><strong>版本控制工具</strong></p>
<ul>
<li><strong>集中式</strong>：CSV ,<strong>SVN</strong>,VSS</li>
<li><strong>分布式</strong>：<strong>Git</strong>，Darcs,…</li>
</ul>
<h2 id="Git命令行操作"><a href="#Git命令行操作" class="headerlink" title="Git命令行操作"></a>Git命令行操作</h2><h3 id="1-1本地库初始化"><a href="#1-1本地库初始化" class="headerlink" title="1.1本地库初始化"></a>1.1本地库初始化</h3><p><code>进入文件夹</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">注意：生成的 .git 目录中存放的是本地库相关文件，不要删除</span><br></pre></td></tr></table></figure>

<h3 id="1-2设置签名"><a href="#1-2设置签名" class="headerlink" title="1.2设置签名"></a>1.2设置签名</h3><ul>
<li><p>项目(仓库)级别<code>仅在当前本地库有效</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config user.name tom  #设置用户名tom</span><br><span class="line">git config user.email liu@qq.com #设置用户邮箱</span><br></pre></td></tr></table></figure></li>
<li><p>系统用户级别<code>仅在当前登录的操作系统用户有效</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name tom</span><br><span class="line">git config --global user.email liu@qq.com</span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>仅仅加了一个 <code>--global</code></p>
<p>优先级别：<code>项目级别</code>  &gt;  <code>系统级别</code></p>
<p>信息保存位置：<code>~/.gitconfig 文件</code>   </p>
</blockquote>
<h3 id="1-3基本操作"><a href="#1-3基本操作" class="headerlink" title="1.3基本操作"></a>1.3基本操作</h3><h4 id="1-3-1-状态查看"><a href="#1-3-1-状态查看" class="headerlink" title="1.3.1 状态查看"></a>1.3.1 状态查看</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status   #查看工作区、暂存区状态</span><br></pre></td></tr></table></figure>

<h4 id="1-3-2-添加"><a href="#1-3-2-添加" class="headerlink" title="1.3.2 添加"></a>1.3.2 添加</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add fileName  #指定文件</span><br><span class="line">git add . #所有</span><br><span class="line">说明：将工作区的文件添加到暂存区</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git rm --cached filename</span><br><span class="line">git rm /*</span><br><span class="line">＃将filename对应的文件从缓存区撤回，但是文件仍保留在工作区，相当于是add的一个撤回操作　sudo权限</span><br></pre></td></tr></table></figure>



<h4 id="1-3-3-提交"><a href="#1-3-3-提交" class="headerlink" title="1.3.3 提交"></a>1.3.3 提交</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &#x27;commit message&#x27; fileName</span><br><span class="line">说明：将暂存区内容提交到本地库,不需要再通过vim编辑提交的说明信息，没有提交信息，本次提交将无效</span><br><span class="line">git commit *</span><br><span class="line">说明：将暂存区中的所有内容都提交到本地库中</span><br></pre></td></tr></table></figure>

<p>文件修改之后</p>
<ol>
<li>要先git add加到暂存区中，之后再添加到git commit</li>
<li>或者直接使用 git -a commit filename 这时本次提交不能撤回(git rm /*)</li>
</ol>
<h4 id="1-3-4-查看历史记录"><a href="#1-3-4-查看历史记录" class="headerlink" title="1.3.4 查看历史记录"></a>1.3.4 查看历史记录</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git log #查看提交的日志，包括时间，提交的说明信息</span><br><span class="line">git reflog  #常用</span><br><span class="line">git log --greph #图形显示,更直观</span><br><span class="line">git log --pretty=oneline #每行日志</span><br><span class="line">git log --oneline #简洁显示</span><br><span class="line">说明：HEAD@&#123;移动到当前版本需要多少步&#125;</span><br></pre></td></tr></table></figure>

<p><img src="git%E5%91%BD%E4%BB%A4/2020-08-23_10-49.png" alt="2020-08-23_10-49"></p>
<p>哈希值＋指针移动量＋提交的说明信息</p>
<p>多屏控制的方式：</p>
<p>１．空格向下翻页　，ｂ向下翻页　，ｑ退出</p>
<p><img src="git%E5%91%BD%E4%BB%A4/2020-08-23_10-59.png" alt="2020-08-23_10-59"></p>
<h4 id="1-3-5-前进后退"><a href="#1-3-5-前进后退" class="headerlink" title="1.3.5 前进后退"></a>1.3.5 前进后退</h4><ul>
<li><p>基于索引值<code>推荐</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard 指针位置</span><br><span class="line">例子：git reset --hard a6ace91 #回到这个状态，a6ace91指的是日志的短的索引值</span><br></pre></td></tr></table></figure></li>
<li><p>使用 <strong>^</strong> 符号<code>只能后退</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard HEAD^</span><br><span class="line">例子：git reset --hard HEAD^^</span><br><span class="line">注意：几个 ^ 表示后退几步</span><br></pre></td></tr></table></figure></li>
<li><p>使用 <strong>~</strong> 符号<code>只能后退</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard HEAD~n</span><br><span class="line">例子：git reset --hard HEAD~3</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="1-3-6-reset的三个参数比较"><a href="#1-3-6-reset的三个参数比较" class="headerlink" title="1.3.6 reset的三个参数比较"></a>1.3.6 reset的三个参数比较</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">soft: </span><br><span class="line">  - 仅本地库移动HEAD 指针</span><br><span class="line">mixed:</span><br><span class="line">  - 在本地库移动HEAD指针</span><br><span class="line">  - 重置暂存区</span><br><span class="line">hard:</span><br><span class="line">  - 在本地库移动HEAD指针</span><br><span class="line">  - 重置暂存区－－及暂存区的文件也回到之前的版本</span><br><span class="line">  - 重置工作区</span><br></pre></td></tr></table></figure>

<h4 id="1-3-7-删除文件并找回"><a href="#1-3-7-删除文件并找回" class="headerlink" title="1.3.7　删除文件并找回"></a>1.3.7　删除文件并找回</h4><ul>
<li><strong>相当于建立一个快照，虽然删除了，但只要添加到暂存区，就能找回</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard 指针位置</span><br></pre></td></tr></table></figure>

<p><img src="git%E5%91%BD%E4%BB%A4/2020-08-23_11-45.png" alt="2020-08-23_11-45"></p>
<h4 id="1-3-8-文件差异比较"><a href="#1-3-8-文件差异比较" class="headerlink" title="1.3.8 文件差异比较"></a>1.3.8 文件差异比较</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git diff 文件名 #将工作区中的文件与暂存区中的文件进行比较</span><br><span class="line">git diff 哈希值 文件名  #将工作区中的文件和历史中的一个版本比较</span><br><span class="line">git diff  #不带文件名，则比较多个文件</span><br></pre></td></tr></table></figure>

<h3 id="2-2-分支管理"><a href="#2-2-分支管理" class="headerlink" title="2.2 分支管理"></a>2.2 分支管理</h3><p><code>hot_fix</code> <code>master</code> <code>feature_x</code> <code>feature_y</code></p>
<h4 id="2-2-1-什么是分支管理"><a href="#2-2-1-什么是分支管理" class="headerlink" title="2.2.1 什么是分支管理"></a>2.2.1 什么是分支管理</h4><ul>
<li>在版本控制中，使用推进多个任务</li>
</ul>
<h4 id="2-2-2-分支的好处"><a href="#2-2-2-分支的好处" class="headerlink" title="2.2.2 分支的好处"></a>2.2.2 分支的好处</h4><ul>
<li>同时并行推进多个功能开发，提高开发效率</li>
<li>某一分支开发失败，不会对其它分支有任何影响</li>
</ul>
<h4 id="2-2-3-分支操作"><a href="#2-2-3-分支操作" class="headerlink" title="2.2.3 分支操作"></a>2.2.3 分支操作</h4><ul>
<li>创建分支</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch 分支名</span><br></pre></td></tr></table></figure>

<ul>
<li>查看分支</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git branch</span><br><span class="line">git branch -v </span><br></pre></td></tr></table></figure>

<ul>
<li>切换分支</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout 分支名</span><br><span class="line">git checkout -b 分支名   #创建分支并直接切换到该分支</span><br></pre></td></tr></table></figure>

<ul>
<li>合并分支<code>相当于把修改了的文件拉过来</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git merge xxx</span><br><span class="line">注意：合并分支的时候要明确谁谁合并</span><br><span class="line">	我在a分支里面修改了。要合并到master，就先切换到master，然后合并b</span><br></pre></td></tr></table></figure>

<ul>
<li>删除分支</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -d 分支名</span><br></pre></td></tr></table></figure>



<h4 id="2-2-4-解决冲突"><a href="#2-2-4-解决冲突" class="headerlink" title="2.2.4 解决冲突"></a>2.2.4 解决冲突</h4><ul>
<li>冲突的表现</li>
<li>冲突的解决<ul>
<li>第一步：编辑，删除特殊标记<code>&lt;&lt;&lt;</code> <code>===</code></li>
<li>第二步：修改到满意位置，保存退出</li>
<li>第三步：添加到缓存区  <code>git  add 文件名</code></li>
<li>第四步：提交到本地库<code>git commit -m &#39;日志信息&#39; </code>  <code>注意：后面一定不能带文件名</code></li>
</ul>
</li>
</ul>
<h2 id="Git-结合Github"><a href="#Git-结合Github" class="headerlink" title="Git 结合Github"></a>Git 结合Github</h2><p><code>别分手</code>  <code>别名 分支名</code></p>
<h4 id="1-1-创建远程库地址别名"><a href="#1-1-创建远程库地址别名" class="headerlink" title="1.1 创建远程库地址别名"></a>1.1 创建远程库地址别名</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote -v  #查看远程地址别名</span><br><span class="line">git remote add 别名 远程地址 </span><br><span class="line">例子：git remote add origin https://xx</span><br></pre></td></tr></table></figure>

<h4 id="1-2-推送"><a href="#1-2-推送" class="headerlink" title="1.2 推送"></a>1.2 推送</h4><p><code>开发修改完把本地库的文件推送到远程仓库</code> <code>前提是提交到了本地库才可以推送</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git push 远程库别名 分支名</span><br><span class="line">git push -u 远程库别名 分支名    #-u指定默认主机</span><br><span class="line">例子：git push origin master</span><br></pre></td></tr></table></figure>

<h4 id="1-3-克隆"><a href="#1-3-克隆" class="headerlink" title="1.3 克隆"></a>1.3 克隆</h4><p><code>完整的把远程库克隆到本地</code>  <code>克隆下来后不要在主分支里面做开发</code> <code>clone进行一次，从无到有的过程，更新用pull</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone  远程地址</span><br><span class="line">例子：git clone https://xx</span><br></pre></td></tr></table></figure>

<h4 id="1-4-拉取"><a href="#1-4-拉取" class="headerlink" title="1.4 拉取"></a>1.4 拉取</h4><p>  <code>本地存在clone下来的文件  就用pull更新</code>  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pull = fetch + merge</span><br><span class="line">	git fetch 别名 分支名</span><br><span class="line">	git merge 别名 分支名</span><br><span class="line">git pull 别名 分支名</span><br></pre></td></tr></table></figure>

<h4 id="1-5-解决冲突"><a href="#1-5-解决冲突" class="headerlink" title="1.5 解决冲突"></a>1.5 解决冲突</h4><p><code>注意：解决冲突后的提交是不能带文件名的</code></p>
<p><code>如果不是基于远程库最新版做的修改不能推送，必须先pull下来安装冲突办法解决</code></p>
<h4 id="1-6-rebase"><a href="#1-6-rebase" class="headerlink" title="1.6 rebase"></a>1.6 rebase</h4><p><code>提交记录简洁不分叉</code>  <code>没学懂，感觉有点鸡肋</code> <code>混眼熟</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git rebase -i 索引号</span><br><span class="line">git rebase -i HEAD~3  #合并最近三条记录</span><br><span class="line">说明：在vim编辑里面改成s</span><br></pre></td></tr></table></figure>

<h4 id="1-7-beyond-compare"><a href="#1-7-beyond-compare" class="headerlink" title="1.7 beyond compare"></a>1.7 beyond compare</h4><p><code>用软件解决冲突</code> </p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.安装 ：</span><br><span class="line">	beyond compare </span><br><span class="line">2.配置：</span><br><span class="line">    git config --local merge.tool bc3  #合并名称</span><br><span class="line">    git config --local mergetool.path &#x27;/usr/local/bin/bcomp&#x27; #软件路径</span><br><span class="line">    git config --local mergetool.keepBackup false  #False不用保存备份</span><br><span class="line">3.应用：</span><br><span class="line">	git mergetool</span><br><span class="line">说明：--local指只在当前操作系统有效</span><br></pre></td></tr></table></figure>

<h4 id="1-8-跨团队合作"><a href="#1-8-跨团队合作" class="headerlink" title="1.8 跨团队合作"></a>1.8 跨团队合作</h4><p><code>代码review之后合并</code></p>
<ul>
<li><p><strong>适用于个人</strong></p>
<p><strong>邀请成员</strong>:<code>Settings</code> –&gt; <code>Collaborators</code> –&gt;<code>填写用户名</code> –&gt;<code>打开链接接受邀请</code></p>
</li>
<li><p><strong>企业</strong>   <code>创建一个组织</code> <code>方便管理</code></p>
</li>
<li><p><strong>review</strong></p>
<p><code>组织做review</code>  <code>通过Pull request</code></p>
</li>
<li><p><strong>给开源社区共享代码</strong></p>
<p><code>点击别人仓库的fork 到自己的仓库</code>   – &gt; <code>然后clone下来 修改后推送到远程库</code>  –&gt; <code>点击Pull Request请求</code> –&gt; <code>Create pull request发消息</code></p>
</li>
</ul>
<h4 id="1-9-Tag标签"><a href="#1-9-Tag标签" class="headerlink" title="1.9 Tag标签"></a>1.9 Tag标签</h4><p><code>为了清晰的版本管理，公司一般不会直接使用commit提交</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git tag -a v1.0 -m &#x27;版本介绍&#x27;   #创建本地tag信息</span><br><span class="line">git tag -d v1.0    		#删除tag</span><br><span class="line">git push origin --tags   #将本地tag信息推送到远程库</span><br><span class="line">git pull origin --tags    #拉取到本地</span><br><span class="line"></span><br><span class="line">git checkout v.10    #切换tag</span><br><span class="line">git clone -b v0.1 地址   #指定tag下载代码</span><br></pre></td></tr></table></figure>



<h4 id="1-10-SSH-免密登录"><a href="#1-10-SSH-免密登录" class="headerlink" title="1.10 SSH 免密登录"></a>1.10 SSH 免密登录</h4><ul>
<li>输入:<code>ssh-keygen -t rsa -C GitHub邮箱地址</code>  </li>
<li>进入<code>.ssh</code>目录，复制<code>id_rsa.pub</code>文件内容</li>
<li>登录GitHub。<code>Settings</code>  –&gt; <code>SSH and GPG keys </code> –&gt; <code>New SSH Key    </code></li>
<li>回到git通过ssh地址创建。<code>git remote add 别名 SSH地址  </code></li>
</ul>
<h2 id="Git工作流"><a href="#Git工作流" class="headerlink" title="Git工作流"></a>Git工作流</h2><h4 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在项目开发过程中使用Git的方式</span><br></pre></td></tr></table></figure>

<h4 id="1-2-分类"><a href="#1-2-分类" class="headerlink" title="1.2 分类"></a>1.2 分类</h4><h5 id="1-2-1-集中式工作流"><a href="#1-2-1-集中式工作流" class="headerlink" title="1.2.1 集中式工作流"></a>1.2.1 集中式工作流</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">像SVN一样，集中式工作流有一个中央仓库，所有的修改都提交到了Master分支上</span><br></pre></td></tr></table></figure>

<h5 id="1-2-2-GitFlow工作流"><a href="#1-2-2-GitFlow工作流" class="headerlink" title="1.2.2 GitFlow工作流 *"></a>1.2.2 GitFlow工作流 <code>*</code></h5><p>主干分支<code>master</code>  开发分支<code>develop</code>  修复分支<code>hotfix</code>   预发布分支<code>release</code>  功能分支<code>feature</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GitFlow 有独立的分支，让发布迭代过程更流畅。</span><br></pre></td></tr></table></figure>

<h5 id="1-2-3-Forking-工作流"><a href="#1-2-3-Forking-工作流" class="headerlink" title="1.2.3 Forking 工作流"></a>1.2.3 Forking 工作流</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在 GitFlow 基础上， 充分利用了 Git 的 Fork 和 pull request 的功能以达到代码审核的目的。 </span><br><span class="line">安全可靠地管理大团队的开发者</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/30/cluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/05/30/cluster/" class="post-title-link" itemprop="url">聚类分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">نُشر في</span>

      <time title="أُنشأ: 2020-05-30 17:19:37" itemprop="dateCreated datePublished" datetime="2020-05-30T17:19:37+08:00">2020-05-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">عُدل في</span>
        <time title="عُدل: 2021-07-25 09:22:55" itemprop="dateModified" datetime="2021-07-25T09:22:55+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">في</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ، 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">聚类</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="机器学习—聚类篇"><a href="#机器学习—聚类篇" class="headerlink" title="机器学习—聚类篇"></a>机器学习—聚类篇</h2><h3 id="1-聚类算法的概述"><a href="#1-聚类算法的概述" class="headerlink" title="1.聚类算法的概述:"></a>1.聚类算法的概述:</h3><p>聚类试图 将数据集中的样本划分为若干个通常是不相交的子集,每个子集对聚类算法而言,样本簇亦称”类”称为一个”簇” (cluster). 通过这样的划分,每个簇可能对应于一些潜在的概念(类别) , </p>
<p>聚类既能作为一个单独过程,用于找寻数据内在的分布结构,也可作为分类等其他学习任务的前驱过程.例如,在一些商业应用中需对新用户的类型进行判别, {且定义”用户类型”对商家来说却可能不太容易,此时往往可先对用户数据进行聚类,根据聚类结果将每个簇定义为一个类,然后再基于这些类训练分类模型,用于判别新用户的类型.</p>
<p>训练的目标:</p>
<p>1.簇间的距离较大,簇内的距离较小</p>
<h3 id="2-指标和距离公式的选择"><a href="#2-指标和距离公式的选择" class="headerlink" title="2.指标和距离公式的选择"></a>2.指标和距离公式的选择</h3><p> 性能指标:簇内的相似度 和  簇间的相似度</p>
<p><img src="cluster/7.png" alt="7"></p>
<p><img src="cluster/8.png" alt="8"></p>
<p><img src="cluster/8.5.png" alt="8.5"></p>
<p><img src="cluster/9.png" alt="9"></p>
<h3 id="3-常见聚类算法"><a href="#3-常见聚类算法" class="headerlink" title="3.常见聚类算法"></a>3.常见聚类算法</h3><h4 id="3-1聚类算法类别"><a href="#3-1聚类算法类别" class="headerlink" title="3.1聚类算法类别"></a>3.1聚类算法类别</h4><table>
<thead>
<tr>
<th>类别</th>
<th>典型算法</th>
<th>算法的基本策略</th>
</tr>
</thead>
<tbody><tr>
<td>Partitioning approach</td>
<td>K-means,k-medoids,CLARANS,PAM</td>
<td>Construct various partitions and then evaluate them by come criterion</td>
</tr>
<tr>
<td>Hierarchical (层次) approach</td>
<td>Diana,Agnes,BIRCH,ROCK,CAMELEON</td>
<td>create a hierarchical decomposition of the set of data(or object) using some criterion</td>
</tr>
<tr>
<td>Density-based approach</td>
<td>DBSACN,OPTICS, DenClue</td>
<td>based on connectivity and density functions</td>
</tr>
<tr>
<td>Grid-based approach</td>
<td>STING,WaveCluster,CLQUE</td>
<td>based multiple-level granularity (多层次粒度)structure</td>
</tr>
<tr>
<td>Model-based</td>
<td>EM,SOM,COBWEB</td>
<td>A model is hypothesized for each of the clusters and tries to find the best fit of that model to each other</td>
</tr>
<tr>
<td>Frequent pattern-based</td>
<td>pCluster</td>
<td>based on the analysis of frequent patterns</td>
</tr>
<tr>
<td>used-guided or constraint-based</td>
<td>COD(obstacles),constrained clustering</td>
<td>cluster by considering user-specified or application-specifier constraints</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="4-Partition-Algorithm"><a href="#4-Partition-Algorithm" class="headerlink" title="4. Partition Algorithm"></a>4. Partition Algorithm</h3><h4 id="4-1-k-means"><a href="#4-1-k-means" class="headerlink" title="4.1 k-means"></a>4.1 k-means</h4><h5 id="4-1-1算法的基本步骤"><a href="#4-1-1算法的基本步骤" class="headerlink" title="4.1.1算法的基本步骤:"></a>4.1.1算法的基本步骤:</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.创建k个点作为初始质点(经常是随机选取)</span><br><span class="line">2.当任意一个点的簇分配结果发生变化时</span><br><span class="line">			*对于每个数据集中的数据点</span><br><span class="line">					#对于每个质点</span><br><span class="line">							计算出质心与数据点之间的距离</span><br><span class="line">					#将数据点分类到距离最近的簇 </span><br><span class="line">3.对于每个簇,计算簇中的所有点的均值作为新的质心</span><br><span class="line">4.重复上述操作,直至目标函数不再出现更优的结果</span><br></pre></td></tr></table></figure>

<p>目标函数 : 假设<img src="https://www.zhihu.com/equation?tex=x_i(i=1,2,%E2%80%A6,n)" alt="[公式]">是数据点，<img src="https://www.zhihu.com/equation?tex=%5Cmu_j(j=1,2,%E2%80%A6,k)" alt="[公式]">是初始化的数据中心</p>
<p>​                                                        $min\sum_{i=1}^n min ||x_i -\mu_j||^2$</p>
<p>这个函数是非凸优化函数,会收敛于局部最优解</p>
<p>函数的曲线如下:[以k=2为例]</p>
<p><img src="cluster/3.jpg" alt="3"></p>
<h5 id="4-1-2算法的基本评价"><a href="#4-1-2算法的基本评价" class="headerlink" title="4.1.2算法的基本评价"></a>4.1.2算法的基本评价</h5><p>时间复杂度: $O(tkn)$ :t :迭代的次数,k表示类别的个数,n表示样本的数量</p>
<p>相比较     RAM :$O(k(n-k)^2)$                                       CLARA: $O(ks^2 +k(n-k)$</p>
<p>算法的复杂度比较低,不过很可能得到的是局部最优解(可以通过,模拟退火和 遗传算法 得到最优解)</p>
<p>缺点: 1. 只能适用于 中心值有意义的数据,不适用于categorical data</p>
<p>​            2.需要提前确定k值</p>
<p>​            3.因为会出现进入局部最优解的现象,所以会对噪音点,以及最初中心点的设置会很敏感</p>
<p>​            4. Not suitable to discover clusters with non-convex shapes</p>
<h5 id="4-1-3对于K-means-算法的改进"><a href="#4-1-3对于K-means-算法的改进" class="headerlink" title="4.1.3对于K-means 算法的改进"></a>4.1.3对于K-means 算法的改进</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">可以用 modes来代替均值,用其他方法取确定质点</span><br><span class="line">更换其它衡量clusters相似性的标准,处理categorical object---目标函数</span><br><span class="line">using  frequency-based method to update modes of cluster</span><br><span class="line">a mixture of categorical </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="4-2-K-Medoids"><a href="#4-2-K-Medoids" class="headerlink" title="4.2 K-Medoids"></a>4.2 K-Medoids</h4><p>Find representative object,called medoids ,in clusters</p>
<h5 id="4-2-1-PAM"><a href="#4-2-1-PAM" class="headerlink" title="4.2.1 PAM"></a>4.2.1 PAM</h5><p>PAM(Partitioning Around Medoids,1987)是作为K-medoids的基础算法,基本流程:</p>
<ol>
<li><p>首先随机选择k个对象作为中心</p>
</li>
<li><p>把每个对象分配给离它最近的中心,然后随机地选择一个非中心交替替换中心对象</p>
</li>
<li><p>计算分配后的距离改进量,进而得到改进的cost ,如果cost&lt;0,即结果有改进,则进行交换</p>
</li>
<li><p>直到目标函数不再有改进为止</p>
<p>算法复杂度比较高,只适用于小数据集</p>
</li>
</ol>
<h5 id="4-2-2-CLARA"><a href="#4-2-2-CLARA" class="headerlink" title="4.2.2 CLARA"></a>4.2.2 CLARA</h5><p>它从数据集中抽取多个样本集, 对每个样本集使用PAM, 并以最好的聚类作为输出</p>
<h6 id="CLARA-算法的步骤"><a href="#CLARA-算法的步骤" class="headerlink" title="CLARA 算法的步骤:"></a><strong>CLARA 算法的步骤:</strong></h6><p>　　(1) for 　i = 1 to v (选样的次数) ,重复执行下列步骤( (2) ～ (4) ) :<br>　　(2) 随机地从整个数据库中抽取一个N(例如：(40 + 2 k))个对象的样本,调用PAM方法从样本中找出样本的k个最优的中心点。<br>　　(3)对于每个点,把它归类到距离它最近的点的簇<br>　　(4) 计算上一步中得到的聚类的总代价. 若该值小于当前的最小值,用该值替换当前的最小值,保留在这次选样中得到的k个代表对象作为到目前为止得到的最好的代表对象的集合.<br>　　(5) 返回到步骤(1) ,开始下一个循环.<br>　　算法结束后，输出最好的聚类结果</p>
<h6 id="优点"><a href="#优点" class="headerlink" title="优点:"></a>优点:</h6><p>可以处理的数据集比 PAM大</p>
<h6 id="缺点"><a href="#缺点" class="headerlink" title="缺点:"></a>缺点:</h6><p>1有效性依赖于样本集的大小</p>
<p>2 基于样本的好的聚类并不一定是整个数据集的好的聚类, 样本可能发生倾斜<br>　　例如, Oi是整个数据集上最佳的k个中心点之一, 但它不包含在样本中, CLARA将找不到最佳聚类</p>
<h5 id="4-2-3-CLARANS"><a href="#4-2-3-CLARANS" class="headerlink" title="4.2.3 CLARANS"></a>4.2.3 CLARANS</h5><h5 id="focusing-spatial-data-structure"><a href="#focusing-spatial-data-structure" class="headerlink" title="focusing+spatial data structure"></a>focusing+spatial data structure</h5><h3 id="5-层次化聚类方法"><a href="#5-层次化聚类方法" class="headerlink" title="5.层次化聚类方法"></a>5.层次化聚类方法</h3><p>一般来说分为两类:</p>
<ul>
<li>Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 <code>cluster</code>，每次按一定的准则将最相近的两个 <code>cluster</code> 合并生成一个新的 <code>cluster</code>，如此往复，直至最终所有的对象都属于一个 <code>cluster</code>。这里主要关注此类算法。</li>
<li><strong>Divisive 层次聚类</strong>： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 <code>cluster</code>，每次按一定的准则将某个 <code>cluster</code> 划分为多个 <code>cluster</code>，如此往复，直至每个对象均是一个 <code>cluster</code>。</li>
</ul>
<p>优点:1 .不需要提前确定k,但是需要一个停止条件</p>
<p>​            2.可以用树状结构来反映训练的过程,并且得到不同k情况下</p>
<p>的划分方案</p>
<p>缺点: 1</p>
<p>主要思想:</p>
<ol>
<li><p>每次找到距离最近的两个点作为一个簇</p>
<pre><code>              2. 把同一个簇的所有点看作一个点
                                             3. 重复上述操作.....直到达到停止条件
</code></pre>
</li>
</ol>
<h4 id="5-1-AGNES（Agglomerative-Nesting"><a href="#5-1-AGNES（Agglomerative-Nesting" class="headerlink" title="5.1 AGNES（Agglomerative Nesting)"></a>5.1 AGNES（Agglomerative Nesting)</h4><p>sklearn  源码:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity=’euclidean’, memory=None, connectivity=None, compute_full_tree=’auto’, linkage=’ward’, pooling_func=&lt;function mean&gt;)</span><br></pre></td></tr></table></figure>

<p>算法的主要过程:</p>
<p>single linkage: 适用两个簇之间 距离最小的一对点的距离作为簇之间的</p>
<p><img src="cluster/2.1.png" alt="2.1"></p>
<p>可以看到，该 算法的时间复杂度为 <img src="https://www.zhihu.com/equation?tex=O(n%5E3)" alt="[公式]"> （由于每次合并两个 <code>cluster</code> 时都要遍历大小为 <img src="https://www.zhihu.com/equation?tex=O(n%5E2)+" alt="[公式]">的距离矩阵来搜索最小距离，而这样的操作需要进行 <img src="https://www.zhihu.com/equation?tex=n%E2%88%921" alt="[公式]"> 次），空间复杂度为<img src="https://www.zhihu.com/equation?tex=O(n%5E2)+" alt="[公式]"> （由于要存储距离矩阵）</p>
<h4 id="5-2-DIANA"><a href="#5-2-DIANA" class="headerlink" title="5.2 DIANA"></a>5.2 DIANA</h4><p>是AGNES算法的反过程,最后结果是每个样本都是属于一类</p>
<h4 id="5-3-BIRCH"><a href="#5-3-BIRCH" class="headerlink" title="5.3 BIRCH"></a>5.3 BIRCH</h4><p>链接:<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6179132.html">https://www.cnblogs.com/pinard/p/6179132.html</a></p>
<p>链接:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22458092">https://zhuanlan.zhihu.com/p/22458092</a></p>
<h5 id="5-3-1-CF-Tree动态建立的过程"><a href="#5-3-1-CF-Tree动态建立的过程" class="headerlink" title="5.3.1 CF Tree动态建立的过程:"></a>5.3.1 CF Tree动态建立的过程:</h5><p>基本概念:</p>
<p><strong>CF（Clustering Feature）：</strong>类簇总体信息三元组<img src="https://www.zhihu.com/equation?tex=(N,+LS,+SS)" alt="[公式]">，其中<img src="https://www.zhihu.com/equation?tex=N" alt="[公式]">是一个类簇中数据点个数，<img src="https://www.zhihu.com/equation?tex=LS" alt="[公式]">是类簇中所有数据点的加和值，即<img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi=1%7D%5E%7BN%7D%7B%5Cbar%7BX_%7Bi%7D%7D%7D+" alt="[公式]">，<img src="https://www.zhihu.com/equation?tex=SS" alt="[公式]">是类簇中所有数据点的平方和<img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi=1%7D%5E%7BN%7D%7B%5Cbar%7BX_%7Bi%7D%7D%5E2%7D+" alt="[公式]">，CF相当于对一个类簇的信息做了总结。</p>
<p>性质: </p>
<p><strong>CF可加性定理</strong>：</p>
<p>​                针对两个不相交的类簇，其CF向量分别为<img src="https://www.zhihu.com/equation?tex=CF_%7B1%7D=(N_%7B1%7D,+LS_%7B1%7D,+SS_%7B1%7D)" alt="[公式]">，<img src="https://www.zhihu.com/equation?tex=CF_%7B2%7D=(N_%7B2%7D,+LS_%7B2%7D,+SS_%7B2%7D)" alt="[公式]">，那么这两个不相交类簇合并后的CF向量为：<img src="https://www.zhihu.com/equation?tex=CF_%7B1%7D+CF_%7B2%7D=(N_%7B1%7D+N_%7B2%7D,+LS_%7B1%7D+LS_%7B2%7D,+SS_%7B1%7D+SS_%7B2%7D)" alt="[公式]">。这里的证明比较简单，就略去了。<br>在BRICH中，针对一个类簇，通常只保留其CF向量信息，这样做比较节省空间且高效，后续的聚类需要的计算只需要根据类簇的CF向量即可完成。<br><strong>CF Tree：</strong>类似于B树的一颗高度平衡树，有三个参数：内部节点平衡因子B，叶节点平衡因子L，簇半径T。每个非叶子节点至多包含B个项，形式为<img src="https://www.zhihu.com/equation?tex=%5BCF_%7Bi%7D,+child_%7Bi%7D%5D" alt="[公式]">，其中<img src="https://www.zhihu.com/equation?tex=child_%7Bi%7D" alt="[公式]">是指向第i个子节点的指针。每个叶子节点至多包含L个项，且包含指向前后叶子节点的指针prev和next，其中的每一项代表一个类簇，且满足类簇直径小于T。树的结构如下图：</p>
<p><img src="https://pic1.zhimg.com/80/e117c5b3d038b06c3ab4ebd0748c25f4_720w.png" alt="img"></p>
<p>CF Tree会随着新的数据点的加入而动态的建立起来，其插入过程包含以下几步：<br>（1）从根节点开始，递归向下选择最近的孩子节点，这里最近的度量是根据前面提到的<img src="https://www.zhihu.com/equation?tex=D0,+D1,+D2,+D3,+D4" alt="[公式]">中任意一个确定；<br>（2）如果在（1）中找到了最近的叶子节点中的一个<img src="https://www.zhihu.com/equation?tex=L_%7Bt%7D" alt="[公式]">，检查其中最近的CF元组能否不超过阈值T吸收此数据点，若能，更新CF值；若不能，是否有空间（<strong>这里每个节点能分配的空间有限，可参见下面的参数表</strong>）添加新的元组，若能则添加新的元组，若不能，分裂距离最远的一对元组到两个叶子节点，作为初始的种子，将其他元组按距离最近原则重新分配到两个新的叶子节点上。<br>（3）从叶子节点向上回溯修改每个非叶子节点的CF值，若叶子节点发生分裂，则在父节点中增加相应的CF元组，同样，父节点也可能需要分裂，则持续分裂直至根节点，最终如果根节点发生分类，则树的高度需要加1。</p>
<p>算法的流程:</p>
<p><img src="https://pic2.zhimg.com/80/f44accd4b4f605ee470e547e53b7c411_720w.png" alt="img"></p>
<h5 id="5-3-3算法评价"><a href="#5-3-3算法评价" class="headerlink" title="5.3.3算法评价"></a>5.3.3算法评价</h5><p>优点:</p>
<p>1.节省内存 2.速度快 4可以识别噪音点</p>
<p>缺点:</p>
<p>1.结果依赖于数据点的插入顺序  </p>
<p>2.对于非球状的簇聚类效果非常不好 </p>
<p>3.对于高维数据簇类效果不好</p>
<p>4.局部性可能会导致聚类效果不佳</p>
<h4 id="5-4-ROCK"><a href="#5-4-ROCK" class="headerlink" title="5.4 ROCK"></a>5.4 ROCK</h4><p>相似度的计算:<strong>jaccard系数,余弦相似度</strong></p>
<blockquote>
<p>Jaccard 系数    定义为A与B交集的大小与A与B并集的大小的比值，值越大，相似度越高。</p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-4f2c545cdc35a92f8c72cfd26f39ad8b_720w.png" alt="img"></p>
<blockquote>
<p>余弦相似度，是通过计算两个向量的夹角余弦值来评估他们的相似度。<br>值越接近1，就说明夹角角度越接近0°，也就是两个向量越相似，就叫做余弦相似</p>
</blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-d3b5323b80d3f1919739f33135a2954d_720w.jpg" alt="img"></p>
<p>适用于类别型数据,核心思想是利用链接作为相似性的度量,而不仅仅是依赖于距离</p>
<p>clustering categorical data by neighbor and link analysis</p>
<h4 id="5-5-CHAMELEON"><a href="#5-5-CHAMELEON" class="headerlink" title="5.5 CHAMELEON"></a>5.5 CHAMELEON</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55896918">https://zhuanlan.zhihu.com/p/55896918</a></p>
<h3 id="6-基于密度的聚类算法"><a href="#6-基于密度的聚类算法" class="headerlink" title="6. 基于密度的聚类算法"></a>6. 基于密度的聚类算法</h3><p>“密度聚类” 基于密度的聚类(density-based clustering),此类算法假设聚类结构能通过样本分布的紧密程度确定.通常情况下,从样本密度的角度来考察样本之间的可连接性,并基于<strong>可连接样本</strong>不断扩展聚类簇以获得最后的聚类结果</p>
<p>主要特点: 1. 可以处理任何形状的cluster</p>
<p>​                    2.可以处理噪音</p>
<p>​                    3.一次扫描</p>
<p>​                    4.需要设置密度参数,作为终止条件</p>
<h4 id="6-1-DBSCAN"><a href="#6-1-DBSCAN" class="headerlink" title="6.1 DBSCAN"></a>6.1 DBSCAN</h4><p>参考链接:<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6208966.html">https://www.cnblogs.com/pinard/p/6208966.html</a></p>
<p>​                    机器学习–周志华</p>
<p>==一般建议:当数据比较稠密时,而且数据集不是凸的,那么用DBSCAN 会比 K-means聚类效果很好,如果聚类效果不是很好时,不建议用DBSCAN==</p>
<h5 id="6-1-1算法介绍"><a href="#6-1-1算法介绍" class="headerlink" title="6.1.1算法介绍"></a>6.1.1算法介绍</h5><p>是基于一组”邻域”(neighborhood) 参数($\epsilon $  ,$MinPts$)来刻画样本分类的紧密程度,</p>
<p>$\epsilon$ 表示了 域的半径长度</p>
<p>$MinPts$ :成为核心点的最低密度标准</p>
<p>定义: 给定的数据集$D = { x_1,x_2…x_m }$</p>
<p><img src="cluster/3.png" alt="3"></p>
<p><img src="cluster/4.png" alt="4"></p>
<p>所以想要找到满足连接性和最大性的簇的方法就是:如果 $x$ 为核心对象,由 $x$密度可达的所有样本组成的集合$X = {  x^{‘}  \in D | x_{‘}由x密度可达 }$ 即为满足要求的簇</p>
<h5 id="6-1-2算法的主要流程"><a href="#6-1-2算法的主要流程" class="headerlink" title="6.1.2算法的主要流程"></a>6.1.2算法的主要流程</h5><p>I:寻找样本集中所有的核心对象</p>
<p>II:形成 满足连接性和最大性的所有核心对象</p>
<p><img src="cluster/5.png" alt="5"></p>
<h5 id="6-1-3算法小结"><a href="#6-1-3算法小结" class="headerlink" title="6.1.3算法小结"></a>6.1.3算法小结</h5><p>　DBSCAN的主要优点有：</p>
<p>　　　1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</p>
<p>　　　2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。</p>
<p>　　　3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</p>
<p>  DBSCAN的主要缺点有：</p>
<p>　　　　1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</p>
<p>　　　2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的$KD$树或者球树进行规模限制来改进。</p>
<p>　　　3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵϵ，邻域样本数阈值$MinPts$联合调参，不同的参数组合对最后的聚类效果有较大影响。</p>
<h4 id="6-2-OPTICS"><a href="#6-2-OPTICS" class="headerlink" title="6.2 OPTICS"></a>6.2 OPTICS</h4><p>该算法是DBSCAN算法的改进(相关概念延续上面)</p>
<h5 id="6-2-1-定义"><a href="#6-2-1-定义" class="headerlink" title="6.2.1 定义"></a>6.2.1 定义</h5><p><img src="cluster/6.png" alt="6"></p>
<p>说明:如果当x为核心点时,</p>
<p>$$rd(y,x) =\begin{cases} d(x,y) \qquad 当y不是x邻域中的点\ cd(x) \qquad 当y是邻域中的点时,为x的核心距离   \end{cases}$$</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/LoveCarpenter/article/details/85049135">https://blog.csdn.net/LoveCarpenter/article/details/85049135</a></p>
<p><a target="_blank" rel="noopener" href="https://bacterous.github.io/2018/01/06/OPTICS%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/#%E5%AE%9A%E4%B9%89">https://bacterous.github.io/2018/01/06/OPTICS%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/#%E5%AE%9A%E4%B9%89</a></p>
<h4 id="6-3-DENCLUE"><a href="#6-3-DENCLUE" class="headerlink" title="6.3 DENCLUE"></a>6.3 DENCLUE</h4><h4 id="6-4-CLIQUE"><a href="#6-4-CLIQUE" class="headerlink" title="6.4 CLIQUE"></a>6.4 CLIQUE</h4><h3 id="7-网格聚类法"><a href="#7-网格聚类法" class="headerlink" title="7.网格聚类法"></a>7.网格聚类法</h3><ol>
<li><p>STING (a STatistical INformation Grid approach) by Wang, Yang<br>and Muntz (1997)</p>
</li>
<li><p>WaveCluster by Sheikholeslami, Chatterjee, and Zhang (VLDB’98)</p>
<p>​    A multi-resolution clustering approach using wavelet method</p>
</li>
<li><p>CLIQUE: Agrawal, et al. (SIGMOD’98)On high-dimensional data (thus put in the section of clustering high-dimensional data</p>
</li>
</ol>
<h3 id="参考链接🔗"><a href="#参考链接🔗" class="headerlink" title="参考链接🔗"></a>参考链接🔗</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104355127">https://zhuanlan.zhihu.com/p/104355127</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="الصفحة السابقة"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="الصفحة التالية"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sherry Wang</span>
</div>
  <div class="powered-by">تطبيق الموقع <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
