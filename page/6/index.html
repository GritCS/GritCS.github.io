<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="记录点滴成长">
<meta property="og:type" content="website">
<meta property="og:title" content="Think World">
<meta property="og:url" content="http://example.com/page/6/index.html">
<meta property="og:site_name" content="Think World">
<meta property="og:description" content="记录点滴成长">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Sherry Wang">
<meta property="article:tag" content="computer vision, DeepLearning ,MachineLearning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Think World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Think World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">从个人成长角度来说，从经历中学点什么总是重要的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sherry Wang"
      src="/images/aa.webp">
  <p class="site-author-name" itemprop="name">Sherry Wang</p>
  <div class="site-description" itemprop="description">记录点滴成长</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/26/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/26/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" class="post-title-link" itemprop="url">随机梯度下降</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-26 09:41:39" itemprop="dateCreated datePublished" datetime="2020-08-26T09:41:39+08:00">2020-08-26</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:06:33" itemprop="dateModified" datetime="2021-07-25T09:06:33+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_09-57.png" alt="2020-08-26_09-57"></p>
<p>梯度是方向导数最大的地方</p>
<h3 id="利用SGD深度学习的一般步骤"><a href="#利用SGD深度学习的一般步骤" class="headerlink" title="利用SGD深度学习的一般步骤"></a>利用SGD深度学习的一般步骤</h3><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-05.png" alt="2020-08-26_10-05"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-21.png" alt="2020-08-26_10-21"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-22.png" alt="2020-08-26_10-22"></p>
<h3 id="影响优化器的因素"><a href="#影响优化器的因素" class="headerlink" title="影响优化器的因素"></a>影响优化器的因素</h3><p>每个都是一个研究方向，这里只是简单列出，只有有机会会分专题详细学习</p>
<h4 id="梯度问题"><a href="#梯度问题" class="headerlink" title="梯度问题"></a>梯度问题</h4><h6 id="局部最小值"><a href="#局部最小值" class="headerlink" title="局部最小值"></a>局部最小值</h6><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-26.png" alt="2020-08-26_10-26"></p>
<h6 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h6><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-31.png" alt="2020-08-26_10-31"></p>
<h4 id="权重初始值"><a href="#权重初始值" class="headerlink" title="权重初始值"></a>权重初始值</h4><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-34.png" alt="2020-08-26_10-34"></p>
<p>不仅仅课程中所提到的问题：由于之后网络层次比较深刻，所以会出现<strong>梯度消失</strong>或者<strong>梯度爆炸</strong>的问题</p>
<p>常见的初始化方法：[还没有彻底的理解，先学习一个框架]</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/80025785?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param">https://blog.csdn.net/u012328159/article/details/80025785?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62850258">https://zhuanlan.zhihu.com/p/62850258</a></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-35.png" alt="2020-08-26_10-35"></p>
<h4 id="learning-rate"><a href="#learning-rate" class="headerlink" title="learning rate"></a>learning rate</h4><p>逐步衰减;过大，可能会出现震荡，不会达到局部最优解;过小，优化的速度会很慢</p>
<h4 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h4><p>动量：用来逃出局部最优解</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_11-04.png" alt="2020-08-26_11-04"></p>
<h2 id="激活函数与loss的梯度计算"><a href="#激活函数与loss的梯度计算" class="headerlink" title="激活函数与loss的梯度计算"></a>激活函数与loss的梯度计算</h2><p>注意！！！：梯度是向量，这是我之前一致都不大注意的点，将它与导数混淆</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_11-16.png" alt="2020-08-26_11-16"></p>
<h3 id="激活函数的概念"><a href="#激活函数的概念" class="headerlink" title="激活函数的概念"></a>激活函数的概念</h3><p>灵感来自 青蛙的神经元的结构–一个阈值函数：神经元并不是各个输入的加权求和而是只有大于某个阈值之后才会输出，输出值是固定的值</p>
<p>早期的激活函数</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_11-22.png" alt="2020-08-26_11-22"></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Doris12138lucky/article/details/104375745">之前自己总结有各种激活函数</a>：</p>
<p>课程里所提到的几个简单的激活函数，以及对应的导数</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-14.png" alt="2020-08-26_16-14"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-15.png" alt="2020-08-26_16-15"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-15_1.png" alt="2020-08-26_16-15_1"></p>
<h4 id="在pytorch相对应的函数"><a href="#在pytorch相对应的函数" class="headerlink" title="在pytorch相对应的函数"></a>在pytorch相对应的函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.sigmoid()</span><br><span class="line"></span><br><span class="line">torch.tanh()</span><br><span class="line"></span><br><span class="line">torch.nn.functional.relu()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="常见的loss函数"><a href="#常见的loss函数" class="headerlink" title="常见的loss函数"></a>常见的loss函数</h3><p>mean-square-error</p>
<p>Cross-Entropy-loss</p>
<p>Softmax</p>
<h4 id="相关导数推导"><a href="#相关导数推导" class="headerlink" title="相关导数推导"></a>相关导数推导</h4><h6 id="mse"><a href="#mse" class="headerlink" title="mse"></a>mse</h6><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-20_2.png" alt="2020-08-26_16-20_2"></p>
<h6 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h6><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-20.png" alt="2020-08-26_16-20"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-20_1.png" alt="2020-08-26_16-20_1"></p>
<h3 id="pytorch中的自动求导"><a href="#pytorch中的自动求导" class="headerlink" title="pytorch中的自动求导"></a>pytorch中的自动求导</h3><h4 id="torch-autograd-grad"><a href="#torch-autograd-grad" class="headerlink" title="torch.autograd.grad()"></a><code>torch.autograd.grad()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.rand(<span class="number">3</span>) <span class="comment"># [batch, class_num]</span></span><br><span class="line">a.requires_grad_()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">p = F.softmax(a,dim=<span class="number">0</span>)<span class="comment">#在第0维上，也就是 class_num上,定义损失函数</span></span><br><span class="line"><span class="built_in">print</span>(p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法2：</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.grad(p[<span class="number">0</span>],[a],retain_graph= <span class="literal">True</span>)) <span class="comment">#单个计算每个损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.grad(p[<span class="number">1</span>],[a],retain_graph= <span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.grad(p[<span class="number">2</span>],[a],retain_graph= <span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([0.5666, 0.6252, 0.9636], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor([0.2819, 0.2989, 0.4192], grad_fn=&lt;SoftmaxBackward&gt;)</span></span><br><span class="line"><span class="string">(tensor([ 0.2024, -0.0842, -0.1182]),)</span></span><br><span class="line"><span class="string">(tensor([-0.0842,  0.2096, -0.1253]),)</span></span><br><span class="line"><span class="string">(tensor([-0.1182, -0.1253,  0.2435]),)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="loss-backword"><a href="#loss-backword" class="headerlink" title="loss.backword()"></a><code>loss.backword()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.rand(<span class="number">3</span>) <span class="comment"># [batch, class_num]</span></span><br><span class="line">a.requires_grad_()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">p = F.softmax(a,dim=<span class="number">0</span>)<span class="comment">#在第0维上，也就是 class_num上,定义损失函数</span></span><br><span class="line"><span class="built_in">print</span>(p)</span><br><span class="line"></span><br><span class="line"><span class="comment">##p.backward() 报错 ，因为此时p为向量，没有办法进行求导</span></span><br><span class="line"><span class="comment"># pytorch: grad can be implicitly created only for scalar outputs</span></span><br><span class="line"><span class="comment">##只能对单个y进行自动求导，</span></span><br><span class="line"><span class="comment">#所以单独列开，同时由于p[0]-p[2]都使用同一个计算图，</span></span><br><span class="line"><span class="comment"># 但是pytorch的计算图如果没有显式声明要保存，计算一次之后会作废，所以要置retain_graph=True</span></span><br><span class="line"><span class="comment"># retain_graph 有效次数是1次，即如果下次还需要用到计算图，还是需要置为True</span></span><br><span class="line"></span><br><span class="line">p[<span class="number">0</span>].backward(retain_graph =<span class="literal">True</span>)</span><br><span class="line">p[<span class="number">1</span>].backward(retain_graph =<span class="literal">True</span>)</span><br><span class="line">p[<span class="number">2</span>].backward(retain_graph =<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.grad) </span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([0.1281, 0.9067, 0.9391], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor([0.1842, 0.4013, 0.4145], grad_fn=&lt;SoftmaxBackward&gt;)</span></span><br><span class="line"><span class="string">tensor([1.4901e-08, 4.4703e-08, 1.4901e-08])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="两种方法对比"><a href="#两种方法对比" class="headerlink" title="两种方法对比"></a>两种方法对比</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(</span><br><span class="line"></span><br><span class="line">outputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]], </span><br><span class="line"></span><br><span class="line">inputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]],</span><br><span class="line"></span><br><span class="line">grad_outputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], <span class="literal">None</span>] = <span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">retain_graph: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>, <span class="comment"># True--------the function will only return a list of gradients w.r.t the specified inputs.</span></span><br><span class="line">    																		<span class="comment"># False,  will be accumulated into their .grad attribute.</span></span><br><span class="line"></span><br><span class="line">create_graph: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line"></span><br><span class="line">only_inputs: <span class="built_in">bool</span> = <span class="literal">True</span>, <span class="comment"># True--------the function will only return a list of gradients w.r.t the specified inputs.</span></span><br><span class="line">    												<span class="comment"># False,  will be accumulated into their .grad attribute.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">allow_unused: <span class="built_in">bool</span> = <span class="literal">False</span>) → <span class="type">Tuple</span>[torch.Tensor, ...] </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.backward(</span><br><span class="line">			tensors: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]], </span><br><span class="line">			</span><br><span class="line">			grad_tensors: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], <span class="literal">None</span>] = <span class="literal">None</span>, </span><br><span class="line">			</span><br><span class="line">			retain_graph: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>, </span><br><span class="line">			</span><br><span class="line">			create_graph: <span class="built_in">bool</span> = <span class="literal">False</span>, </span><br><span class="line">			</span><br><span class="line">			grad_variables: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor],<span class="literal">None</span>] = <span class="literal">None</span>) → <span class="literal">None</span></span><br><span class="line">			</span><br><span class="line">	</span><br><span class="line">    <span class="comment">#该函数是将所有的梯度计算都累加到需要计算梯度的变量的grad属性中</span></span><br></pre></td></tr></table></figure>

<p>官方文档建议使用第一种，因为第二种存在内存泄漏问题（也不是很清楚</p>
<h4 id="遇到的bug和解决问题"><a href="#遇到的bug和解决问题" class="headerlink" title="遇到的bug和解决问题"></a>遇到的bug和解决问题</h4><p>代码背景</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.rand(<span class="number">3</span>) <span class="comment"># [batch, class_num]</span></span><br><span class="line">a.requires_grad_()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">p = F.softmax(a,dim=<span class="number">0</span>)<span class="comment">#在第0维上，也就是 class_num上,定义损失函数</span></span><br><span class="line"><span class="built_in">print</span>(p)</span><br></pre></td></tr></table></figure>

<p><strong>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</strong></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-45.png" alt="2020-08-26_16-45"></p>
<p>修改方案： retain_graph</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-46.png" alt="2020-08-26_16-46"></p>
<p><strong>pytorch: grad can be implicitly created only for scalar outputs</strong></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-49.png" alt="2020-08-26_16-49"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/25/%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/25/%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/" class="post-title-link" itemprop="url">数学运算</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-25 07:33:03" itemprop="dateCreated datePublished" datetime="2020-08-25T07:33:03+08:00">2020-08-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:12:05" itemprop="dateModified" datetime="2021-07-25T09:12:05+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h2><h3 id="1-element-cal"><a href="#1-element-cal" class="headerlink" title="1. element-cal"></a>1. element-cal</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#汇总</span></span><br><span class="line">torch.add ;  +</span><br><span class="line">torch.sub; -</span><br><span class="line">torch.div ; /</span><br><span class="line">torch.mul; *</span><br><span class="line">a.<span class="built_in">pow</span>(n) ; a**n</span><br><span class="line">a.sqrt()</span><br><span class="line">a.rsqrt()  <span class="comment">#平方根的倒数</span></span><br><span class="line">a.exp()</span><br><span class="line">a.log()</span><br><span class="line">a.log2()</span><br><span class="line">a.log10()</span><br></pre></td></tr></table></figure>





<p>利用重载后的运算符</p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_10-49.png" alt="2020-08-25_10-49"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_10-40.png" alt="2020-08-25_10-40"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_17-58.png" alt="2020-08-25_17-58"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-06.png" alt="2020-08-25_11-06"></p>
<h3 id="2-矩阵的乘除"><a href="#2-矩阵的乘除" class="headerlink" title="2.矩阵的乘除"></a>2.矩阵的乘除</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.mm()<span class="comment">#只适用于2D</span></span><br><span class="line"><span class="meta">@</span></span><br><span class="line">torch.matmul()</span><br></pre></td></tr></table></figure>



<h4 id="2-1-２D矩阵的乘除"><a href="#2-1-２D矩阵的乘除" class="headerlink" title="2.1 ２D矩阵的乘除"></a>2.1 ２D矩阵的乘除</h4><p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_10-49_1.png" alt="2020-08-25_10-49_1"></p>
<h4 id="2-2-多维矩阵的乘除"><a href="#2-2-多维矩阵的乘除" class="headerlink" title="2.2 多维矩阵的乘除"></a>2.2 多维矩阵的乘除</h4><p>实际就是支持多个矩阵对并行相乘[只计算最低的两维数据]</p>
<p><strong>broadcasting</strong>机制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在计算过程中，可以使用broadcasting对矩阵进行扩展，</span></span><br><span class="line"><span class="comment">#可以使用matmul的标准：1. 低维（1,2）数据要满足矩阵乘法的行/列数的要求，</span></span><br><span class="line"><span class="comment">#高维数据要完全匹配或者经过broadcasting机制可以匹配</span></span><br><span class="line"><span class="comment">#一维数据，求点积</span></span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">2</span>])</span><br><span class="line">b= torch.tensor([<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b).shape) <span class="comment">#计算得到标量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#只计算低维数据，高维数据完全匹配</span></span><br><span class="line">a = torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">b = torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">c = torch.matmul(a,b)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br><span class="line"><span class="comment">#broadcasting 机制</span></span><br><span class="line">b = torch.rand(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b).shape)</span><br><span class="line"></span><br><span class="line">b = torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b).shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">b = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b).shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor(6)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([4, 3, 28, 32])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([4, 3, 28, 32])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([4, 3, 28, 32])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([1, 3, 28, 32])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="2-3其他计算"><a href="#2-3其他计算" class="headerlink" title="2.3其他计算"></a>2.3其他计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a.floor()</span><br><span class="line"></span><br><span class="line">a.ceil()</span><br><span class="line"></span><br><span class="line">a.<span class="built_in">round</span>()<span class="comment">#四舍五入</span></span><br><span class="line"></span><br><span class="line">a.trunc()<span class="comment">#取整数部分</span></span><br><span class="line"></span><br><span class="line">a.frac()<span class="comment">#取小数部分</span></span><br><span class="line"></span><br><span class="line">a.clamp(<span class="built_in">min</span>,<span class="built_in">max</span>) <span class="comment">#如果a[i]&lt;min ，a[i] =min ； 如果a[i]&gt;max ，a[i]=max</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p>函数相关实例</p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_18-07.png" alt="2020-08-25_18-07"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">3</span>)*<span class="number">15</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.clamp(<span class="number">5</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[ 0.1522,  8.6243, 10.4471],</span></span><br><span class="line"><span class="string">        [ 2.0610,  8.4947, 10.7278]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[ 5.0000,  8.6243, 10.0000],</span></span><br><span class="line"><span class="string">        [ 5.0000,  8.4947, 10.0000]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h2 id="统计属性"><a href="#统计属性" class="headerlink" title="统计属性"></a>统计属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##相关函数计算</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11_12.png" alt="2020-08-25_11_12"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-19.png" alt="2020-08-25_11-19"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-25.png" alt="2020-08-25_11-25"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-26.png" alt="2020-08-25_11-26"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-54.png" alt="2020-08-25_11-54"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-59.png" alt="2020-08-25_11-59"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_12-05.png" alt="2020-08-25_12-05"></p>
<p>附录：</p>
<p>1。在某维度上argmax的理解—感觉自己对抽象几何不是特别理解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>,<span class="number">10</span>)a</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.argmax())</span><br><span class="line"><span class="built_in">print</span>(a.argmax(dim=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(a.argmax(dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 7.0244e-01, -1.8047e-01, -1.3860e+00, -2.7422e-02,  1.0189e-01,</span></span><br><span class="line"><span class="string">         -4.1896e-01,  2.3863e+00, -2.3053e-01, -1.7369e-02,  1.8899e-01],</span></span><br><span class="line"><span class="string">        [ 1.0445e-01, -5.9622e-01, -8.3666e-01, -7.0087e-01, -2.1692e-01,</span></span><br><span class="line"><span class="string">          1.2431e-01, -2.3204e-01, -5.3630e-01, -1.1655e+00,  4.6021e-01],</span></span><br><span class="line"><span class="string">        [-1.1210e+00, -4.4479e-01,  1.0892e+00, -1.1631e+00,  7.8703e-01,</span></span><br><span class="line"><span class="string">         -3.9018e-01, -6.1701e-01,  8.2018e-01,  1.2167e-01, -1.6905e+00],</span></span><br><span class="line"><span class="string">        [-1.4846e+00,  9.5270e-01,  2.2757e-01,  1.5931e+00,  2.9075e-01,</span></span><br><span class="line"><span class="string">          1.3509e+00,  1.5353e+00, -1.1048e+00,  7.0249e-04, -1.1067e-01]])</span></span><br><span class="line"><span class="string">tensor(6)</span></span><br><span class="line"><span class="string">tensor([0, 3, 2, 3, 2, 3, 0, 2, 2, 1])</span></span><br><span class="line"><span class="string">tensor([6, 9, 2, 3])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>2.在某维度上范式的计算的理解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.FloatTensor([<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>,<span class="number">7.</span>,<span class="number">8.</span>,<span class="number">9.</span>,<span class="number">10.</span>])</span><br><span class="line">b = a.view(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b.norm(<span class="number">1</span>,dim=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(b.norm(<span class="number">1</span>,dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 1.,  2.,  3.,  4.,  5.],</span></span><br><span class="line"><span class="string">        [ 6.,  7.,  8.,  9., 10.]])</span></span><br><span class="line"><span class="string">tensor([ 7.,  9., 11., 13., 15.])</span></span><br><span class="line"><span class="string">tensor([15., 40.])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="高阶操作"><a href="#高阶操作" class="headerlink" title="高阶操作"></a>高阶操作</h2><h3 id="where-语句"><a href="#where-语句" class="headerlink" title="where 语句"></a>where 语句</h3><p>代替for循环，可以利用GPU进行加速计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cond = torch.tensor([[<span class="number">0.6769</span>,<span class="number">0.7271</span>],[<span class="number">0.8884</span>,<span class="number">0.4163</span>]])</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">0.</span>,<span class="number">0.</span>],[<span class="number">0.</span>,<span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">b = torch.tensor([[<span class="number">1.</span>,<span class="number">1.</span>],[<span class="number">1.</span>,<span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.where(cond&gt;<span class="number">0.5</span>,a,b))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##同上面等效，可以体会一下语义</span></span><br><span class="line">c =torch.randn(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        <span class="keyword">if</span>(cond[i][j]&gt;<span class="number">0.5</span>):</span><br><span class="line">            c[i][j] = a[i][j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c[i][j] = b[i][j]</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1.]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="gather-语句"><a href="#gather-语句" class="headerlink" title="gather 语句"></a>gather 语句</h3><p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_18-11.png" alt="2020-08-25_18-11"></p>
<h4 id="提出的背景"><a href="#提出的背景" class="headerlink" title="提出的背景"></a>提出的背景</h4><p>可以自行体会含义</p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_17-49.png" alt="2020-08-25_17-49"></p>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_18-10.png" alt="2020-08-25_18-10"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_18-14.png" alt="2020-08-25_18-14"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/25/%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/25/%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/" class="post-title-link" itemprop="url">合并与分割</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-25 00:06:15" itemprop="dateCreated datePublished" datetime="2020-08-25T00:06:15+08:00">2020-08-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:17:08" itemprop="dateModified" datetime="2021-07-25T09:17:08+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="合并与分割"><a href="#合并与分割" class="headerlink" title="合并与分割"></a>合并与分割</h2><h3 id="1-cat"><a href="#1-cat" class="headerlink" title="1. cat"></a>1. cat</h3><p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_06-36.png" alt="2020-08-25_06-36"></p>
<p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_06-38.png" alt="2020-08-25_06-38"></p>
<p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_06-38_1.png" alt="2020-08-25_06-38_1"></p>
<h3 id="2-stack"><a href="#2-stack" class="headerlink" title="2. stack"></a>2. stack</h3><p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_06-35.png" alt="2020-08-25_06-35"></p>
<h3 id="cat-v-s-stack"><a href="#cat-v-s-stack" class="headerlink" title="cat v.s. stack"></a>cat v.s. stack</h3><p><img src="%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2/2020-08-25_07-04.png" alt="2020-08-25_07-04"></p>
<h3 id="3-split"><a href="#3-split" class="headerlink" title="3. split"></a>3. split</h3><h4 id="by-sub-len"><a href="#by-sub-len" class="headerlink" title="by sub-len"></a>by sub-len</h4><p>​    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(sub_size ,dim =dd)	<span class="comment">#拆分成若干个size都为sub_size的tensor</span></span><br><span class="line"></span><br><span class="line">torch.chunk([sub.size1,sub_size2.....],dim=d) <span class="comment">#拆分成各分布为size1,size2...的tensor</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="4-chunk"><a href="#4-chunk" class="headerlink" title="4. chunk"></a>4. chunk</h3><h4 id="by-sub-num"><a href="#by-sub-num" class="headerlink" title="by sub-num"></a>by sub-num</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(sub_num ,dim =dd)</span><br></pre></td></tr></table></figure>



<p>汇总代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">a1 = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">a2 = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">b = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">b2= torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line">b3 = torch.randn(<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c = torch.stack([a,b],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([a,b],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([a1,d],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([a2,d],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([b2,d],dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat([b3,d],dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(d.shape)</span><br><span class="line"></span><br><span class="line">n,m = d.split([<span class="number">2</span>*<span class="number">32</span>,<span class="number">4</span>*<span class="number">32</span>],dim=<span class="number">0</span>) <span class="comment">#指定每个sub-tensor的size,使用list表示，所有size的和必须等于拆分前的tensor的size</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(n.shape)</span><br><span class="line"><span class="built_in">print</span>(m.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nn,mm = d.split(<span class="number">3</span>*<span class="number">32</span>,dim=<span class="number">0</span>) <span class="comment">#如果所有长度都固定，就用一个数来表示每个sub-tensor的长度，sub-tensor的个数可以自动计算得到</span></span><br><span class="line"><span class="comment"># nn,mm = d.split(3,dim=0) #报错，如果每个sub-tensor第一维的size＝３，会返回64个sub-tensor</span></span><br><span class="line"><span class="built_in">print</span>(nn.shape)</span><br><span class="line"><span class="built_in">print</span>(mm.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nnn ,mmm = d.chunk(<span class="number">2</span>,dim=<span class="number">0</span>)<span class="comment">#拆分成两个tensor,每个tensor的长度相同为３*32</span></span><br><span class="line"><span class="built_in">print</span>(nnn.shape)</span><br><span class="line"><span class="built_in">print</span>(mmm.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/24/Broadcasting/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/24/Broadcasting/" class="post-title-link" itemprop="url">Broadcasting</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2020-08-24 23:43:31 / Modified: 16:05:46" itemprop="dateCreated datePublished" datetime="2020-08-24T23:43:31+08:00">2020-08-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="1-Broadcasting的基本准则"><a href="#1-Broadcasting的基本准则" class="headerlink" title="1.Broadcasting的基本准则"></a>1.Broadcasting的基本准则</h3><p><img src="Broadcasting/2020-08-25_00-03.png" alt="2020-08-25_00-03"></p>
<p>例子</p>
<p><img src="Broadcasting/2020-08-24_23-52.png" alt="2020-08-24_23-52"></p>
<h3 id="2-Broadcasting引入的背景"><a href="#2-Broadcasting引入的背景" class="headerlink" title="2.Broadcasting引入的背景"></a>2.Broadcasting引入的背景</h3><pre><code>1. 实际计算需求：可以允许不同shape(但又满足某种标准)的tensor可以进行计算
 2. 同时这种原则满足数学上的运算，又无需手动操作，又不会消耗很多的内存（对比repeat)
</code></pre>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/24/%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/24/%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/" class="post-title-link" itemprop="url">维度转换</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-24 16:42:48" itemprop="dateCreated datePublished" datetime="2020-08-24T16:42:48+08:00">2020-08-24</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:09:49" itemprop="dateModified" datetime="2021-07-25T09:09:49+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p><strong>数据没有被改变，改变的只有数据的理解方式</strong></p>
<h3 id="1-view-reshape"><a href="#1-view-reshape" class="headerlink" title="1. view / reshape"></a>1. view / reshape</h3><p>1.view reshape ,两个方法的用法完全相同<br>要求转换前后tensor的numel()相同即可，prod(a.size()) ==prod(b.size())</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="comment">#把每张照片打成一维数据,这个常做为－卷积网络的输入</span></span><br><span class="line">b = a.view(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="comment">#把只是关注照片中每行的特点</span></span><br><span class="line">c = a.view(<span class="number">4</span>*<span class="number">1</span>*<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#不关心像素点是来自哪里，只是关注图片中不同像素点的不同</span></span><br><span class="line">d = a.view(<span class="number">4</span>*<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(d.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据的存储／维度顺序非常重要，但是这里view 和reshape函数在转换时把这一信息丢掉了，无额外记录之前的形状，无法恢复</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([4, 1, 28, 28])</span></span><br><span class="line"><span class="string">torch.Size([4, 784])</span></span><br><span class="line"><span class="string">torch.Size([112, 28])</span></span><br><span class="line"><span class="string">torch.Size([4, 28, 28])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="2-squeeze-v-s-unsqueeze-维度"><a href="#2-squeeze-v-s-unsqueeze-维度" class="headerlink" title="2.squeeze v.s. unsqueeze-维度"></a>2.squeeze v.s. unsqueeze-维度</h3><h4 id="2-1-unsqueeze"><a href="#2-1-unsqueeze" class="headerlink" title="2.1 unsqueeze()"></a>2.1 unsqueeze()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#unsqueeze( input , dim ) -&gt;Tensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#####参数说明：</span></span><br><span class="line"><span class="comment">#input (Tensor) – the input tensor.</span></span><br><span class="line"><span class="comment"># dim (int) – the index at which to insert the singleton dimension,dim的取值范围[- input.dim() -1 , input.dim()+1 ]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果dim为负数，则　dim = dim + input.dim()+1 将其转换成正数</span></span><br><span class="line"><span class="comment">#如果dim为整数，则　新增的维度添加到　dim的前面</span></span><br><span class="line"><span class="comment">#维度上的元素个数只有一个，所以数据规模没有改变，改变的只是数据的含义</span></span><br></pre></td></tr></table></figure>

<p><strong>图示</strong></p>
<p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/1.jpg" alt="1"></p>
<p>可以看出来维度变换之后，数据的理解方式不同，要好好体会</p>
<p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_19-08.png" alt="2020-08-24_19-08"></p>
<p><strong>数据处理的实例应用</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">b = torch.rand(<span class="number">32</span>)</span><br><span class="line">f = torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">14</span>,<span class="number">14</span>)</span><br><span class="line">b = b.unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="comment">#方便之后 f+b的计算。b 即bias，相当于给每个channel上的所有像素增加一个偏置</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output : torch.Size([1, 32, 1, 1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">##常用的：　就是如果向后面插就使用　unsqueeze(-1)多次插入，就多次写</span></span><br><span class="line"><span class="comment">#向开头插入就，直接调用 unsqueeze(0)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="2-2-squeeze"><a href="#2-2-squeeze" class="headerlink" title="2.2 squeeze"></a>2.2 squeeze</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#torch.squeeze(input , dim =None ,Out = None) -&gt;tensor</span></span><br><span class="line"><span class="comment">#当不传参数时，会将input所有元素只有一个的维度给去掉</span></span><br><span class="line"><span class="comment">#传参数dim，在指定维度上且维度只有一个元素时，将挤压掉该维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#b.shape = torch.Size([1,32,1,1])</span></span><br><span class="line"><span class="built_in">print</span>(b.squeeze(-<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(b.squeeze(<span class="number">1</span>).shape) <span class="comment">#无效</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([1, 32, 1])</span></span><br><span class="line"><span class="string">torch.Size([1, 32, 1, 1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-Expand-repeat-行"><a href="#3-Expand-repeat-行" class="headerlink" title="3.Expand /repeat-行"></a>3.Expand /repeat-行</h3><p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_19-29.png" alt="2020-08-24_19-29"></p>
<h4 id="3-1-expand"><a href="#3-1-expand" class="headerlink" title="3.1 expand"></a>3.1 expand</h4><p>参数是广播的目标tensor的shape</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">14</span>,<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">b = torch.rand(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">b = b.unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">b1 = b.expand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">14</span>,<span class="number">14</span>)<span class="comment">#之后b2即可以与a进行运算</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b1.shape)</span><br><span class="line"><span class="comment"># b2 = b.expand(4,33,14,14) 会报错 The expanded size of the tensor (33) must match the existing size (32) at non-singleton dimension 1</span></span><br><span class="line"></span><br><span class="line">b2 = b.expand(-<span class="number">1</span>,-<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) <span class="comment">#-1表示不想对该维度进行修改</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b2.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br></pre></td></tr></table></figure>

<h4 id="3-2-repeat－－不建议使用"><a href="#3-2-repeat－－不建议使用" class="headerlink" title="3.2 repeat－－不建议使用"></a>3.2 repeat－－不建议使用</h4><p>进行repeat之后，可能会开辟新的空间去保存repeat的结果，会降低效率</p>
<p><em>参数传递是每个维度上要重复的次数，需要自己计算</em></p>
<p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_19-43.png" alt="2020-08-24_19-43"></p>
<h3 id="4-转置操作-t（只能适用于2D操作）"><a href="#4-转置操作-t（只能适用于2D操作）" class="headerlink" title="4.转置操作　.t（只能适用于2D操作）"></a>4.转置操作　.t（只能适用于2D操作）</h3><p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_19-44.png" alt="2020-08-24_19-44"></p>
<h3 id="5-Transpose"><a href="#5-Transpose" class="headerlink" title="5.Transpose()"></a>5.Transpose()</h3><p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_20-18.png" alt="2020-08-24_20-18"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转换效果图</span></span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = torch.transpose(a,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">c = torch.transpose(a,<span class="number">1</span>,<span class="number">2</span>).contiguous().view(<span class="number">2</span>,<span class="number">2</span>*<span class="number">3</span>).view(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">d = torch.transpose(a,<span class="number">1</span>,<span class="number">2</span>).contiguous().view(<span class="number">2</span>,<span class="number">2</span>*<span class="number">3</span>).view(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[-0.2507, -0.4298],</span></span><br><span class="line"><span class="string">         [-2.8421,  0.9166],</span></span><br><span class="line"><span class="string">         [ 3.2584,  1.1366]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.8887, -0.5606],</span></span><br><span class="line"><span class="string">         [ 1.3798, -0.5037],</span></span><br><span class="line"><span class="string">         [ 0.9862,  0.8550]]])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([[[-0.2507, -2.8421,  3.2584],</span></span><br><span class="line"><span class="string">         [-0.4298,  0.9166,  1.1366]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.8887,  1.3798,  0.9862],</span></span><br><span class="line"><span class="string">         [-0.5606, -0.5037,  0.8550]]])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([[[-0.2507, -2.8421],</span></span><br><span class="line"><span class="string">         [ 3.2584, -0.4298],</span></span><br><span class="line"><span class="string">         [ 0.9166,  1.1366]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.8887,  1.3798],</span></span><br><span class="line"><span class="string">         [ 0.9862, -0.5606],</span></span><br><span class="line"><span class="string">         [-0.5037,  0.8550]]])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([[[-0.2507, -0.4298],</span></span><br><span class="line"><span class="string">         [-2.8421,  0.9166],</span></span><br><span class="line"><span class="string">         [ 3.2584,  1.1366]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.8887, -0.5606],</span></span><br><span class="line"><span class="string">         [ 1.3798, -0.5037],</span></span><br><span class="line"><span class="string">         [ 0.9862,  0.8550]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="6-permute"><a href="#6-permute" class="headerlink" title="6.permute"></a>6.permute</h3><p>permute也会打乱内存的顺序，需要调用coutigious函数</p>
<p>permute底层是调用多次transpose()实现的</p>
<p><img src="%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2/2020-08-24_20-24.png" alt="2020-08-24_20-24"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上面遮挡住的命令是</span></span><br><span class="line">b.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).shape</span><br><span class="line"><span class="comment">#out : torch.Size([4,28,32,3])</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/24/pytroch%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/24/pytroch%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87/" class="post-title-link" itemprop="url">pytroch索引与切片</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-24 07:33:55" itemprop="dateCreated datePublished" datetime="2020-08-24T07:33:55+08:00">2020-08-24</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 08:51:19" itemprop="dateModified" datetime="2021-07-25T08:51:19+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="1-pytorch风格的索引－维度选择"><a href="#1-pytorch风格的索引－维度选择" class="headerlink" title="1.pytorch风格的索引－维度选择"></a>1.pytorch风格的索引－维度选择</h3><p>跟据Tensor的shape,从前往后索引，依次在每个维度上进行索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>])</span><br><span class="line"><span class="built_in">print</span>( a[<span class="number">0</span>].shape)<span class="comment">#和Ｃ＋＋等高级问题类似，a[0]表示选择第一章图片</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>,<span class="number">0</span>].shape)<span class="comment">#选择第一章图片的第一个通道</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>])<span class="comment">#选择某个像素点，</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="string">torch.Size([28, 28])</span></span><br><span class="line"><span class="string">tensor(0.1076) #是维度为０的元素</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="2-python风格的索引-特定维度上"><a href="#2-python风格的索引-特定维度上" class="headerlink" title="2.python风格的索引-特定维度上"></a>2.python风格的索引-特定维度上</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">基本结构：</span></span><br><span class="line"><span class="string">eg : a[0,0,0:28:2,0]</span></span><br><span class="line"><span class="string">#0:28:2－－在某个维度上，start: end[:offset]  其中start:end:offset 表示从start开始，到end结束，其中不包括end，每次步长为offset;当省略offset时，表示offset=1</span></span><br><span class="line"><span class="string"># 0:28其实等价与 0:28:1这种结构</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">-1:表示倒数第１个元素，-2表示倒数第２个元素</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 譬如：4张图片，每张三个通道，每个通道28行28列的像素</span></span><br><span class="line">a = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在第一个维度上取后0和1，等同于取第一、第二张图片</span></span><br><span class="line"><span class="built_in">print</span>(a[:<span class="number">2</span>].shape)  </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取0，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第一个通道</span></span><br><span class="line"><span class="built_in">print</span>(a[:<span class="number">2</span>, :<span class="number">1</span>, :, :].shape)  </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取1,2，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第二个通道与第三个通道</span></span><br><span class="line"><span class="built_in">print</span>(a[:<span class="number">2</span>, <span class="number">1</span>:, :, :].shape) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取1,2，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第二个通道与第三个通道</span></span><br><span class="line"><span class="built_in">print</span>(a[:<span class="number">2</span>, -<span class="number">2</span>:, :, :].shape)  </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 使用step隔行采样</span></span><br><span class="line"><span class="comment"># 在第一、第二维度取所有元素，在第三、第四维度步长为２采样</span></span><br><span class="line"><span class="comment"># 等同于所有图片所有通道的行列每个一行或者一列采样</span></span><br><span class="line"><span class="comment"># 注意：下面的代码不包括28</span></span><br><span class="line"><span class="built_in">print</span>(a[:, :, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>].shape) </span><br><span class="line"><span class="built_in">print</span>(a[:, :, ::<span class="number">2</span>, ::<span class="number">2</span>].shape)  <span class="comment"># 等同于上面语句</span></span><br></pre></td></tr></table></figure>



<h3 id="3-选择特定的元素"><a href="#3-选择特定的元素" class="headerlink" title="3. 选择特定的元素"></a>3. 选择特定的元素</h3><h4 id="3-1-index-select"><a href="#3-1-index-select" class="headerlink" title="3.1 index_select"></a>3.1 index_select</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.index_select(input, dim, index, out=None) → Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">input (Tensor) – the input tensor.</span></span><br><span class="line"><span class="string">dim (int) – the dimension in which we index</span></span><br><span class="line"><span class="string">index (LongTensor) – the 1-D tensor containing the indices to index；是list转换成的一维tensor</span></span><br><span class="line"><span class="string">out (Tensor, optional) – the output tensor.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>((a.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">2</span>]))).shape)</span><br><span class="line"><span class="comment">#选取第一个维度上的　索引为0,2的tensor</span></span><br><span class="line"><span class="comment">#第二个参数是将list [0,2]转换成 tensor</span></span><br><span class="line"><span class="built_in">print</span>((a.index_select(<span class="number">2</span>,torch.arange(<span class="number">8</span>))).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#选择第二维度上的前８个元素</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([4, 3, 28, 28])</span></span><br><span class="line"><span class="string">torch.Size([2, 3, 28, 28])</span></span><br><span class="line"><span class="string">torch.Size([4, 3, 8, 28])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>索引效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>((a.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">2</span>]))))</span><br><span class="line"><span class="built_in">print</span>((a.index_select(<span class="number">1</span>,torch.arange(<span class="number">2</span>))))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.9071, 0.5350, 0.4018],</span></span><br><span class="line"><span class="string">        [0.9565, 0.9739, 0.7234],</span></span><br><span class="line"><span class="string">        [0.1984, 0.3562, 0.8078]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0.9071, 0.5350, 0.4018],</span></span><br><span class="line"><span class="string">        [0.1984, 0.3562, 0.8078]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0.9071, 0.5350],</span></span><br><span class="line"><span class="string">        [0.9565, 0.9739],</span></span><br><span class="line"><span class="string">        [0.1984, 0.3562]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-2-masked-select"><a href="#3-2-masked-select" class="headerlink" title="3.2 masked_select()"></a>3.2 masked_select()</h4><h6 id="方式一"><a href="#方式一" class="headerlink" title="方式一"></a>方式一</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">函数说明</span></span><br><span class="line"><span class="string">torch.masked_select( input, mask ,out = None ) -&gt; 张量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据掩码张量mask中的二元值(0,1)，取输入张量中的指定项( mask为一个 ByteTensor)，将取值返回到一个新的1D张量－－－是打平的张量; 张量 mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。</span></span><br><span class="line"><span class="string">注意： 返回的张量不与原始张量共享内存空间。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">mask2 = a.ge(<span class="number">0.3</span>)</span><br><span class="line">mask3 = a.le(<span class="number">0.7</span>)</span><br><span class="line"><span class="built_in">print</span>(mask2)</span><br><span class="line"><span class="built_in">print</span>(a[mask2])</span><br><span class="line"><span class="built_in">print</span>(a[mask3])</span><br><span class="line"><span class="built_in">print</span>(torch.masked_select(a,mask2))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[-0.7769, -0.0803, -0.4235, -0.3562],</span></span><br><span class="line"><span class="string">        [-0.4744,  1.2078,  0.6371, -0.6981],</span></span><br><span class="line"><span class="string">        [-1.1653, -0.3432, -2.3189,  0.1708]])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([[ True, False,  True,  True],</span></span><br><span class="line"><span class="string">        [False, False, False, False],</span></span><br><span class="line"><span class="string">        [ True, False, False, False]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([1.2078, 0.6371])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([-0.7769, -0.0803, -0.4235, -0.3562, -0.4744,  0.6371, -0.6981, -1.1653,</span></span><br><span class="line"><span class="string">        -0.3432, -2.3189,  0.1708])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([1.2078, 0.6371])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h6 id="方式二-不使用a-ge-a-le-方法"><a href="#方式二-不使用a-ge-a-le-方法" class="headerlink" title="方式二　不使用a.ge() \ a.le()方法"></a>方式二　不使用a.ge() \ a.le()方法</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">mask1 = torch.ByteTensor((a&gt;<span class="number">0.5</span>).byte())　</span><br><span class="line"><span class="built_in">print</span>(mask1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[mask1])</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">输出：</span></span><br><span class="line"><span class="string">tensor([[-1.8973, -0.2158,  1.0196, -0.2119],</span></span><br><span class="line"><span class="string">        [-0.2365,  0.4743, -1.2473, -0.6554],</span></span><br><span class="line"><span class="string">        [ 0.3040, -0.0906, -0.8517, -0.2679]])</span></span><br><span class="line"><span class="string">tensor([[0, 0, 1, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0]], dtype=torch.uint8)</span></span><br><span class="line"><span class="string">tensor([1.0196])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">提醒一下：不是特别建议使用这种方式</span></span><br><span class="line"><span class="string"> UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"> 感觉自己对python和pytorch基本类型的转换有些模糊　－－－torch.bool　与　torch.uint8</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-2-torch-take"><a href="#3-2-torch-take" class="headerlink" title="3.2 torch.take()"></a>3.2 torch.take()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">使用打平的index进行索引</span></span><br><span class="line"><span class="string">torch.take(input, index) -&gt;Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">index(Long Tensor) 把input Tensor看作一维Tensor对每个元素的索引</span></span><br><span class="line"><span class="string">输出:一个一维Tensor</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = torch.take(a, torch.tensor([<span class="number">1</span>,<span class="number">5</span>,<span class="number">7</span>]))</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">输出：</span></span><br><span class="line"><span class="string">tensor([[-0.3353,  1.2220,  0.7055,  0.4678],</span></span><br><span class="line"><span class="string">        [-0.4759,  0.5173,  1.1912, -0.8545],</span></span><br><span class="line"><span class="string">        [-1.2037,  0.5052,  0.0388, -0.3160]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([ 1.2220,  0.5173, -0.8545])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/" class="post-title-link" itemprop="url">分类问题的引入</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-23 15:54:03" itemprop="dateCreated datePublished" datetime="2020-08-23T15:54:03+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-08-24 08:45:00" itemprop="dateModified" datetime="2020-08-24T08:45:00+08:00">2020-08-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h2><p><strong>MNIST　数据集</strong></p>
<p>由不同风格的手写数字组成(0-9)</p>
<p>每个数字都有7000张，每张图片都是28*28的灰度图片</p>
<p>训练时：将训练集和测试集分为60k Vs 10k</p>
<h3 id="1-No-deeping-learning"><a href="#1-No-deeping-learning" class="headerlink" title="1. No deeping learning"></a>1. No deeping learning</h3><p>X:[1….28*28]</p>
<p>每个点都是０－１，表示该像素点的灰度值</p>
<p>关键点：参数的维度定义，以及每层转换的含义</p>
<p><img src="%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/2020-08-22_16-28.png" alt="2020-08-22_16-28"></p>
<p><img src="%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/2020-08-22_16-55.png" alt="2020-08-22_16-55"></p>
<p><img src="%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BC%95%E5%85%A5/2020-08-22_16-57.png" alt="2020-08-22_16-57"></p>
<p>2.代码实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-encodng: utf-8-*-</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">@File: minist.py.py</span></span><br><span class="line"><span class="string">@Contact: 2257925767@qq.com</span></span><br><span class="line"><span class="string">@Author:wangyu</span></span><br><span class="line"><span class="string">@Version:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@Desciption:</span></span><br><span class="line"><span class="string">        手写数字识别的核心代码</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        还存在一些问题没有解决－－－自己得到的数据比老师的代码迭代的次数要少很多</span></span><br><span class="line"><span class="string">    env: </span></span><br><span class="line"><span class="string">        pytorch 1.3.1</span></span><br><span class="line"><span class="string">@DateTime: 2020/8/22下午5:04 </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F <span class="comment">#常见的激活函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> plot_image, plot_curve, one_hot</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1 . load dataset,采用向量并行</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">512</span></span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    torchvision.datasets.MNIST(<span class="string">&quot;mnist_data&quot;</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>,</span><br><span class="line">                               transform= torchvision.transforms.Compose([torchvision.transforms.ToTensor(),<span class="comment">#将numby格式数据转成pytorch</span></span><br><span class="line">                                                                            torchvision.transforms.Normalize(</span><br><span class="line">                                                                                (<span class="number">0.1307</span>,),(<span class="number">0.3081</span>,)), <span class="comment">#对像素点的灰度值进行正则化，会提高优化效率</span></span><br><span class="line">                                                                          ])),batch_size = batch_size,shuffle = <span class="literal">True</span>)<span class="comment">#随机打散</span></span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    torchvision.datasets.MNIST(<span class="string">&quot;mnist_data/&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,</span><br><span class="line">                               transform= torchvision.transforms.Compose([torchvision.transforms.ToTensor(),<span class="comment">#将numby格式数据转成pytorch</span></span><br><span class="line">                                                                            torchvision.transforms.Normalize(</span><br><span class="line">                                                                                (<span class="number">0.1307</span>,),(<span class="number">0.3081</span>,)), <span class="comment">#对像素点的灰度值进行正则化，会提高优化效率</span></span><br><span class="line">                                                                          ])),batch_size = batch_size,shuffle = <span class="literal">False</span>)<span class="comment">#随机打散</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x,y = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"><span class="built_in">print</span>(x.shape,y.shape,x.<span class="built_in">min</span>(),x.<span class="built_in">max</span>())</span><br><span class="line">plot_image(x,y,<span class="string">&#x27;image_test&#x27;</span>) <span class="comment">#检查数据集</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span><span class="comment">#初始化函数:搭建网络结构</span></span><br><span class="line">        <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#wx+b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">256</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>,<span class="number">64</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">64</span>,<span class="number">10</span>)<span class="comment">#28*28 =&gt; 256 =&gt;64这个是随机确定的,最后一层的输出是由分类的种类数决定的</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span><span class="comment">#网络的计算过程</span></span><br><span class="line">        <span class="comment"># x :[batch_size,1,28,28] 1:表示只有一个通道</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#h1= relu(xw1+b1)</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="comment">#h1=relu(h1w2+b2)</span></span><br><span class="line">        x=F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment">#h3=h2w3+b3 --这里没有使用激活函数，只是简单输出</span></span><br><span class="line">        x=self.fc3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()<span class="comment">#实例化一个net</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义优化器</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(),lr=<span class="number">0.01</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">train_loss=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_idx , (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader): <span class="comment">#迭代一次数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.调整数据的尺寸，构建网络，通过网络计算预测值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#x: [b,1,28,28], y:[512]</span></span><br><span class="line">        <span class="comment"># [b,1,28,28] =&gt; [b,784]</span></span><br><span class="line">        x =x.view(x.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>) <span class="comment">#size[0]表示batch_size</span></span><br><span class="line">        <span class="comment">#=&gt;[b,10]</span></span><br><span class="line">        out = net(x) <span class="comment">#经过网络计算出来的值</span></span><br><span class="line">        y_onehot = one_hot(y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.定义梯度</span></span><br><span class="line">        <span class="comment">#loss = mse_loss(out,y_onehot) 欧式距离</span></span><br><span class="line">        loss = F.mse_loss(out,y_onehot)</span><br><span class="line"><span class="comment">#3.梯度清０</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.梯度计算过程</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.参数更新</span></span><br><span class="line">        <span class="comment">#w&#x27;= w-lr*grad</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_loss.append(loss.item())<span class="comment">#loss: tensor =&gt; numpy</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(epoch,batch_idx,loss.item())</span><br><span class="line"></span><br><span class="line">plot_curve(train_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#get optimal[w1,b2,w2,b2,w3,b3]</span></span><br><span class="line"></span><br><span class="line">total_correct = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> test_loader:</span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">    out = net(x)</span><br><span class="line">    <span class="comment">#out = net(x)</span></span><br><span class="line">    pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    correct = pred.eq(y).<span class="built_in">sum</span>().<span class="built_in">float</span>().item()</span><br><span class="line">    total_correct+= correct</span><br><span class="line"></span><br><span class="line">total_num = <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">acc=total_correct/total_num</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test acc:&#x27;</span>,acc)</span><br><span class="line"></span><br><span class="line">x,y = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line">out = net(x.view(x.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">plot_image(x,pred,<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-encodng: utf-8-*-</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">@File: util.py.py</span></span><br><span class="line"><span class="string">@Contact: 2257925767@qq.com</span></span><br><span class="line"><span class="string">@Author:wangyu</span></span><br><span class="line"><span class="string">@Version:</span></span><br><span class="line"><span class="string">        手写数字识别的工具文件</span></span><br><span class="line"><span class="string">@Desciption:</span></span><br><span class="line"><span class="string">    env: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@DateTime: 2020/8/22下午5:04 </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span>    matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_curve</span>(<span class="params">data</span>):</span><span class="comment">#计算训练曲线</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(data)), data, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;value&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;step&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;value&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span>(<span class="params">img, label, name</span>):</span><span class="comment">#展现出识别结果</span></span><br><span class="line"></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        plt.subplot(<span class="number">2</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        plt.imshow(img[i][<span class="number">0</span>]*<span class="number">0.3081</span>+<span class="number">0.1307</span>, cmap=<span class="string">&#x27;gray&#x27;</span>, interpolation=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(name, label[i].item()))</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span>(<span class="params">label, depth=<span class="number">10</span></span>):</span><span class="comment">#完成one-hot编码</span></span><br><span class="line">    out = torch.zeros(label.size(<span class="number">0</span>), depth)</span><br><span class="line">    idx = torch.LongTensor(label).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    out.scatter_(dim=<span class="number">1</span>, index=idx, value=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">简单的回归问题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-23 15:53:54" itemprop="dateCreated datePublished" datetime="2020-08-23T15:53:54+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 08:59:29" itemprop="dateModified" datetime="2021-07-25T08:59:29+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="简单的回归问题"><a href="#简单的回归问题" class="headerlink" title="简单的回归问题"></a>简单的回归问题</h2><h3 id="１．从简单到复杂"><a href="#１．从简单到复杂" class="headerlink" title="１．从简单到复杂"></a>１．从简单到复杂</h3><p>==梯度下降算法==：梯度是深度学习的核心</p>
<p>１.简单的小例子</p>
<p><img src="%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/2020-08-22_06-44.png" alt="2020-08-22_06-44"></p>
<p>２.问题的迭代</p>
<p>求解$y = wx+b$　二元一次方程$w, b$的值</p>
<p>　　i.中学阶段：　求解二元一次方程的方法－－&gt;消元法(利用Closed Form Solution精确求得ｗ,b的解)</p>
<p>​        ii.引入噪音(noise：），模拟现实情况，我们的目标并不是为了得到一个精确解，而是得到一个从经验上精度可行的近似解即可，</p>
<p>解决方法：需要更多的样本点＋之后采用梯度下降算法求解</p>
<p>​        iii.首先构造一个函数(均方差)，因为梯度下降算法是求解极值的算法</p>
<p>​<br>$$<br>loss = (y-wx-b)^2<br>$$<br>$loss$方程值最小所对应的$w,b$ 可以近似认为是二元一次方程的解</p>
<p>​        注:在实际问题中，首先根据样本分布的情况，选择它可能对应的方程（二元一次，二元二次….)</p>
<p>​        V:优化过程（Convex Optimization-凸优化问题)</p>
<p><img src="%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/2020-08-22_06-57.png" alt="2020-08-22_06-57"></p>
<p>​        针对于一个样本来讲<br>$$<br>对于loss函数，变量ｗ,b；按照梯度下降的算法求loss的极值\<br>            找到loss最小时，对应的w,b\<br>            通过梯度迭代更新\<br>initial : w=0,b=0</p>
<p>\<br>b = b+ learningRate* \partial w\<br>w = w+ learningRate*\partial b</p>
<p>\<br>\partial w = 2(y-wx-b)*(-x)<br>\<br>\partial b= 2(y-wx-b)(-1)<br>$$</p>
<p>之后编程实现的是在Ｎ个样本上的问题</p>
<p><img src="%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/2020-08-22_06-59.png" alt="2020-08-22_06-59"></p>
<p>​        </p>
<h3 id="2-问题类型"><a href="#2-问题类型" class="headerlink" title="2.问题类型"></a>2.问题类型</h3><p><img src="%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/2020-08-22_07-01.png" alt="2020-08-22_07-01"></p>
<h3 id="3-二元一次方程-编程实现"><a href="#3-二元一次方程-编程实现" class="headerlink" title="3. 二元一次方程　编程实现"></a>3. 二元一次方程　编程实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computer_error_for_line_given_points</span>(<span class="params">b,w,points</span>):</span> <span class="comment">##计算错误率</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(points)):</span><br><span class="line">        x=points[i,<span class="number">0</span>]</span><br><span class="line">        y=points[i,<span class="number">1</span>]</span><br><span class="line">        totalError += (y-(w*x+b))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError/(<span class="built_in">float</span>)(<span class="built_in">len</span>(points))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_gradient</span>(<span class="params">b_current,w_current,points,learningRate</span>):</span><span class="comment">##在所有的节点上进行，一次梯度下降，更新参数</span></span><br><span class="line">    b_gradient=<span class="number">0</span></span><br><span class="line">    w_gradient=<span class="number">0</span></span><br><span class="line">    N= <span class="built_in">float</span>(<span class="built_in">len</span>(points))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(points)):</span><br><span class="line">        x=points[i,<span class="number">0</span>]</span><br><span class="line">        y=points[i,<span class="number">1</span>]</span><br><span class="line">        b_gradient += -(<span class="number">2</span>/N) *(y-((w_current*x)+b_current))</span><br><span class="line">        w_gradient += -(<span class="number">2</span>/N)*x*(y-((w_current*x)+b_current))</span><br><span class="line">    new_b = b_current -(learningRate*b_gradient)</span><br><span class="line">    new_w = w_current -(learningRate*w_gradient)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>[new_b,new_w]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span>(<span class="params">points,starting_b,starting_w,learning_rate,num_iterator</span>):</span></span><br><span class="line"></span><br><span class="line">    b= starting_b</span><br><span class="line">    w= starting_w</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterator):</span><br><span class="line">        b,w = step_gradient(b,w,np.array(points),learning_rate)<span class="comment">#这里应该是没有选择最小的loss只是把最后迭代的结果返回</span></span><br><span class="line">    <span class="keyword">return</span> [b,w]</span><br><span class="line"></span><br><span class="line"><span class="comment">#这里没有数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    points = np.genfromtxt(<span class="string">&quot;/home/doriswang/workplace/coding/pytorch_learning/venv/include/2.1/data.csv&quot;</span>,delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    <span class="comment"># print(points)</span></span><br><span class="line">    learning_rate =<span class="number">0.0001</span></span><br><span class="line">    initial_b =<span class="number">0</span></span><br><span class="line">    initial_w =<span class="number">0</span></span><br><span class="line">    num_iterations = <span class="number">1000</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting gradient descent at b=&#123;0&#125; , w=&#123;1&#125; ,error =&#123;2&#125;&quot;</span>.<span class="built_in">format</span>(initial_b,initial_w,computer_error_for_line_given_points(initial_b,initial_w,points)))</span><br><span class="line">    [b,w]=gradient_descent_runner(points,initial_b,initial_w,learning_rate,num_iterations)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After &#123;0&#125; iterations b=&#123;1&#125;,w=&#123;2&#125;,error=&#123;3&#125;&quot;</span>.<span class="built_in">format</span>(num_iterations,b,w,computer_error_for_line_given_points(b,w,points)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/%E5%88%9D%E8%AF%86pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/%E5%88%9D%E8%AF%86pytorch/" class="post-title-link" itemprop="url">初始pytorch</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-23 15:53:10" itemprop="dateCreated datePublished" datetime="2020-08-23T15:53:10+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-08-28 14:36:18" itemprop="dateModified" datetime="2020-08-28T14:36:18+08:00">2020-08-28</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="初试深度学习"><a href="#初试深度学习" class="headerlink" title="初试深度学习"></a>初试深度学习</h2><h3 id="1-深度学习框架"><a href="#1-深度学习框架" class="headerlink" title="1.深度学习框架"></a>1.深度学习框架</h3><p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_19-40.png" alt="2020-08-21_19-40"></p>
<p>==pytorch &amp;&amp; tensorflow的本质区别在于动态图优先还是静态图优先==</p>
<p>1.动态优先图–pytorch</p>
<p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_19-53.png" alt="2020-08-21_19-53"></p>
<p>2.静态图的方式–tensorflow</p>
<p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_19-59.png" alt="2020-08-21_19-59"></p>
<p>1.首先先建立一个计算图（框架）</p>
<p>2.向计算图传递参数来运行计算图</p>
<p>在计算图运行过程中，我们不能干预，调试或者动态改变比较麻烦</p>
<h4 id="综合评价"><a href="#综合评价" class="headerlink" title="综合评价"></a>综合评价</h4><p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_20-04.png" alt="2020-08-21_20-04"></p>
<h3 id="２-pytorch的生态"><a href="#２-pytorch的生态" class="headerlink" title="２.pytorch的生态"></a>２.pytorch的生态</h3><p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_20-10.png" alt="2020-08-21_20-10"></p>
<h3 id="３-pytorch的优势"><a href="#３-pytorch的优势" class="headerlink" title="３.pytorch的优势"></a>３.pytorch的优势</h3><p>１.使用GPU进行加速</p>
<p>２.自动求导</p>
<p>３.常用网络层</p>
<p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_20-19.png" alt="2020-08-21_20-19"></p>
<h2 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h2><p><img src="%E5%88%9D%E5%A7%8Bpytorch/2020-08-21_20-26.png" alt="2020-08-21_20-26"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/23/pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/23/pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/" class="post-title-link" itemprop="url">pytorch基础语法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-23 12:16:28" itemprop="dateCreated datePublished" datetime="2020-08-23T12:16:28+08:00">2020-08-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 08:51:37" itemprop="dateModified" datetime="2021-07-25T08:51:37+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-基本数据类型"><a href="#1-基本数据类型" class="headerlink" title="1 基本数据类型"></a>1 基本数据类型</h2><h3 id="1-1-python中数据类型与torch中数据类型的对比"><a href="#1-1-python中数据类型与torch中数据类型的对比" class="headerlink" title="1.1 python中数据类型与torch中数据类型的对比"></a>1.1 python中数据类型与torch中数据类型的对比</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_18-08.png" alt="2020-08-23_18-08"></p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_18-12.png" alt="2020-08-23_18-12"></p>
<p>《下图中常用的数据类型使用红色方框表示出来》</p>
<h3 id="1-2-pytorch中String的表示方法"><a href="#1-2-pytorch中String的表示方法" class="headerlink" title="1.2 pytorch中String的表示方法"></a>1.2 pytorch中String的表示方法</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_18-10.png" alt="2020-08-23_18-10"></p>
<h3 id="1-3-pytorch中CPU和GPU数据类型的区别"><a href="#1-3-pytorch中CPU和GPU数据类型的区别" class="headerlink" title="1.3　pytorch中CPU和GPU数据类型的区别"></a>1.3　pytorch中CPU和GPU数据类型的区别</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_17-07.png" alt="2020-08-23_17-07"></p>
<p>如果在GPU上面，则需要将其数据类型转换:<br>方法：``data=data.cuda()`　，调用此函数会返回一个ＧＰＵ上的一个应用</p>
<h3 id="1-4-torch中数据类型的判断"><a href="#1-4-torch中数据类型的判断" class="headerlink" title="1.4 torch中数据类型的判断"></a>1.4 torch中数据类型的判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch中的数据类型</span></span><br><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">3</span>)<span class="comment">#随机初始化一个两行三列的数据</span></span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">type</span>()) <span class="comment">#查看数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(a,torch.FloatTensor)) <span class="comment">#数据类型合法性检验</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(a,torch.IntTensor))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.FloatTensor</span></span><br><span class="line"><span class="string">&lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="1-5-Tensor的形状"><a href="#1-5-Tensor的形状" class="headerlink" title="1.5 Tensor的形状"></a>1.5 Tensor的形状</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_18-57.png" alt="2020-08-23_18-57"></p>
<p><code>data.shape</code>, <code>data.size()</code>,<code>data.dim()</code>,<code>data.numel()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dim =  size(data.shape) ,表示数据的深度／维度</span></span><br><span class="line"><span class="comment"># size=shape，表示数据的形状，记录每一维度的长度</span></span><br><span class="line"><span class="comment"># numel:表示tensor占用内存的长度</span></span><br><span class="line">d4 = torch.FloatTensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(d4.size())</span><br><span class="line"><span class="built_in">print</span>(d4.shape)</span><br><span class="line"><span class="built_in">print</span>(d4.dim())</span><br><span class="line"><span class="built_in">print</span>(d4.size(<span class="number">0</span>)) <span class="comment">#可以进行索引，某维度的长度，为了更好地与python进行交互，一般直接将size和shape转换成list</span></span><br><span class="line"><span class="built_in">print</span>(d4.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([2, 3])</span></span><br><span class="line"><span class="string">torch.Size([2, 3])</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-00.png" alt="2020-08-23_19-00"></p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-19.png" alt="2020-08-23_19-19"></p>
<h3 id="1-6-Dim-0的数据"><a href="#1-6-Dim-0的数据" class="headerlink" title="1.6 Dim 0的数据"></a>1.6 Dim 0的数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape) <span class="comment">#1.0/tensor(1.)是0维的是标量，但是[1.]是１维长度为１的Tensor</span></span><br><span class="line">                			<span class="comment">#标量的应用：计算的loss函数都是标量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(a.shape))</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([]),这里表示size中的‘list&#x27;长度为０</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">torch.Size([])</span></span><br><span class="line"><span class="string">tensor(2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>标量的用途：　1.常见是在计算loss函数的结果是用标量表示的</p>
<h3 id="1-7-Dim-1的数据"><a href="#1-7-Dim-1的数据" class="headerlink" title="1.7 Dim 1的数据"></a>1.7 Dim 1的数据</h3><p>常用于  1. Bias: wx+b 中的 b         2. Linear Input 例如，手写数字识别中的28*28 可以看作长度为784的一维数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##Dim 1/ rank 1</span></span><br><span class="line"><span class="comment">#其实在tensor 0.3 之前没有dim=0的Tensor，实际上维度为１，size=1的数即可表示标量,不过0.4之后把他们区分开了</span></span><br><span class="line"></span><br><span class="line">a1=torch.tensor([<span class="number">1.1</span>])<span class="comment">#dim=1,size=1</span></span><br><span class="line">a2=torch.tensor([<span class="number">1.1</span>,<span class="number">2.2</span>]) <span class="comment"># dim=1,size=2</span></span><br><span class="line"></span><br><span class="line">a3=torch.FloatTensor(<span class="number">1</span>) <span class="comment">#这里的参数指定的是Dim１的长度，随机初始化</span></span><br><span class="line"><span class="built_in">print</span>(a1.size())</span><br><span class="line"><span class="built_in">print</span>(a2.size())</span><br><span class="line"><span class="built_in">print</span>(a3.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([1])</span></span><br><span class="line"><span class="string">torch.Size([2])</span></span><br><span class="line"><span class="string">torch.Size([1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="1-8-Dim２的数据"><a href="#1-8-Dim２的数据" class="headerlink" title="1.8 Dim２的数据"></a>1.8 Dim２的数据</h3><p>常用场景：　1. Linear Input batch [ batch_size , features_num]</p>
<p>a=torch.tensor([4,784])<br>其中4是指数据图片的数目，而784是指每一张图片的特征维度<br>适用于普通的机器学习数据</p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-12_1.png" alt="2020-08-23_19-12_1"></p>
<h3 id="1-9-Dim-3的数据"><a href="#1-9-Dim-3的数据" class="headerlink" title="1.9 Dim 3的数据"></a>1.9 Dim 3的数据</h3><p><strong>RNN Input Batch</strong>　[batch_size, sentence_num, word_one_sentence]</p>
<p>例如，对于RNN神经网络进行语音识别与处理时[10,20,100]表示:每个单词包含100个特征，一句话一共有10个单词，而每次输20句话</p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-12.png" alt="2020-08-23_19-12"></p>
<h3 id="1-10-Dim-4-的数据"><a href="#1-10-Dim-4-的数据" class="headerlink" title="1.10 Dim 4 的数据"></a>1.10 Dim 4 的数据</h3><p><strong>CNN input Batch</strong> [batch_size, channel_num, height, weight ]　</p>
<p>通道数，图片的高度，图片的宽度</p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_19-19_1.png" alt="2020-08-23_19-19_1"></p>
<h2 id="2-创建Tensor"><a href="#2-创建Tensor" class="headerlink" title="2.创建Tensor"></a>2.创建Tensor</h2><h3 id="1-Import-from-numpy"><a href="#1-Import-from-numpy" class="headerlink" title="1. Import from numpy"></a>1. Import from numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([<span class="number">2</span>,<span class="number">3.3</span>]) <span class="comment"># 数据实际是double类型</span></span><br><span class="line"><span class="built_in">print</span>(torch.from_numpy(a))</span><br><span class="line"></span><br><span class="line">a = np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.from_numpy(a))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([2.0000, 3.3000], dtype=torch.float64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]], dtype=torch.float64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="2-Import-from-list"><a href="#2-Import-from-list" class="headerlink" title="2. Import from list"></a>2. Import from list</h3><p><strong>关于参数传递的小问题</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor / FloatTensor /IntTensor　等既可以传递数值型，也可以传递shape相关的类型的数据</span><br><span class="line">tensor只能传递数据list作为参数</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">2.</span>,<span class="number">3.2</span>])</span><br><span class="line">b = torch.tensor([[<span class="number">2.</span>,<span class="number">2.3</span>],[<span class="number">1.</span>,<span class="number">2.34</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">a2 = torch.FloatTensor([<span class="number">2.</span>,<span class="number">3.2</span>])<span class="comment">##不建议用该方法，tensor可以 代替</span></span><br><span class="line">a3 = torch.FloatTensor(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#建议传递参数为shape参数</span></span><br><span class="line"><span class="built_in">print</span>(a2)</span><br><span class="line"><span class="built_in">print</span>(a3)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;        </span></span><br><span class="line"><span class="string">tensor([2.0000, 3.2000])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[2.0000, 2.3000],</span></span><br><span class="line"><span class="string">        [1.0000, 2.3400]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([2.0000, 3.2000])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[[ 2.0281e+15,  4.5682e-41,  2.5162e-26,  4.5682e-41],</span></span><br><span class="line"><span class="string">         [ 2.0260e+15,  4.5682e-41,  2.5162e-26,  4.5682e-41],</span></span><br><span class="line"><span class="string">         [ 2.0260e+15,  4.5682e-41,  2.5157e-26,  4.5682e-41]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 2.0284e+15,  4.5682e-41,  2.6360e-26,  4.5682e-41],</span></span><br><span class="line"><span class="string">         [ 3.6957e+15,  4.5682e-41,  2.6361e-26,  4.5682e-41],</span></span><br><span class="line"><span class="string">         [ 2.0284e+15,  4.5682e-41, -2.9008e+29,  3.0639e-41]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-未初始化的数据"><a href="#3-未初始化的数据" class="headerlink" title="3. 未初始化的数据"></a>3. 未初始化的数据</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_20-25.png" alt="2020-08-23_20-25"></p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-23_20-29.png" alt="2020-08-23_20-29"></p>
<h3 id="4-随机初始化"><a href="#4-随机初始化" class="headerlink" title="4.随机初始化"></a>4.随机初始化</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#相关函数</span><br><span class="line">torch.rand(Tensor.shape)　#生成一个[0,1)之间的随机数</span><br><span class="line">torch.randlike(Tensor tt)</span><br><span class="line"></span><br><span class="line">torch.randint(  int low , int high , [d1,d2....])#均匀采样，只采整数值</span><br><span class="line">torch.randint_like(Tensor tt ,int low, int high) </span><br></pre></td></tr></table></figure>

<p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">a= torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># a1 = torch.rand()</span></span><br><span class="line"></span><br><span class="line">b = torch.rand_like(a)<span class="comment">#会生成与ａ尺寸大小相同的随机数的tensor</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">c = torch.randint(<span class="number">1</span>,<span class="number">99</span>,[<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line">d = torch.randint_like(c,<span class="number">1</span>,<span class="number">10</span>) <span class="comment">#使用randint一定要指出数据范围</span></span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.9843, 0.0908, 0.6296],</span></span><br><span class="line"><span class="string">        [0.5270, 0.4654, 0.3570],</span></span><br><span class="line"><span class="string">        [0.9419, 0.1018, 0.1724]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0.3775, 0.3013, 0.3237],</span></span><br><span class="line"><span class="string">        [0.6003, 0.1313, 0.1082],</span></span><br><span class="line"><span class="string">        [0.7574, 0.4202, 0.0610]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[20, 93, 85],</span></span><br><span class="line"><span class="string">        [20, 81, 95],</span></span><br><span class="line"><span class="string">        [22, 10, 88]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[8, 9, 8],</span></span><br><span class="line"><span class="string">        [2, 7, 4],</span></span><br><span class="line"><span class="string">        [1, 4, 9]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="5-生成正态分布"><a href="#5-生成正态分布" class="headerlink" title="5.生成正态分布"></a>5.生成正态分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#相关函数说明</span></span><br><span class="line">tensor.normal(mean,std,*,generator=<span class="literal">None</span>,out = <span class="literal">None</span>) -&gt;Tensor</span><br><span class="line"><span class="comment">#参数说明:该方法只能生成一维,需要进行形状变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1. mean, std可以都为tensor，此时可以为每个元素指定分布来源</span></span><br><span class="line"><span class="comment">#2. mean,std可以一个为float/tensor，此时最后生成的Tensor的尺寸mean/std(数据类型为tensor)决定，此时所有数据共享均值或者方差</span></span><br><span class="line"><span class="comment">#3. mean,std都为float时，要求指明参数，此时所有数据都来源一个分布</span></span><br><span class="line"></span><br><span class="line">tensor,randn(*size , out = <span class="literal">None</span>, dtype = <span class="literal">None</span>,....)<span class="comment">#参数列表没有列全</span></span><br><span class="line"><span class="comment">#从平均值为0，方差为1的正态分布中返回一个填充有随机数的张量（也称为标准正态分布</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">a1 = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#从平均值为0，方差为1的正态分布中返回一个填充有随机数的张量，形状由参数决定（也称为标准正态分布）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#关于torch.normal：比较灵活，掌握一种方式就行</span></span><br><span class="line"><span class="comment">#方式１：保证mean和std的尺寸相同</span></span><br><span class="line">a = torch.normal( mean = torch.arange(<span class="number">1.</span>,<span class="number">11.</span>),std = torch.arange(<span class="number">1</span>,<span class="number">0</span>,-<span class="number">0.1</span>))</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment">#方式２：</span></span><br><span class="line">b = torch.normal(mean = torch.full([<span class="number">10</span>],<span class="number">0</span>),std = <span class="number">0.5</span>) <span class="comment">#这种方式也是可以实现所有数据来自一个分布</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">c = torch.normal(mean = <span class="number">10</span>, std = torch.arange(<span class="number">5</span>,<span class="number">0</span>,-<span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment">#方式３</span></span><br><span class="line">d = torch.normal(<span class="number">10</span>,<span class="number">0.1</span>,size=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-0.6377,  0.0554, -0.7616],</span></span><br><span class="line"><span class="string">        [ 0.1244, -0.1076,  0.2659]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([-1.5609,  0.1449,  2.7791,  3.4830,  4.6098,  5.9147,  7.2475,  8.0593,</span></span><br><span class="line"><span class="string">         9.3134, 10.0676])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([ 0.8977, -0.6111, -0.2214, -0.0326,  0.4336, -0.2131, -0.0297, -0.0359,</span></span><br><span class="line"><span class="string">         0.0294, -0.3355])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">tensor([11.8549, 12.7589, 13.7979, 12.9034,  8.1115, 15.1006, 13.7763, 12.9549,10.0838, 10.1662])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[10.0064,  9.8803,  9.9584],</span></span><br><span class="line"><span class="string">        [10.1267, 10.2988,  9.8402]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="6-生成元素相同的tensor"><a href="#6-生成元素相同的tensor" class="headerlink" title="6.生成元素相同的tensor"></a>6.生成元素相同的tensor</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-13.png" alt="2020-08-24_00-13"></p>
<h3 id="7-生成等差数列的tensor"><a href="#7-生成等差数列的tensor" class="headerlink" title="7.生成等差数列的tensor"></a>7.生成等差数列的tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#相关函数说明</span></span><br><span class="line">torch.arange( start ,end , offset )</span><br><span class="line"><span class="comment">#torch.range(start,end, offset) #闭区间</span></span><br><span class="line">torch.linspace(start, end, steps=<span class="number">100</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Return </span></span><br><span class="line"><span class="string">start (float) – the starting value for the set of points</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">end (float) – the ending value for the set of points</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">steps (int) – number of points to sample between start and end. Default: 100.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">torch.logspace(torch.logspace(start, end, steps=<span class="number">100</span>, base=<span class="number">10.0</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>) → Tensor</span><br><span class="line">               </span><br><span class="line"> <span class="string">&#x27;&#x27;&#x27;  </span></span><br><span class="line"><span class="string">    Returns a one-dimensional tensor of steps points logarithmically spaced with base base between base^&#123;start&#125;  and  base^&#123;end&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-14.png" alt="2020-08-24_00-14"></p>
<p>???=＝如何设置base==</p>
<h3 id="8-其他生成函数-Ones-zeros-eye"><a href="#8-其他生成函数-Ones-zeros-eye" class="headerlink" title="8.其他生成函数(Ones, zeros,eye)"></a>8.其他生成函数(Ones, zeros,eye)</h3><p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-23.png" alt="2020-08-24_00-23"></p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-23_1.png" alt="2020-08-24_00-23_1"></p>
<h3 id="9-randperm-random-shuffle"><a href="#9-randperm-random-shuffle" class="headerlink" title="9.randperm(random.shuffle)"></a>9.randperm(random.shuffle)</h3><p>??</p>
<p><img src="pytorch%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/2020-08-24_00-26.png" alt="2020-08-24_00-26"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sherry Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
