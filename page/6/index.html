<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Suche...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="记录点滴成长">
<meta property="og:type" content="website">
<meta property="og:title" content="Think World">
<meta property="og:url" content="http://example.com/page/6/index.html">
<meta property="og:site_name" content="Think World">
<meta property="og:description" content="记录点滴成长">
<meta property="og:locale">
<meta property="article:author" content="Sherry Wang">
<meta property="article:tag" content="computer vision, DeepLearning ,MachineLearning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Think World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Navigationsleiste an/ausschalten" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Think World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">从个人成长角度来说，从经历中学点什么总是最重要的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Inhaltsverzeichnis
        </li>
        <li class="sidebar-nav-overview">
          Übersicht
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sherry Wang"
      src="/images/aa.webp">
  <p class="site-author-name" itemprop="name">Sherry Wang</p>
  <div class="site-description" itemprop="description">记录点滴成长</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">Artikel</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">Kategorien</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">schlagwörter</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/30/CircleNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/30/CircleNet/" class="post-title-link" itemprop="url">CircleNet</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>
      

      <time title="Erstellt: 2020-08-30 20:51:16 / Geändert am: 23:34:32" itemprop="dateCreated datePublished" datetime="2020-08-30T20:51:16+08:00">2020-08-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">关键点检测</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/" itemprop="url" rel="index"><span itemprop="name">医学影像</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/30/FR-DDH/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/30/FR-DDH/" class="post-title-link" itemprop="url">FR_DDH</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>
      

      <time title="Erstellt: 2020-08-30 19:31:49 / Geändert am: 12:26:54" itemprop="dateCreated datePublished" datetime="2020-08-30T19:31:49+08:00">2020-08-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">关键点检测</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/" itemprop="url" rel="index"><span itemprop="name">医学影像</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>本文的主要想法：首先我们提取以landmark为中心的(2N+1)×(2N+1)大小的image patch作为local neighborhood;我们把关键但的检测转换成local neighborhood的检测，目标检测的中心就是我们要检索的landmark。</p>
<p>所以网络框架是目标检测的框架.　本文论文关于网络具体框架什么的描述的细节比较少，而且没有代码，所以就..写的很简单，感觉可以学的就只是一个思路</p>
<h2 id="网络框架"><a href="#网络框架" class="headerlink" title="网络框架"></a>网络框架</h2><p><img src="FR-DDH/2020-08-30_20-12.png" alt="2020-08-30_20-12"></p>
<p><img src="FR-DDH/2020-08-30_20-12_1.png" alt="2020-08-30_20-12_1"></p>
<p>网络框架比较简单</p>
<h2 id="loss函数"><a href="#loss函数" class="headerlink" title="loss函数"></a>loss函数</h2><p>该损失函数和<a target="_blank" rel="noopener" href="https://gritcs.github.io/2020/08/27/Faster%20R-CNN/">Faster R-CNN</a>的损失函数相同，这里不再赘述</p>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><p>1.采用均值为０，方差为0.01的 Gaussian distribution随机初始化</p>
<ol start="2">
<li>learning rate of 0.001 for 80k mini-batches,  0.0001 for the next 30k mini-batches on the dataset. </li>
<li>The momentum is set to be 0.9 and the weight decay is set to be 0.0005. </li>
</ol>
<p>[论文中描述的比较简单]</p>
<h2 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h2><p>感觉这篇论文写的不是特别明确，[等之后再多看点论文，再总结关键点检测的评价标准吧]</p>
<p><img src="FR-DDH/2020-08-30_20-22.png" alt="2020-08-30_20-22"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/30/BN_dropout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/30/BN_dropout/" class="post-title-link" itemprop="url">batch normalize</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>
      

      <time title="Erstellt: 2020-08-30 11:38:23 / Geändert am: 12:50:50" itemprop="dateCreated datePublished" datetime="2020-08-30T11:38:23+08:00">2020-08-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">经典网络</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E8%AE%BE%E8%AE%A1%E7%BB%93%E6%9E%84/" itemprop="url" rel="index"><span itemprop="name">经典设计结构</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h1><p>参考文献：</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Batch_normalization">wiki</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/guoyaohua/p/8724433.html">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></p>
<p>问题背景：</p>
<p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98">梯度消失</a>与<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32154263">梯度爆炸</a>的问题</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_25737169/article/details/78847691">一般解决方法汇总</a></p>
<p>drop_out:这里可以参考吴恩达老师的deep learning课件</p>
<h3 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h3><p>内部协变量移位现象(Internal Covariate shift)</p>
<p>机器学习中有一个比较重要的假设：独立同分布，就是假设训练数据和测试数据是同时满足相同分布；</p>
<p>​    在网络的训练阶段，由于前几层的参数发生变化，因此当前层的输入分布也会相应变化，因此当前层需要不断调整以适应新的分布。对于较深的网络，此问题尤为严重，因为较浅的隐藏层的细微变化将在它们在网络中传播时被放大，从而导致较深的隐藏层发生显着变化。因此，提出了批量标准化的方法，以减少这些不必要的偏移，以加快训练速度并生成更可靠的模型。</p>
<p>​    NP的基本思想就是让每个隐层节点的激活输入分布固定下来（一般是放置在激活层之前）;类似于白化操作：对深层神经网络的每个隐层神经元的激活值做简化版本的白化操作.</p>
<p>​    同时，我们可以发现BN还有其他的用途</p>
<p>​        1.会使网络使用更高的学习速率而不会出现梯度消失或者梯度爆炸</p>
<p>​        2.似乎存在正则化效果从而改善了归一化属性，可以不使用drop out　</p>
<h3 id="本质思想"><a href="#本质思想" class="headerlink" title="本质思想"></a>本质思想</h3><p><img src="BN_dropout/2020-08-30_18-13.png" alt="2020-08-30_18-13"></p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p><img src="BN_dropout/2020-08-30_18-04.png" alt="2020-08-30_18-04"></p>
<p><img src="BN_dropout/2020-08-30_18-12.png" alt="2020-08-30_18-12"></p>
<p><img src="BN_dropout/2020-08-30_18-06.png" alt="2020-08-30_18-06"></p>
<p>①不仅仅极大提升了训练速度，收敛过程大大加快；</p>
<p>②还能增加分类效果，一种解释是这是类似于 Dropout 的一种防止过拟合的正则化表达方式，所以不用 Dropout 也能达到相当的效果；</p>
<p>③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/29/ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/29/ResNet/" class="post-title-link" itemprop="url">ResNet</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>

      <time title="Erstellt: 2020-08-29 20:22:22" itemprop="dateCreated datePublished" datetime="2020-08-29T20:22:22+08:00">2020-08-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Bearbeitet am</span>
        <time title="Geändert am: 2020-09-02 00:34:34" itemprop="dateModified" datetime="2020-09-02T00:34:34+08:00">2020-09-02</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">经典网络</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-ResNet"><a href="#1-ResNet" class="headerlink" title="1.ResNet"></a>1.ResNet</h1><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p><img src="ResNet/2020-08-30_10-40.png" alt="2020-08-30_10-40"></p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>在原论文中，作者不断加深网络结构来研究随着网络层次的加深，残差模块的表现</p>
<p>所以存在 ResNet-34, ResNet-50, ResNet-101, ResNet-152</p>
<p>这里只以ResNet-34为主</p>
<p><img src="ResNet/2020-08-29_20-35.png" alt="2020-08-29_20-35"></p>
<p>可以看到整个网络分为两部分Plain Network 和Residual Network</p>
<h3 id="Plain-Network"><a href="#Plain-Network" class="headerlink" title="Plain Network"></a>Plain Network</h3><p>参考最左侧VGG网络的框架，都是采用３×３的卷积核</p>
<p> (i) for the same output feature map size, the layers have the same number of filters; 而且stride=1, padding =1,</p>
<p>(ii) if the feature map size is halved, the number of filters is doubled </p>
<p>so as to preserve the time complexity per layer.[池化层使用步长为２,然后３×３的卷积核]</p>
<p>可以看出来网络比较工整</p>
<h3 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h3><h4 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h4><p>最右侧的网络即是残差网络，在plainNet的基础上增加了Residual模块</p>
<p><img src="ResNet/2020-08-30_07-17.png" alt="2020-08-30_07-17"></p>
<p>实弧线，表示这里shortcuts是正常的恒等映射<br>$$<br>Y = F(X,{w_i}) + X<br>$$<br>虚弧线，表示这里恒等映射(identity mapping / shortcuts)要进行增加维度；原文中提到了两种方式</p>
<p>1.　with extra zero entries padded for increasing dimensions;我理解的是仅仅是用０来填充多余的维度<br>2.　采用 projection shortcut (采用１*1 的卷积核)，公式为,$W_s$用来调整维度和尺寸。</p>
<p>$$<br>Y = F(X,{w_i}) + W_sX<br>$$</p>
<p>对于以上两种方式，stride=2</p>
<h4 id="残差结构的原理"><a href="#残差结构的原理" class="headerlink" title="残差结构的原理"></a>残差结构的原理</h4><p>＝＝自己的数学比较鸡肋，感觉应该是重点理解残差结构（而不仅仅是保证梯度不会很小）</p>
<p>本片论文的introduction中提到了，增加学习网络的深度确实有利于提高神经网络的能，但是会因为一些意料之外的原因导致出现在较深网络层次中准确率降低的情况；</p>
<p>其中比较重要的影响因素是　较深的网络会出现梯度消失和梯度爆炸的问题</p>
<p>1.梯度消失：导致深层的梯度不能传递到浅层,不能更新参数</p>
<p>2.梯度爆炸的：导致网络不稳定</p>
<p>有一些其他的解决方法　比如　normalized initialization, intermediate normalization layers[NP]</p>
<p>本文采用使用　残差结构：在深层网路，训练 F(x)的梯度趋向０，保证不出现梯度下降和梯度爆炸的现象</p>
<p>利用公式，我们可以简单验证一下梯度更新的情况</p>
<p><img src="ResNet/2020-08-30_07-17.png" alt="2020-08-30_07-17"><br>$$<br>Y = F(X,{ W_i }) +X\<br>F = W_2\sigma(W_1 X) \<br>$$<br>这里忽略了　偏移量$b_i$, $\sigma$ 表示relu函数</p>
<p>实际上我们训练的就是$F(x)$,而且在网络结构的深层我们趋向于将它的梯度训练为０</p>
<p>我们设损失函数为$\Epsilon$</p>
<p>$$<br>\<br>X_{l+1} = X_{l} + F(X_l,{ W_i })\即上一个残差模块的输出作为下一个残差模块的输入\<br>X_{l+2 } = X_{l+1}+F(X_{l+1},{W_I}) = X_l +  F(X_l,{ W_i })+F(X_{l+1},{W_I})\<br>…\<br>X_L = X_l + \sum_{i =l}^{L-1} F(X_i,W_i)\<br> \　X_l表示第l个残差模块的输入，X_L表示第L个残差模块的输入;满足l&lt;L\<br>$$</p>
<p>$$<br>\frac{\partial \Epsilon}{\partial X_l }  = \frac{\partial \Epsilon }{\partial X_L}\times \frac {\part X_L} {X_l}= \frac{\partial \Epsilon }{\partial X_L}(1+\frac{\part}{\part X_l}(\sum_{i=l}^{L-1} F(X_i,W_i)))<br>$$<br>注：这里恒等映射没有经过１×１的卷积运算</p>
<p>可以看出来，较为深层的网络，我们也可以保证梯度不为消失，可以较为无损地传播梯度</p>
<p>其他优秀博主的意见：[可以换个角度思考一下]</p>
<p><img src="ResNet/2020-08-30_10-26.png" alt="2020-08-30_10-26"></p>
<h3 id="残差网络的其他设计"><a href="#残差网络的其他设计" class="headerlink" title="残差网络的其他设计"></a>残差网络的其他设计</h3><p>1.由浅层的网络到深层的网络，将两层（3*3)转换成３层（１×１，３×３，１×１），减少参数，便于训练(１×１卷积，用来调整维度的；第１个降低维度到原来的1/2，第二个再恢复原来的维度)</p>
<p><img src="ResNet/2020-08-30_07-30_1.png" alt="2020-08-30_07-30_1"></p>
<p><img src="ResNet/2020-08-30_07-30.png" alt="2020-08-30_07-30"></p>
<p>２.关于涉及维度变换的残差模块的恒等映射，作者比较了三种实现</p>
<p><img src="ResNet/2020-08-30_10-35.png" alt="2020-08-30_10-35"></p>
<p>比较结果</p>
<p><img src="ResNet/2020-08-30_10-38.png" alt="2020-08-30_10-38"></p>
<p>但是考虑到训练的成本，建议选择方案A，即Zero-padding　shortcut,毕竟重点是残差结构</p>
<h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">（1）使用color augmentation做数据扩增</span><br><span class="line">（2）在每个卷积层之后，激活函数之前使用batch normalization (BN)</span><br><span class="line">（3）SGD作优化，weight decay =0.0001，momentum=0.9</span><br><span class="line">（4）learning rate=0.1,当错误率停滞时除以10</span><br><span class="line">（5）不使用dropout  </span><br></pre></td></tr></table></figure>

<p>参考一些博客，博主们都提到了训练过程中　BN位置的设置等问题</p>
<p>参考博客：</p>
<p>实验：<a target="_blank" rel="noopener" href="https://juejin.im/entry/6844903564590972936">https://juejin.im/entry/6844903564590972936</a></p>
<p>原理：<a target="_blank" rel="noopener" href="https://www.itread01.com/content/1544868722.html">https://www.itread01.com/content/1544868722.html</a></p>
<p>​        <a target="_blank" rel="noopener" href="https://blog.csdn.net/u014296502/article/details/80438616">https://blog.csdn.net/u014296502/article/details/80438616</a></p>
<p>综述：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31852747">https://zhuanlan.zhihu.com/p/31852747</a></p>
<p>原文地址：</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/27/Faster%20R-CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/27/Faster%20R-CNN/" class="post-title-link" itemprop="url">Faster R-CNN 论文笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>

      <time title="Erstellt: 2020-08-27 23:23:29" itemprop="dateCreated datePublished" datetime="2020-08-27T23:23:29+08:00">2020-08-27</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Bearbeitet am</span>
        <time title="Geändert am: 2020-09-02 00:28:02" itemprop="dateModified" datetime="2020-09-02T00:28:02+08:00">2020-09-02</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">关键点检测</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>主要<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31426458">参考博文</a></p>
<p><a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">原文地址</a></p>
<p>在原论文的摘要中提出：</p>
<p>在之前SPP-Net 和Fast R-CNN提出依靠region proposal algorithms去预测物体的位置会减少很多计算量，但是这也暴露了区域提议计算的瓶颈</p>
<p>本文关键提出了RPN(region proposal network)，使用卷积层，减少参数的数量，有较快的速度和较高的准确率</p>
<p>RPN ==shares full-image convolutional features== with the detection network, thus enabling nearly ==cost-free== region proposals. </p>
<p>​    An RPN is a fully-convolutional network that simultaneously ==predicts object bounds and objectness scores== at each position. RPNs are trained end-to-end to generate highquality region proposals, which are used by Fast R-CNN for detection.     </p>
<p>softmax交叉熵损失函数：<a target="_blank" rel="noopener" href="https://blog.csdn.net/chaipp0607/article/details/73392175">https://blog.csdn.net/chaipp0607/article/details/73392175</a></p>
<h2 id="网络框架"><a href="#网络框架" class="headerlink" title="网络框架"></a>网络框架</h2><p>![2020-08-28_16-35](Faster R-CNN/2020-08-28_16-35.png)</p>
<p>主要分为四部分：</p>
<ol>
<li><p>Conv layers:　作为一种CNN网络目标检测方法，使用conv+pooling+relu来提取feature maps，被之后的RPN 和全连接层共享</p>
</li>
<li><p>RPN(region proposal network )</p>
<p>​    ==用于生成region proposals==，该层主要工作：</p>
<p>​        a. 通过softmax判断anchors属于positive , negative，</p>
<p>​        b. 再利用bounding box regression 修正anchors获得较为准确的            proposals</p>
</li>
<li><p>roi pooling</p>
<p>​    该层收集输入的feature maps和proposals，综合这些信息提区proposal feature maps，</p>
</li>
<li><p>classification </p>
<p>​    利用 proposal feature maps 计算 proposal 的类别，同时再次 bounding box regression 获得检测框最终的精确位置</p>
</li>
</ol>
<p>下图为python的VGG16模型中的faster_rcnn_test.pt网络结构</p>
<p>![2020-08-28_16-34](Faster R-CNN/2020-08-28_16-34.png)</p>
<h3 id="Conv-layers"><a href="#Conv-layers" class="headerlink" title="Conv layers"></a>Conv layers</h3><p>在Conv layers中</p>
<ol>
<li>所有的conv层都是　kernel_size = 3, pad =1 ,stride =1</li>
<li>所有的pooling层都是　kernel_size =2, pad =0, stride =2</li>
</ol>
<p>导致　Conv layers中conv层不改变输入输出矩阵的大小，只有在pooling层中M×N的矩阵变成(M/2)×(N/2)的大小。所以从Conv layers输出的矩阵的尺寸为M×N</p>
<p>目的是为了在ROI Pooling的输入层中proposal（M×N)与 feature maps尺寸一致</p>
<h3 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h3><p>这一层的主要任务： 获取有效的proposals，完成目标定位</p>
<p>![2020-08-28_17-42](Faster R-CNN/2020-08-28_17-42.png)</p>
<p>![2020-08-28_18-50](Faster R-CNN/2020-08-28_18-50.png)</p>
<p><strong>主要流程：生成 anchors -&gt; softmax 分类器提取 positvie anchors -&gt; bbox reg 回归 positive anchors -&gt; Proposal Layer 生成 proposals</strong></p>
<p><strong>这是主要的原理就是：利用3×３的卷积核的中心作为anchor的中心，在每个点设置9个anchor作为候选区，之后再有cnn来判断anchor是negitive or positive anchor</strong>，目前这里只是二分类,而且后面还有 2 次 bounding box regression 可以修正检测框位置</p>
<h4 id="1-生成anchors"><a href="#1-生成anchors" class="headerlink" title="1.生成anchors"></a>1.生成anchors</h4><p>略(思路是比较简单的,可以参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31426458">博客)</a></p>
<h4 id="2-softmax分类器提取positive-anchors"><a href="#2-softmax分类器提取positive-anchors" class="headerlink" title="2.softmax分类器提取positive anchors"></a>2.softmax分类器提取positive anchors</h4><p>输入：anchors，输出:rpn_cls_score</p>
<h4 id="3-bbox-回归"><a href="#3-bbox-回归" class="headerlink" title="3.bbox 回归"></a>3.bbox 回归</h4><h6 id="bounding-box-regression原理"><a href="#bounding-box-regression原理" class="headerlink" title="bounding box regression原理"></a>bounding box regression原理</h6><p>大致思想：我们目标是像通过　bounding box regression，对anchors进行调整，让他接近GT(但是我们anchor的选取按照固定的方法，没有结合GT的位置信息)；所以我们学习的是anchors和GT之间变换(每张图片的GT是固定的，而且anchors的选取也是固定的)</p>
<p>​    所以本文学习的是预测的窗口(anchor)=&gt;GT窗口之间的一种变换（先平移后进行缩放）(如果相差比较小，即positive可以看作是线性回归模型)</p>
<p>​    并设计相应的loss函数对其进行约束</p>
<p><strong>原理</strong></p>
<p>![2020-08-28_18-29](Faster R-CNN/2020-08-28_18-29.png)</p>
<p>![2020-08-28_18-35](Faster R-CNN/2020-08-28_18-35.png)</p>
<p>![2020-08-28_18-36](Faster R-CNN/2020-08-28_18-36.png)</p>
<p>![2020-08-28_21-22](Faster R-CNN/2020-08-28_21-22.png)</p>
<p>而在原论文中是这样子记录的</p>
<p>![](Faster R-CNN/2020-08-28_21-21.png)</p>
<p>![2020-08-28_20-51](Faster R-CNN/2020-08-28_20-51.png)</p>
<p>==<strong>在bbox-regression 中要训练的参数就是，我们需要预测的box(x,y,w,h)</strong>==</p>
<h4 id="的4-proposal-layer-生成-proposals"><a href="#的4-proposal-layer-生成-proposals" class="headerlink" title="的4. proposal layer 生成 proposals"></a>的4. proposal layer 生成 proposals</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">根据bbox回归得到的[dx(A), dy(A), dw(A) ,dh(A)]对所有的anchors进行修正和微调</span><br><span class="line">按照输入的positive softmax scores　由大到小排序anchors，提取前pre_nms_topN，即选择较好的修正后的　positive anchors</span><br><span class="line">限定超出图像边界的 positive anchors 为图像边界，防止后续 roi pooling 时 proposal 超出图像边界</span><br><span class="line">剔除尺寸非常小的 positive anchors</span><br><span class="line">对剩余的 positive anchors 进行 NMS（nonmaximum suppression）</span><br></pre></td></tr></table></figure>

<p>注意由于第三步生成的proposal要和原图像进行对比，所以它的尺寸是M*N</p>
<h6 id="NMS：非极大值抑制"><a href="#NMS：非极大值抑制" class="headerlink" title="NMS：非极大值抑制"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37489043">NMS：非极大值抑制</a></h6><p>![2020-08-28_19-07](Faster R-CNN/2020-08-28_19-07.png)</p>
<p>消除冗余的边界框</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">根据置信度得分进行排序</span><br><span class="line">选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除</span><br><span class="line">计算所有边界框的面积</span><br><span class="line">计算置信度最高的边界框与其它候选框的IoU。</span><br><span class="line">删除IoU大于阈值的边界框</span><br><span class="line">重复上述过程，直至边界框列表为空</span><br></pre></td></tr></table></figure>

<p>​    </p>
<h3 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h3><p>是一个简单的SPP-Net, 属于卷积层和连接层之间的过渡层，将大小不一的proposals变成固定大小(之后classifier　模块是利用全连接层进行分类，所以需要固定大小)</p>
<p>存在其他方法：crop，warp,但是他们会破坏图像原有信息</p>
<p>![2020-08-28_21-44](Faster R-CNN/2020-08-28_21-44.png)</p>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>![2020-08-28_19-14](Faster R-CNN/2020-08-28_19-14.png)</p>
<h2 id="loss函数"><a href="#loss函数" class="headerlink" title="loss函数"></a>loss函数</h2><p>在训练过程和bbox回归中都有涉及</p>
<p>![2020-08-28_20-51](Faster R-CNN/2020-08-28_20-51.png)</p>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><h3 id="交替训练过程"><a href="#交替训练过程" class="headerlink" title="交替训练过程"></a>交替训练过程</h3><p>![2020-08-28_19-38](Faster R-CNN/2020-08-28_19-38.png)</p>
<p>![2020-08-28_19-40](Faster R-CNN/2020-08-28_19-40.png)</p>
<p><strong>公共卷积层的不是已经训练好了么？？</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">参数说明：</span><br><span class="line">rpn_cls_prob_reshape:　positive vs negative anchors 分类器结果 </span><br><span class="line"></span><br><span class="line"> rpn_bbox_pred:　对应的 bbox reg 的  变换量[dx(A,dy(A),dw(A),dh(A))]</span><br><span class="line"> </span><br><span class="line">im_info:  对于一副任意大小 PxQ 图像，传入 Faster RCNN 前首先 reshape 到固定 MxN，im_info=[M, N, scale_factor] 则保存了此次缩放的所有信息。这样做的是为了方便之后训练网络[历史遗留问题]</span><br></pre></td></tr></table></figure>

<p>![2020-08-28_20-08](Faster R-CNN/2020-08-28_20-08.png)</p>
<p>![2020-08-28_20-26](Faster R-CNN/2020-08-28_20-26.png)</p>
<h4 id="rpn-train1"><a href="#rpn-train1" class="headerlink" title="rpn-train1"></a>rpn-train1</h4><p>![2020-08-28_20-25](Faster R-CNN/2020-08-28_20-25.png)</p>
<p>![2020-08-28_20-29](Faster R-CNN/2020-08-28_20-29.png)</p>
<p>![2020-08-28_20-02](Faster R-CNN/2020-08-28_20-02.png)</p>
<h4 id="RPN-test"><a href="#RPN-test" class="headerlink" title="RPN-test"></a>RPN-test</h4><p>![2020-08-28_20-11](Faster R-CNN/2020-08-28_20-11.png)</p>
<p>![2020-08-28_20-02_1](Faster R-CNN/2020-08-28_20-02_1.png)</p>
<h4 id="faster-rcnn"><a href="#faster-rcnn" class="headerlink" title="faster rcnn"></a>faster rcnn</h4><p>![2020-08-28_20-10](Faster R-CNN/2020-08-28_20-10.png)</p>
<p>![2020-08-28_20-15](Faster R-CNN/2020-08-28_20-15.png)</p>
<p>第二轮训练方式与第一轮大同小异。</p>
<h3 id="端到端方式"><a href="#端到端方式" class="headerlink" title="端到端方式"></a>端到端方式</h3><h2 id="仍然存在的问题"><a href="#仍然存在的问题" class="headerlink" title="仍然存在的问题"></a>仍然存在的问题</h2><p>１。读不懂代码，意味不知道网络需要什么样子的输入以及标注,而且看论文只能看个大概，不是特别清楚得了解每层网络之间的尺寸，维度等等的变换</p>
<p>２.不是特别明白训练的过程</p>
<p>３.不是特别明白　这里关于faster-rcnn　和　rpn　之间的参数共享的问题</p>
<ol start="4">
<li>要训练的参数都有哪些？？　反向传播，参数更新的过程</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/27/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/27/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/" class="post-title-link" itemprop="url">目标检测综述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>

      <time title="Erstellt: 2020-08-27 23:23:17" itemprop="dateCreated datePublished" datetime="2020-08-27T23:23:17+08:00">2020-08-27</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Bearbeitet am</span>
        <time title="Geändert am: 2020-08-30 11:26:10" itemprop="dateModified" datetime="2020-08-30T11:26:10+08:00">2020-08-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">目标检测</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="目标检测summary"><a href="#目标检测summary" class="headerlink" title="目标检测summary"></a>目标检测summary</h2><p>本文在 其他优秀博主的基础上进行 修改和总结，主要描述目标检测的问题以及解决方法</p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/20180502184712966.jpg" alt="20180502184712966"></p>
<p>目标检测的任务本质只有两个问题：图像识别，定位</p>
<p>这里着重介绍一下目标检测的评价方式</p>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/2020-08-30_19-20.png" alt="2020-08-30_19-20"></p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/2020-08-30_19-20_1.png" alt="2020-08-30_19-20_1"></p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/2020-08-30_19-20_2.png" alt="2020-08-30_19-20_2"></p>
<p><strong>数据集不平衡时， precision和recall等指标就不具备参考性</strong>，所以引出了AP<br>，它的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56961620">相关介绍用例子介绍比较合适</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/27/%E6%9C%80%E5%A4%A7%E6%B5%81%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/27/%E6%9C%80%E5%A4%A7%E6%B5%81%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">最大流问题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>
      

      <time title="Erstellt: 2020-08-27 12:36:16 / Geändert am: 04:42:24" itemprop="dateCreated datePublished" datetime="2020-08-27T12:36:16+08:00">2020-08-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%BB%8F%E5%85%B8%E9%97%AE%E9%A2%98/" itemprop="url" rel="index"><span itemprop="name">经典问题</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-最大流问题"><a href="#1-最大流问题" class="headerlink" title="1. 最大流问题"></a>1. 最大流问题</h2>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/26/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/26/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" class="post-title-link" itemprop="url">随机梯度下降</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>

      <time title="Erstellt: 2020-08-26 09:41:39" itemprop="dateCreated datePublished" datetime="2020-08-26T09:41:39+08:00">2020-08-26</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Bearbeitet am</span>
        <time title="Geändert am: 2020-09-03 03:54:58" itemprop="dateModified" datetime="2020-09-03T03:54:58+08:00">2020-09-03</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_09-57.png" alt="2020-08-26_09-57"></p>
<p>梯度是方向导数最大的地方</p>
<h3 id="利用SGD深度学习的一般步骤"><a href="#利用SGD深度学习的一般步骤" class="headerlink" title="利用SGD深度学习的一般步骤"></a>利用SGD深度学习的一般步骤</h3><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-05.png" alt="2020-08-26_10-05"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-21.png" alt="2020-08-26_10-21"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-22.png" alt="2020-08-26_10-22"></p>
<h3 id="影响优化器的因素"><a href="#影响优化器的因素" class="headerlink" title="影响优化器的因素"></a>影响优化器的因素</h3><p>每个都是一个研究方向，这里只是简单列出，只有有机会会分专题详细学习</p>
<h4 id="梯度问题"><a href="#梯度问题" class="headerlink" title="梯度问题"></a>梯度问题</h4><h6 id="局部最小值"><a href="#局部最小值" class="headerlink" title="局部最小值"></a>局部最小值</h6><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-26.png" alt="2020-08-26_10-26"></p>
<h6 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h6><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-31.png" alt="2020-08-26_10-31"></p>
<h4 id="权重初始值"><a href="#权重初始值" class="headerlink" title="权重初始值"></a>权重初始值</h4><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-34.png" alt="2020-08-26_10-34"></p>
<p>不仅仅课程中所提到的问题：由于之后网络层次比较深刻，所以会出现<strong>梯度消失</strong>或者<strong>梯度爆炸</strong>的问题</p>
<p>常见的初始化方法：[还没有彻底的理解，先学习一个框架]</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/80025785?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param">https://blog.csdn.net/u012328159/article/details/80025785?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62850258">https://zhuanlan.zhihu.com/p/62850258</a></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_10-35.png" alt="2020-08-26_10-35"></p>
<h4 id="learning-rate"><a href="#learning-rate" class="headerlink" title="learning rate"></a>learning rate</h4><p>逐步衰减;过大，可能会出现震荡，不会达到局部最优解;过小，优化的速度会很慢</p>
<h4 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h4><p>动量：用来逃出局部最优解</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_11-04.png" alt="2020-08-26_11-04"></p>
<h2 id="激活函数与loss的梯度计算"><a href="#激活函数与loss的梯度计算" class="headerlink" title="激活函数与loss的梯度计算"></a>激活函数与loss的梯度计算</h2><p>注意！！！：梯度是向量，这是我之前一致都不大注意的点，将它与导数混淆</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_11-16.png" alt="2020-08-26_11-16"></p>
<h3 id="激活函数的概念"><a href="#激活函数的概念" class="headerlink" title="激活函数的概念"></a>激活函数的概念</h3><p>灵感来自 青蛙的神经元的结构–一个阈值函数：神经元并不是各个输入的加权求和而是只有大于某个阈值之后才会输出，输出值是固定的值</p>
<p>早期的激活函数</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_11-22.png" alt="2020-08-26_11-22"></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Doris12138lucky/article/details/104375745">之前自己总结有各种激活函数</a>：</p>
<p>课程里所提到的几个简单的激活函数，以及对应的导数</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-14.png" alt="2020-08-26_16-14"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-15.png" alt="2020-08-26_16-15"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-15_1.png" alt="2020-08-26_16-15_1"></p>
<h4 id="在pytorch相对应的函数"><a href="#在pytorch相对应的函数" class="headerlink" title="在pytorch相对应的函数"></a>在pytorch相对应的函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.sigmoid()</span><br><span class="line"></span><br><span class="line">torch.tanh()</span><br><span class="line"></span><br><span class="line">torch.nn.functional.relu()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="常见的loss函数"><a href="#常见的loss函数" class="headerlink" title="常见的loss函数"></a>常见的loss函数</h3><p>mean-square-error</p>
<p>Cross-Entropy-loss</p>
<p>Softmax</p>
<h4 id="相关导数推导"><a href="#相关导数推导" class="headerlink" title="相关导数推导"></a>相关导数推导</h4><h6 id="mse"><a href="#mse" class="headerlink" title="mse"></a>mse</h6><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-20_2.png" alt="2020-08-26_16-20_2"></p>
<h6 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h6><p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-20.png" alt="2020-08-26_16-20"></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-20_1.png" alt="2020-08-26_16-20_1"></p>
<h3 id="pytorch中的自动求导"><a href="#pytorch中的自动求导" class="headerlink" title="pytorch中的自动求导"></a>pytorch中的自动求导</h3><h4 id="torch-autograd-grad"><a href="#torch-autograd-grad" class="headerlink" title="torch.autograd.grad()"></a><code>torch.autograd.grad()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.rand(<span class="number">3</span>) <span class="comment"># [batch, class_num]</span></span><br><span class="line">a.requires_grad_()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">p = F.softmax(a,dim=<span class="number">0</span>)<span class="comment">#在第0维上，也就是 class_num上,定义损失函数</span></span><br><span class="line"><span class="built_in">print</span>(p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法2：</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.grad(p[<span class="number">0</span>],[a],retain_graph= <span class="literal">True</span>)) <span class="comment">#单个计算每个损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.grad(p[<span class="number">1</span>],[a],retain_graph= <span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.grad(p[<span class="number">2</span>],[a],retain_graph= <span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([0.5666, 0.6252, 0.9636], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor([0.2819, 0.2989, 0.4192], grad_fn=&lt;SoftmaxBackward&gt;)</span></span><br><span class="line"><span class="string">(tensor([ 0.2024, -0.0842, -0.1182]),)</span></span><br><span class="line"><span class="string">(tensor([-0.0842,  0.2096, -0.1253]),)</span></span><br><span class="line"><span class="string">(tensor([-0.1182, -0.1253,  0.2435]),)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="loss-backword"><a href="#loss-backword" class="headerlink" title="loss.backword()"></a><code>loss.backword()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.rand(<span class="number">3</span>) <span class="comment"># [batch, class_num]</span></span><br><span class="line">a.requires_grad_()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">p = F.softmax(a,dim=<span class="number">0</span>)<span class="comment">#在第0维上，也就是 class_num上,定义损失函数</span></span><br><span class="line"><span class="built_in">print</span>(p)</span><br><span class="line"></span><br><span class="line"><span class="comment">##p.backward() 报错 ，因为此时p为向量，没有办法进行求导</span></span><br><span class="line"><span class="comment"># pytorch: grad can be implicitly created only for scalar outputs</span></span><br><span class="line"><span class="comment">##只能对单个y进行自动求导，</span></span><br><span class="line"><span class="comment">#所以单独列开，同时由于p[0]-p[2]都使用同一个计算图，</span></span><br><span class="line"><span class="comment"># 但是pytorch的计算图如果没有显式声明要保存，计算一次之后会作废，所以要置retain_graph=True</span></span><br><span class="line"><span class="comment"># retain_graph 有效次数是1次，即如果下次还需要用到计算图，还是需要置为True</span></span><br><span class="line"></span><br><span class="line">p[<span class="number">0</span>].backward(retain_graph =<span class="literal">True</span>)</span><br><span class="line">p[<span class="number">1</span>].backward(retain_graph =<span class="literal">True</span>)</span><br><span class="line">p[<span class="number">2</span>].backward(retain_graph =<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.grad) </span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([0.1281, 0.9067, 0.9391], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor([0.1842, 0.4013, 0.4145], grad_fn=&lt;SoftmaxBackward&gt;)</span></span><br><span class="line"><span class="string">tensor([1.4901e-08, 4.4703e-08, 1.4901e-08])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="两种方法对比"><a href="#两种方法对比" class="headerlink" title="两种方法对比"></a>两种方法对比</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(</span><br><span class="line"></span><br><span class="line">outputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]], </span><br><span class="line"></span><br><span class="line">inputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]],</span><br><span class="line"></span><br><span class="line">grad_outputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], <span class="literal">None</span>] = <span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">retain_graph: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>, <span class="comment"># True--------the function will only return a list of gradients w.r.t the specified inputs.</span></span><br><span class="line">    																		<span class="comment"># False,  will be accumulated into their .grad attribute.</span></span><br><span class="line"></span><br><span class="line">create_graph: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line"></span><br><span class="line">only_inputs: <span class="built_in">bool</span> = <span class="literal">True</span>, <span class="comment"># True--------the function will only return a list of gradients w.r.t the specified inputs.</span></span><br><span class="line">    												<span class="comment"># False,  will be accumulated into their .grad attribute.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">allow_unused: <span class="built_in">bool</span> = <span class="literal">False</span>) → <span class="type">Tuple</span>[torch.Tensor, ...] </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.backward(</span><br><span class="line">			tensors: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]], </span><br><span class="line">			</span><br><span class="line">			grad_tensors: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], <span class="literal">None</span>] = <span class="literal">None</span>, </span><br><span class="line">			</span><br><span class="line">			retain_graph: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>, </span><br><span class="line">			</span><br><span class="line">			create_graph: <span class="built_in">bool</span> = <span class="literal">False</span>, </span><br><span class="line">			</span><br><span class="line">			grad_variables: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor],<span class="literal">None</span>] = <span class="literal">None</span>) → <span class="literal">None</span></span><br><span class="line">			</span><br><span class="line">	</span><br><span class="line">    <span class="comment">#该函数是将所有的梯度计算都累加到需要计算梯度的变量的grad属性中</span></span><br></pre></td></tr></table></figure>

<p>官方文档建议使用第一种，因为第二种存在内存泄漏问题（也不是很清楚</p>
<h4 id="遇到的bug和解决问题"><a href="#遇到的bug和解决问题" class="headerlink" title="遇到的bug和解决问题"></a>遇到的bug和解决问题</h4><p>代码背景</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.rand(<span class="number">3</span>) <span class="comment"># [batch, class_num]</span></span><br><span class="line">a.requires_grad_()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">p = F.softmax(a,dim=<span class="number">0</span>)<span class="comment">#在第0维上，也就是 class_num上,定义损失函数</span></span><br><span class="line"><span class="built_in">print</span>(p)</span><br></pre></td></tr></table></figure>

<p><strong>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</strong></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-45.png" alt="2020-08-26_16-45"></p>
<p>修改方案： retain_graph</p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-46.png" alt="2020-08-26_16-46"></p>
<p><strong>pytorch: grad can be implicitly created only for scalar outputs</strong></p>
<p><img src="%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2020-08-26_16-49.png" alt="2020-08-26_16-49"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/25/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/25/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/" class="post-title-link" itemprop="url">语义分割概述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>

      <time title="Erstellt: 2020-08-25 23:15:25" itemprop="dateCreated datePublished" datetime="2020-08-25T23:15:25+08:00">2020-08-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Bearbeitet am</span>
        <time title="Geändert am: 2021-07-05 08:31:25" itemprop="dateModified" datetime="2021-07-05T08:31:25+08:00">2021-07-05</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" itemprop="url" rel="index"><span itemprop="name">基础知识</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="语义分割概述"><a href="#语义分割概述" class="headerlink" title="语义分割概述"></a>语义分割概述</h2><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_08-17.png" alt="2020-08-26_08-17"></p>
<h3 id="语义分割的任务"><a href="#语义分割的任务" class="headerlink" title="语义分割的任务"></a>语义分割的任务</h3><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_08-19.png" alt="2020-08-26_08-19"></p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_08-19_1.png" alt="2020-08-26_08-19_1"></p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_08-22.png" alt="2020-08-26_08-22"></p>
<h4 id="语义分割的概念"><a href="#语义分割的概念" class="headerlink" title="语义分割的概念"></a>语义分割的概念</h4><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_08-21.png" alt="2020-08-26_08-21"></p>
<h4 id="语义分割的应用"><a href="#语义分割的应用" class="headerlink" title="语义分割的应用"></a>语义分割的应用</h4><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_08-23.png" alt="2020-08-26_08-23"></p>
<h4 id="提出任务"><a href="#提出任务" class="headerlink" title="提出任务"></a>提出任务</h4><p>目标是 RGB 彩色图像（H*W*3) 或者灰度图像（高×宽×1）作为输入，输出分割图（即用相应的类别来标记图像的每个像素点）</p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_08-29.png" alt="2020-08-26_08-29"></p>
<p>一般的实现过程：</p>
<ol>
<li><p>通过 一个one-hot编码的label来创建目标–为每个可能的类创建一个输出通道</p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_09-17.png" alt="2020-08-26_09-17"></p>
</li>
<li><p>通过argmax操作，获得每个深度像素向量的argmax值，可以将一个预测图分解成分割图，将目标叠加在观察目标上</p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_09-34.png" alt="2020-08-26_09-34"></p>
</li>
<li><p>同时我们也可以利用掩模技术（某个类别对应的单个通道），发现存在特定类别的图像的区域</p>
</li>
<li><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_09-19.png" alt="2020-08-26_09-19"></p>
</li>
</ol>
<h6 id="关于掩模（mask-的理解"><a href="#关于掩模（mask-的理解" class="headerlink" title="关于掩模（mask)的理解"></a>关于<a target="_blank" rel="noopener" href="https://blog.csdn.net/guofei_fly/article/details/104505795?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.channel_param#commentBox">掩模（mask)</a>的理解</h6><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-26_09-25.png" alt="2020-08-26_09-25"></p>
<h3 id="常用数据库介绍"><a href="#常用数据库介绍" class="headerlink" title="常用数据库介绍"></a>常用数据库介绍</h3><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_15-41.png" alt="2020-08-27_15-41"></p>
<h2 id="常见的网络结构"><a href="#常见的网络结构" class="headerlink" title="常见的网络结构"></a>常见的网络结构</h2><h3 id="编码器和解码器的网络结构"><a href="#编码器和解码器的网络结构" class="headerlink" title="编码器和解码器的网络结构"></a>编码器和解码器的网络结构</h3><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-33.png" alt="2020-08-27_18-33"></p>
<p>​     不同于图像分类，我们只关心图像中所包含的内容（不用关心它的位置信息），因此我们可以采用汇聚或者跨越卷积的方式通过周期性地对特征图进行才应来减轻计算负担。 </p>
<p>​    但是语义分割不仅需要在像素级有分类能力，还需要有把在不同阶段学到的可判别特征映射到像素空间的机制; 所以， 我们需要一个全分辨率的语义预测</p>
<p>​    编码器的任务：-</p>
<p>​        通常是一个预训练的分类网络，对输入空间进行下采样开发低分辨率的特征映射，用来进行分类，eg： VGG，ResNet</p>
<p>​    解码器的任务：–上采样</p>
<p>​        将编码器学习得到的可判别特征从语义上映射到像素空间（较高分辨率，以获得密集分类。        </p>
<p>​        不同的架构采用不同的机制（跳远连接、金字塔池化等）作为解码机制的一部分。</p>
<h3 id="上采样和下采样"><a href="#上采样和下采样" class="headerlink" title="上采样和下采样"></a>上采样和下采样</h3><h4 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h4><h6 id="Unpooling-去池化"><a href="#Unpooling-去池化" class="headerlink" title="Unpooling(去池化)"></a>Unpooling(去池化)</h6><ol>
<li><p>最近临方法</p>
</li>
<li><p>钉床方法</p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-34.png" alt="2020-08-27_18-34"></p>
</li>
<li><p>最大去池化（seg-net)</p>
</li>
</ol>
<p>这些都不需要参数，比较简单</p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-34_1.png" alt="2020-08-27_18-34_1"></p>
<h6 id="interpolation"><a href="#interpolation" class="headerlink" title="interpolation"></a>interpolation</h6><p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-36.png" alt="2020-08-27_18-36"></p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-38.png" alt="2020-08-27_18-38"></p>
<h6 id="transpose-convolutions-U-net"><a href="#transpose-convolutions-U-net" class="headerlink" title="transpose convolutions(U - net )"></a>transpose convolutions(U - net )</h6><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sandy-t/p/7210895.html">https://www.cnblogs.com/sandy-t/p/7210895.html</a></p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-09.png" alt="2020-08-27_18-09"></p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-09_1.png" alt="2020-08-27_18-09_1"></p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-10.png" alt="2020-08-27_18-10"></p>
<p>可学习的上采样方法</p>
<p>​    Transpose convolution (转置卷积)又称<br>• Fractionally-strided convolution (分数步长卷积)<br>• Deconvolution (反卷积)</p>
<p>应该是有很多方法（ 比如是否采用 padding 或者 strides 的方法）</p>
<p>感觉这里自己学习的很模糊：</p>
<ol>
<li><p>不知道各种padding和strides带来什么影响</p>
</li>
<li><p>而且卷积计算过程不是很明确—应该是有多种卷积类型的</p>
<p>​    而且反卷积和卷积的卷积核之间的关系是？？</p>
<p>​    个人理解是 反卷积目的就是提高分辨率：一定是将 小-&gt;大（ 可以采用 padding 或者 较大的卷积核&lt;比输出的卷积的尺寸大些&gt;；或者strides&lt;即填充一些0&gt;</p>
</li>
</ol>
<h6 id="Dilated-Convolutions-膨胀卷积-空洞卷积"><a href="#Dilated-Convolutions-膨胀卷积-空洞卷积" class="headerlink" title="Dilated Convolutions(膨胀卷积/空洞卷积)"></a>Dilated Convolutions(膨胀卷积/空洞卷积)</h6><p>[理解的也不是很透彻的样子，主要是具体操作不是很清楚]</p>
<p>增加网络的感受野，减少特征图像尺寸的损失（没有pooling 层）</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/54149221">https://www.zhihu.com/question/54149221</a></p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-41.png" alt="2020-08-27_18-41"></p>
<p><img src="%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0/2020-08-27_18-41_1.png" alt="2020-08-27_18-41_1"></p>
<p>相关衔接：</p>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/596507">语义分割概述</a></p>
<p><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/752/vote_count">一文概览主要语义分割网络</a></p>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/596507">图像的上采样和下采样</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/25/%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/25/%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/" class="post-title-link" itemprop="url">数学运算</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Veröffentlicht am</span>
      

      <time title="Erstellt: 2020-08-25 07:33:03 / Geändert am: 10:17:22" itemprop="dateCreated datePublished" datetime="2020-08-25T07:33:03+08:00">2020-08-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">in</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          . 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h2><h3 id="1-element-cal"><a href="#1-element-cal" class="headerlink" title="1. element-cal"></a>1. element-cal</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#汇总</span></span><br><span class="line">torch.add ;  +</span><br><span class="line">torch.sub; -</span><br><span class="line">torch.div ; /</span><br><span class="line">torch.mul; *</span><br><span class="line">a.<span class="built_in">pow</span>(n) ; a**n</span><br><span class="line">a.sqrt()</span><br><span class="line">a.rsqrt()  <span class="comment">#平方根的倒数</span></span><br><span class="line">a.exp()</span><br><span class="line">a.log()</span><br><span class="line">a.log2()</span><br><span class="line">a.log10()</span><br></pre></td></tr></table></figure>





<p>利用重载后的运算符</p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_10-49.png" alt="2020-08-25_10-49"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_10-40.png" alt="2020-08-25_10-40"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_17-58.png" alt="2020-08-25_17-58"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-06.png" alt="2020-08-25_11-06"></p>
<h3 id="2-矩阵的乘除"><a href="#2-矩阵的乘除" class="headerlink" title="2.矩阵的乘除"></a>2.矩阵的乘除</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.mm()<span class="comment">#只适用于2D</span></span><br><span class="line"><span class="meta">@</span></span><br><span class="line">torch.matmul()</span><br></pre></td></tr></table></figure>



<h4 id="2-1-２D矩阵的乘除"><a href="#2-1-２D矩阵的乘除" class="headerlink" title="2.1 ２D矩阵的乘除"></a>2.1 ２D矩阵的乘除</h4><p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_10-49_1.png" alt="2020-08-25_10-49_1"></p>
<h4 id="2-2-多维矩阵的乘除"><a href="#2-2-多维矩阵的乘除" class="headerlink" title="2.2 多维矩阵的乘除"></a>2.2 多维矩阵的乘除</h4><p>实际就是支持多个矩阵对并行相乘[只计算最低的两维数据]</p>
<p><strong>broadcasting</strong>机制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在计算过程中，可以使用broadcasting对矩阵进行扩展，</span></span><br><span class="line"><span class="comment">#可以使用matmul的标准：1. 低维（1,2）数据要满足矩阵乘法的行/列数的要求，</span></span><br><span class="line"><span class="comment">#高维数据要完全匹配或者经过broadcasting机制可以匹配</span></span><br><span class="line"><span class="comment">#一维数据，求点积</span></span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">2</span>])</span><br><span class="line">b= torch.tensor([<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b).shape) <span class="comment">#计算得到标量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#只计算低维数据，高维数据完全匹配</span></span><br><span class="line">a = torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">b = torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">c = torch.matmul(a,b)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br><span class="line"><span class="comment">#broadcasting 机制</span></span><br><span class="line">b = torch.rand(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b).shape)</span><br><span class="line"></span><br><span class="line">b = torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b).shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">b = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b).shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor(6)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([4, 3, 28, 32])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([4, 3, 28, 32])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([4, 3, 28, 32])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([1, 3, 28, 32])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="2-3其他计算"><a href="#2-3其他计算" class="headerlink" title="2.3其他计算"></a>2.3其他计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a.floor()</span><br><span class="line"></span><br><span class="line">a.ceil()</span><br><span class="line"></span><br><span class="line">a.<span class="built_in">round</span>()<span class="comment">#四舍五入</span></span><br><span class="line"></span><br><span class="line">a.trunc()<span class="comment">#取整数部分</span></span><br><span class="line"></span><br><span class="line">a.frac()<span class="comment">#取小数部分</span></span><br><span class="line"></span><br><span class="line">a.clamp(<span class="built_in">min</span>,<span class="built_in">max</span>) <span class="comment">#如果a[i]&lt;min ，a[i] =min ； 如果a[i]&gt;max ，a[i]=max</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p>函数相关实例</p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_18-07.png" alt="2020-08-25_18-07"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">3</span>)*<span class="number">15</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.clamp(<span class="number">5</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[ 0.1522,  8.6243, 10.4471],</span></span><br><span class="line"><span class="string">        [ 2.0610,  8.4947, 10.7278]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[ 5.0000,  8.6243, 10.0000],</span></span><br><span class="line"><span class="string">        [ 5.0000,  8.4947, 10.0000]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h2 id="统计属性"><a href="#统计属性" class="headerlink" title="统计属性"></a>统计属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##相关函数计算</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11_12.png" alt="2020-08-25_11_12"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-19.png" alt="2020-08-25_11-19"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-25.png" alt="2020-08-25_11-25"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-26.png" alt="2020-08-25_11-26"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-54.png" alt="2020-08-25_11-54"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_11-59.png" alt="2020-08-25_11-59"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_12-05.png" alt="2020-08-25_12-05"></p>
<p>附录：</p>
<p>1。在某维度上argmax的理解—感觉自己对抽象几何不是特别理解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>,<span class="number">10</span>)a</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.argmax())</span><br><span class="line"><span class="built_in">print</span>(a.argmax(dim=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(a.argmax(dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 7.0244e-01, -1.8047e-01, -1.3860e+00, -2.7422e-02,  1.0189e-01,</span></span><br><span class="line"><span class="string">         -4.1896e-01,  2.3863e+00, -2.3053e-01, -1.7369e-02,  1.8899e-01],</span></span><br><span class="line"><span class="string">        [ 1.0445e-01, -5.9622e-01, -8.3666e-01, -7.0087e-01, -2.1692e-01,</span></span><br><span class="line"><span class="string">          1.2431e-01, -2.3204e-01, -5.3630e-01, -1.1655e+00,  4.6021e-01],</span></span><br><span class="line"><span class="string">        [-1.1210e+00, -4.4479e-01,  1.0892e+00, -1.1631e+00,  7.8703e-01,</span></span><br><span class="line"><span class="string">         -3.9018e-01, -6.1701e-01,  8.2018e-01,  1.2167e-01, -1.6905e+00],</span></span><br><span class="line"><span class="string">        [-1.4846e+00,  9.5270e-01,  2.2757e-01,  1.5931e+00,  2.9075e-01,</span></span><br><span class="line"><span class="string">          1.3509e+00,  1.5353e+00, -1.1048e+00,  7.0249e-04, -1.1067e-01]])</span></span><br><span class="line"><span class="string">tensor(6)</span></span><br><span class="line"><span class="string">tensor([0, 3, 2, 3, 2, 3, 0, 2, 2, 1])</span></span><br><span class="line"><span class="string">tensor([6, 9, 2, 3])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>2.在某维度上范式的计算的理解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.FloatTensor([<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>,<span class="number">7.</span>,<span class="number">8.</span>,<span class="number">9.</span>,<span class="number">10.</span>])</span><br><span class="line">b = a.view(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b.norm(<span class="number">1</span>,dim=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(b.norm(<span class="number">1</span>,dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 1.,  2.,  3.,  4.,  5.],</span></span><br><span class="line"><span class="string">        [ 6.,  7.,  8.,  9., 10.]])</span></span><br><span class="line"><span class="string">tensor([ 7.,  9., 11., 13., 15.])</span></span><br><span class="line"><span class="string">tensor([15., 40.])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="高阶操作"><a href="#高阶操作" class="headerlink" title="高阶操作"></a>高阶操作</h2><h3 id="where-语句"><a href="#where-语句" class="headerlink" title="where 语句"></a>where 语句</h3><p>代替for循环，可以利用GPU进行加速计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cond = torch.tensor([[<span class="number">0.6769</span>,<span class="number">0.7271</span>],[<span class="number">0.8884</span>,<span class="number">0.4163</span>]])</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">0.</span>,<span class="number">0.</span>],[<span class="number">0.</span>,<span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">b = torch.tensor([[<span class="number">1.</span>,<span class="number">1.</span>],[<span class="number">1.</span>,<span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.where(cond&gt;<span class="number">0.5</span>,a,b))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##同上面等效，可以体会一下语义</span></span><br><span class="line">c =torch.randn(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        <span class="keyword">if</span>(cond[i][j]&gt;<span class="number">0.5</span>):</span><br><span class="line">            c[i][j] = a[i][j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c[i][j] = b[i][j]</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1.]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="gather-语句"><a href="#gather-语句" class="headerlink" title="gather 语句"></a>gather 语句</h3><p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_18-11.png" alt="2020-08-25_18-11"></p>
<h4 id="提出的背景"><a href="#提出的背景" class="headerlink" title="提出的背景"></a>提出的背景</h4><p>可以自行体会含义</p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_17-49.png" alt="2020-08-25_17-49"></p>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_18-10.png" alt="2020-08-25_18-10"></p>
<p><img src="%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/2020-08-25_18-14.png" alt="2020-08-25_18-14"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Vorherige Seite"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Nächste Seite"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sherry Wang</span>
</div>
  <div class="powered-by">Erstellt mit  <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
