<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="记录点滴成长">
<meta property="og:type" content="website">
<meta property="og:title" content="Think World">
<meta property="og:url" content="http://example.com/page/5/index.html">
<meta property="og:site_name" content="Think World">
<meta property="og:description" content="记录点滴成长">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Sherry Wang">
<meta property="article:tag" content="computer vision, DeepLearning ,MachineLearning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/5/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/5/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Think World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Think World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">从个人成长角度来说，从经历中学点什么总是重要的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sherry Wang"
      src="/images/aa.webp">
  <p class="site-author-name" itemprop="name">Sherry Wang</p>
  <div class="site-description" itemprop="description">记录点滴成长</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/02/06/c-11%E5%88%86%E7%AB%A01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/02/06/c-11%E5%88%86%E7%AB%A01/" class="post-title-link" itemprop="url">c++11分章1</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-02-06 20:23:18" itemprop="dateCreated datePublished" datetime="2021-02-06T20:23:18+08:00">2021-02-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:23:31" itemprop="dateModified" datetime="2021-07-25T09:23:31+08:00">2021-07-25</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>主要内容: 引用,指针,数组,动态内存分配</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ul>
<li><p>&lt;类型&gt;&amp;&lt;变量&gt; = &lt;对象或变量&gt;</p>
</li>
<li><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>引用就是变量的另一个别名,对应的变量/对象必须存在</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> x;</span><br><span class="line"><span class="keyword">int</span>&amp; rx = x;</span><br><span class="line"><span class="comment">// int x,&amp;rx = x;</span></span><br></pre></td></tr></table></figure></li>
<li><h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><p>​    1.通过引用所做的读写操作实际上是作用于原变量上的</p>
<p>​    2.引用必须在声明时,开始初始化</p>
<p>​    3.引用一旦初始化,引用的名字不能再指定给其他变量</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a &#123; <span class="number">0</span> &#125;, b &#123; <span class="number">1</span> &#125;;</span><br><span class="line"><span class="keyword">int</span>&amp; r &#123; a &#125;;  <span class="comment">// 引用变量 r 在声明的同时就要初始化，r是a的别名</span></span><br><span class="line"> </span><br><span class="line">r = <span class="number">42</span>;  <span class="comment">// 相当于 a = 42</span></span><br><span class="line">r = b;   <span class="comment">// 相当于 a = b; 执行后 a 的值是1</span></span><br><span class="line">         <span class="comment">// 此处不是让r变成b的引用</span></span><br><span class="line">         <span class="comment">// 引用r声明并初始化后，r就不能再改为其它变量的引用</span></span><br><span class="line"><span class="keyword">int</span>&amp; r2 = a;  <span class="comment">// 继续给a起别名</span></span><br><span class="line"><span class="keyword">int</span>&amp; r3 = r;  <span class="comment">// 声明引用变量 r3，用r初始化</span></span><br><span class="line">              <span class="comment">// 这个相当于 int&amp; r3 = a; 因为 r 是 a 的别名</span></span><br></pre></td></tr></table></figure></li>
<li><h3 id="指针和引用的差异"><a href="#指针和引用的差异" class="headerlink" title="指针和引用的差异"></a>指针和引用的差异</h3><p><strong>指针和引用的应用场景是函数参数传递</strong></p>
<p>void f(int* pa ,int* pb);  void f(int&amp; pa,int&amp; pb);</p>
<p>从软件设计角度来看，使用引用更好，因为调用方不需要额外传参数时进行取地址运算.</p>
<ul>
<li>存取值的方式<ul>
<li>对指针变量需要使用*来读取相应内存的内容</li>
</ul>
</li>
<li>初始化<ul>
<li>指针没有要求</li>
</ul>
</li>
<li>对象或变量的存在性<ul>
<li>引用有要求</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="const和引用"><a href="#const和引用" class="headerlink" title="const和引用"></a>const和引用</h3><p>**const &lt;类型&gt;&amp;**常用于返回值以及参数传递（保证不能修改相应的变量）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//变量的引用</span></span><br><span class="line"><span class="keyword">int</span>    val =<span class="number">100</span>;  </span><br><span class="line"><span class="keyword">int</span> &amp; myval1=var;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> &amp; myval2 = var;</span><br><span class="line">myval1 = <span class="number">300</span>;<span class="comment">//合法，可以通过myval2对var进行修改，此时myval2,myval3,var同时被修改</span></span><br><span class="line">myval2 = <span class="number">100</span>;<span class="comment">//非法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//常量的引用</span></span><br><span class="line"> <span class="keyword">int</span> b=<span class="number">100</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>&amp; aa=b;<span class="comment">//b的引用，可以选择用const的修饰</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>&amp; bb=<span class="number">1</span>;<span class="comment">//这里必须用const修饰</span></span><br><span class="line">aa = <span class="number">10</span>；<span class="comment">//非法</span></span><br><span class="line">bb = <span class="number">20</span>;<span class="comment">//非法</span></span><br></pre></td></tr></table></figure></li>
<li><h3 id="引用在参数传递中的使用"><a href="#引用在参数传递中的使用" class="headerlink" title="引用在参数传递中的使用"></a>引用在参数传递中的使用</h3><p>如果在函数中不修改变量，建议使用 const T&amp;</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> var =<span class="number">100</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> cvar  =<span class="number">200</span>;</span><br><span class="line"><span class="comment">/*形参：</span></span><br><span class="line"><span class="comment">void f(int a1,int &amp;a2, const int&amp; a3) </span></span><br><span class="line"><span class="comment">&#123;  &#125;*/</span> </span><br><span class="line"><span class="built_in">f</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>);<span class="comment">//2和a2 不合法</span></span><br><span class="line"><span class="built_in">f</span>(<span class="number">1</span>,var,var);</span><br><span class="line"></span><br><span class="line"><span class="built_in">f</span>(<span class="number">1</span>,cvar,cvar);<span class="comment">//cvar与a2 不合法</span></span><br><span class="line"><span class="built_in">f</span>(<span class="number">1</span>,var,cvar);</span><br><span class="line"></span><br><span class="line"><span class="built_in">f</span>(<span class="number">1</span>,cvar,var);<span class="comment">//cvar和a2 不合法</span></span><br><span class="line"><span class="built_in">f</span>(var,var,var);</span><br><span class="line"><span class="built_in">f</span>(cvar,var,cvar);</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="二维数组的初始化方式"><a href="#二维数组的初始化方式" class="headerlink" title="二维数组的初始化方式"></a>二维数组的初始化方式</h2><h2 id="空指针和动态内存分配"><a href="#空指针和动态内存分配" class="headerlink" title="空指针和动态内存分配"></a>空指针和动态内存分配</h2><p>1.1 0带来的二义性问题</p>
<ul>
<li>C语言中，空指针使用(void *)0来表示, 有时候，用“NULL”来表示空指针(由编译器决定实现方式，一种可能的实现方式是#define NULL 0)</li>
<li>C++03中，空指针使用“0”来表示。0既是一个常量整数，也是一个常量空指针。</li>
<li><strong>C++11中引入保留字“nullptr”作为空指针</strong></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/02/01/C-%E4%B8%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/02/01/C-%E4%B8%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">C++中字符串总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-02-01 14:57:37" itemprop="dateCreated datePublished" datetime="2021-02-01T14:57:37+08:00">2021-02-01</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 08:44:27" itemprop="dateModified" datetime="2021-07-25T08:44:27+08:00">2021-07-25</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="c-中string，字符指针，字符数组之间的区别"><a href="#c-中string，字符指针，字符数组之间的区别" class="headerlink" title="c++中string，字符指针，字符数组之间的区别"></a>c++中string，字符指针，字符数组之间的区别</h2><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><p>c/c++中设置有常量：在程序运行过程中，其值不能改变的量称为常量</p>
<p>常量分为不同的类型：整型常量(3)，浮点型常量(3.12)，字符型常量(‘a’)，字符串常量(“abc”)</p>
<p>常量一般有两种表现形式：</p>
<ul>
<li>直接常量：直接以值的形式表示的常量称之为直接常量</li>
<li>符号常量：用标识符命名的常量称为符号常量，也就是在直接常量上再取一个名字，方便程序后续维护．习惯用大写字母和下划线来命名<ul>
<li>两种定义方式<ul>
<li>const 类型　符号常量名字＝常量值</li>
<li>#define 符号常量名　常量值</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="字符指针和字符数组的区别"><a href="#字符指针和字符数组的区别" class="headerlink" title="字符指针和字符数组的区别"></a>字符指针和字符数组的区别</h3><p>c/c++中<strong>每个字符串都以字符’/0’作为结尾</strong>．为了节省内存，c/c++把常量字符串放到单独的一个内存区域，当几个指针赋值给相同的常量字符串时，它们实际会指向相同的内存地址．但是用常量内存初始化数组时，情况却有所不同．</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> str1[] = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line"><span class="keyword">char</span> str2[] = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line"><span class="comment">//str1!=str2</span></span><br><span class="line"><span class="comment">//c++会为str1,str2分配两个长度为12个字节的空间，并把&quot;hello world&quot;的内容分别复制到数组中．</span></span><br><span class="line"><span class="comment">//str1,str2是两个不同的字符数组，所以初始地址不同</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span>* str3 = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line"><span class="keyword">char</span>* str4 = <span class="string">&quot;helo world&quot;</span>;</span><br><span class="line"><span class="comment">//str3==str4</span></span><br></pre></td></tr></table></figure>



<p>参考资料</p>
<p>＜剑指offer</p>
<p>&lt; <a target="_blank" rel="noopener" href="http://c.biancheng.net/view/2236.html">http://c.biancheng.net/view/2236.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/20/landmark1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/20/landmark1/" class="post-title-link" itemprop="url">landmark1</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-20 18:31:19" itemprop="dateCreated datePublished" datetime="2020-11-20T18:31:19+08:00">2020-11-20</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 08:46:28" itemprop="dateModified" datetime="2021-07-25T08:46:28+08:00">2021-07-25</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>==本篇主要介绍一些　实验过程中用到的一些基础python知识点==</p>
<h2 id="python面向对象"><a href="#python面向对象" class="headerlink" title="python面向对象"></a>python面向对象</h2><h3 id="命名前缀及其含义"><a href="#命名前缀及其含义" class="headerlink" title="命名前缀及其含义"></a>命名前缀及其含义</h3><ul>
<li><strong>前置单下划线</strong><code>_var</code>：命名约定，用来表示该名称仅在内部使用。一般对Python解释器没有特殊含义（通配符导入除外），只能作为对程序员的提示。</li>
<li><strong>后置单下划线</strong><code>var_</code>：命名约定，用于避免与Python关键字发生命名冲突。</li>
<li><strong>前置双下划线</strong><code>__var</code>：在类环境中使用时会触发名称改写，对Python解释器有特殊含义。</li>
<li><strong>前后双下划线</strong><code>__var__</code>：表示由Python语言定义的特殊方法。在自定义的属性中要避免使用这种命名方式。</li>
<li><strong>单下划线</strong><code>_</code>：有时用作临时或无意义变量的名称（“不关心”）。此外还能表示Python REPL会话中上一个表达式的结果。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/03/pytorch%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/03/pytorch%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" class="post-title-link" itemprop="url">pytorch中的网络结构</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-03 17:32:45" itemprop="dateCreated datePublished" datetime="2020-09-03T17:32:45+08:00">2020-09-03</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 08:51:31" itemprop="dateModified" datetime="2021-07-25T08:51:31+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>==<strong>首先先给出一些python面向对象的一些基础知识</strong>==</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/brucewong0516/article/details/79121179">子类中初始化父类</a>的相关问题</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53927068">pytorch中网络构造的深入理解</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/">官方中文文档</a></p>
<h1 id="pytorch-nn-module"><a href="#pytorch-nn-module" class="headerlink" title="pytorch.nn.module"></a>pytorch.nn.module</h1><h2 id="current-layer"><a href="#current-layer" class="headerlink" title="current layer"></a>current layer</h2><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">api:</a></p>
<h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><h3 id="torch-nn-Sequential"><a href="#torch-nn-Sequential" class="headerlink" title="torch.nn.Sequential"></a><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential">torch.nn.Sequential</a></h3><p>A sequential container.</p>
<p>Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<h3 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h3><p>module 保存所有需要计算的参数</p>
<p><strong>自己在设计网络时，要通过<code>nn.Parameter()</code>加入module的优化器管理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLiner</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__int__</span>(<span class="params">self, inp, outp</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyLinear , self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        self.w = nn.Parameter(torch.randn(outp,inp))<span class="comment">#这时生成参数不需要 requires_grad =True</span></span><br><span class="line">        self.b = nn.Parameter(torch.randn(outp))</span><br><span class="line">        </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x </span>):</span></span><br><span class="line">        x = x@self.w.t() +self.b</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<h3 id="modules"><a href="#modules" class="headerlink" title="modules"></a>modules</h3><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">modules</span></span><br><span class="line"><span class="attribute"></span></span><br><span class="line"><span class="attribute">children</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cuda&#x27;)</span><br><span class="line"><span class="built_in">net</span> = <span class="built_in">Net</span>()</span><br><span class="line"><span class="built_in">net</span>.to(device)</span><br></pre></td></tr></table></figure>

<h3 id="save-and-load"><a href="#save-and-load" class="headerlink" title="save and load"></a>save and load</h3><h4 id="save"><a href="#save" class="headerlink" title="save"></a>save</h4><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">cn</span> = MyNet()</span><br><span class="line"><span class="params">...</span><span class="params">...</span></span><br><span class="line">torch.save(<span class="literal">cn</span>.state_dict(), <span class="string">&quot;your_model_path.pth&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="load"><a href="#load" class="headerlink" title="load"></a>load</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cn = MyNet()</span><br><span class="line"></span><br><span class="line">state_dict = torch.load(<span class="string">&quot;your_model_path.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载训练好的参数</span></span><br><span class="line">cn.load_state_dict(state_dict)</span><br></pre></td></tr></table></figure>



<h3 id="train-test"><a href="#train-test" class="headerlink" title="train/test"></a>train/test</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cn = cn.<span class="built_in">eval</span>()<span class="comment">#转换成测试状态</span></span><br><span class="line">cn = cn.train()<span class="comment">#转换成 训练状态</span></span><br></pre></td></tr></table></figure>

<p>了解基本知识之后，以下从构建自己网络的思路开始讲述</p>
<h1 id="mynet"><a href="#mynet" class="headerlink" title="mynet"></a>mynet</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27825451/article/details/90550890">https://blog.csdn.net/qq_27825451/article/details/90550890</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27825451/article/details/90705328">https://blog.csdn.net/qq_27825451/article/details/90705328</a></p>
<p><strong>貌似是需要自己实验forward过程</strong></p>
<h1 id="常用的网络模块"><a href="#常用的网络模块" class="headerlink" title="常用的网络模块"></a>常用的网络模块</h1><h2 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Flatten</span>(<span class="params">nn.Module </span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        	<span class="built_in">super</span>(Flatten, self ).__init__()</span><br><span class="line">     </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span>.view(<span class="built_in">input</span>.size(<span class="number">0</span>),-<span class="number">1</span>) <span class="comment">#将数据打平作为卷积层和全连接层之间的过渡</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/" class="post-title-link" itemprop="url">pytorch卷积的简单操作</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-03 14:19:39" itemprop="dateCreated datePublished" datetime="2020-09-03T14:19:39+08:00">2020-09-03</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 08:49:23" itemprop="dateModified" datetime="2021-07-25T08:49:23+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p><strong>本文罗列一些简单的api</strong></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#">api官方文档连接</a></p>
<p>首先先补一个知识点：关于torch.nn.Xxxx 和 torch.nn.functional.xxx的 api的区别和使用建议</p>
<p>推荐博客： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/66782101/answer/579393790">https://www.zhihu.com/question/66782101/answer/579393790</a></p>
<p>结论：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PyTorch官方推荐：具有学习参数的（例如，conv2d, linear, batch_norm)采用<span class="keyword">nn</span>.Xxx方式，没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用<span class="keyword">nn</span>.functional.xxx或者<span class="keyword">nn</span>.Xxx方式。但关于dropout，个人强烈推荐使用<span class="keyword">nn</span>.Xxx方式，因为一般情况下只有训练阶段才进行dropout，在<span class="built_in">eval</span>阶段都不会进行dropout。使用<span class="keyword">nn</span>.Xxx方式定义dropout，在调用model.<span class="built_in">eval</span>()之后，model中所有的dropout layer都关闭，但以<span class="keyword">nn</span>.<span class="keyword">function</span>.dropout方式定义dropout，在调用model.<span class="built_in">eval</span>()之后并不能关闭dropout。</span><br><span class="line"></span><br><span class="line">作者：有糖吃可好</span><br><span class="line">链接：http<span class="variable">s:</span>//www.zhihu.<span class="keyword">com</span>/question/<span class="number">66782101</span>/answer/<span class="number">579393790</span></span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>



<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_14-30.png" class title="2020-09-03_14-30">

<h3 id="api介绍（2维卷积）"><a href="#api介绍（2维卷积）" class="headerlink" title="api介绍（2维卷积）"></a>api介绍（2维卷积）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(</span><br><span class="line">     in_channels: <span class="built_in">int</span>,</span><br><span class="line">    out_channels: <span class="built_in">int</span>,</span><br><span class="line">    kernel_size: <span class="type">Union</span>[T, <span class="type">Tuple</span>[T, T]], </span><br><span class="line">    stride: <span class="type">Union</span>[T, <span class="type">Tuple</span>[T, T]] = <span class="number">1</span>, </span><br><span class="line">    padding: <span class="type">Union</span>[T, <span class="type">Tuple</span>[T, T]] = <span class="number">0</span>,</span><br><span class="line">    dilation: <span class="type">Union</span>[T, <span class="type">Tuple</span>[T, T]] = <span class="number">1</span>,<span class="comment"># atrous算法</span></span><br><span class="line">    groups: <span class="built_in">int</span> = <span class="number">1</span>,</span><br><span class="line">    bias: <span class="built_in">bool</span> = <span class="literal">True</span>, </span><br><span class="line">    padding_mode: <span class="built_in">str</span> = <span class="string">&#x27;zeros&#x27;</span>)</span><br></pre></td></tr></table></figure>

<img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_14-54.png" class title="2020-09-03_14-54">

<h3 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment">#conv</span></span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">3</span>,<span class="number">5</span>,kernel_size=<span class="number">3</span>,stride =<span class="number">1</span>,padding=<span class="number">0</span>)<span class="comment">#5个 3×（3×3）size=3,channel=3卷积核</span></span><br><span class="line">x = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)<span class="comment">#表示一张尺寸为28×28的rgb图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#forward()</span></span><br><span class="line">out = conv.forward(x)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment">#torch.Size([1, 5, 26, 26]);一张 channels = 5，size=26*26的image</span></span><br><span class="line"><span class="comment"># weigth &amp;&amp; bias</span></span><br><span class="line"><span class="built_in">print</span>(conv.weight.shape)<span class="comment">#这个就是kernel,卷积核</span></span><br><span class="line"><span class="built_in">print</span>(conv.bias.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h2><p>使用 <code>F.avg_pool2d</code>，<code>F.max_pool2d</code></p>
<img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_15-15_1.png" class title="2020-09-03_15-15_1">

<img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_15-15_2.png" class title="2020-09-03_15-15_2">

<img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_15-15.png" class title="2020-09-03_15-15">



<h2 id="upsampling"><a href="#upsampling" class="headerlink" title="upsampling"></a>upsampling</h2><p><code>F.interpolate</code></p>
<img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_15-38.png" class title="2020-09-03_15-38">





<h4 id="实例代码-1"><a href="#实例代码-1" class="headerlink" title="实例代码"></a>实例代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">3</span>,<span class="number">5</span>,kernel_size=<span class="number">3</span>,stride =<span class="number">1</span>,padding=<span class="number">0</span>)<span class="comment">#5个 3×（3×3）size=3,channel=3卷积核</span></span><br><span class="line">x = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)<span class="comment">#表示一张尺寸为28×28的rgb图像</span></span><br><span class="line">out_c= conv.forward(x)</span><br><span class="line"><span class="built_in">print</span>(out_c.shape)</span><br><span class="line"><span class="comment"># layer =</span></span><br><span class="line"><span class="comment"># out_p = F.avg_pool2d(x,5,stride=2)</span></span><br><span class="line">out_p = F.max_pool2d(x,<span class="number">5</span>,stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(out_p.shape)</span><br><span class="line"></span><br><span class="line">out_up = F.interpolate(out_p,scale_factor=<span class="number">2</span>,mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line"><span class="comment">#scale_factor指出了上采样的尺寸</span></span><br><span class="line"><span class="built_in">print</span>(out_up.shape)</span><br><span class="line"></span><br><span class="line">out_final  = torch.sigmoid(out_up)</span><br><span class="line"><span class="built_in">print</span>(out_final.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([1, 5, 26, 26])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([1, 3, 12, 12])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([1, 3, 24, 24])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.Size([1, 3, 24, 24])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h2 id="BatchNorm操作"><a href="#BatchNorm操作" class="headerlink" title="BatchNorm操作"></a>BatchNorm操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">100</span>,<span class="number">16</span>,<span class="number">28</span>,<span class="number">28</span>)<span class="comment">#表示一张尺寸为28×28的rgb图像</span></span><br><span class="line">x = x.view(<span class="number">100</span>,<span class="number">16</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment">#batch_norminize是针对每个通道进行计算的, input [batch_size ,channel, W*H] ,计算出每个channel上的均值和方差</span></span><br><span class="line">layer = nn.BatchNorm1d(<span class="number">16</span>) <span class="comment">#指出channel size</span></span><br><span class="line">out = layer(x) <span class="comment">#进行norm操作</span></span><br><span class="line"><span class="comment">###可以查看参数</span></span><br><span class="line"><span class="built_in">print</span>(layer.running_mean.shape)<span class="comment">#计算得到的每个channel上的均值</span></span><br><span class="line"><span class="built_in">print</span>(layer.running_var.shape)<span class="comment">#计算得到每个channel上的方差</span></span><br><span class="line"><span class="built_in">print</span>(layer.weight)<span class="comment">#在单个channel上线性变换的w,在baclkword过程中需要训练的参数</span></span><br><span class="line"><span class="built_in">print</span>(layer.bias)<span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([100, 16, 784])</span></span><br><span class="line"><span class="string">torch.Size([16])</span></span><br><span class="line"><span class="string">torch.Size([16])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h2 id="常见的激活函数等"><a href="#常见的激活函数等" class="headerlink" title="常见的激活函数等"></a>常见的激活函数等</h2><h4 id="torch-nn-functional-softmax-input-dim-None"><a href="#torch-nn-functional-softmax-input-dim-None" class="headerlink" title="torch.nn.functional.softmax( input, dim= None)"></a><strong><code>torch.nn.functional.softmax( input, dim= None)</code></strong></h4><img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_15-27.png" class title="2020-09-03_15-27">

<h4 id="torch-nn-functional-relu-input-inplace-False"><a href="#torch-nn-functional-relu-input-inplace-False" class="headerlink" title="torch.nn.functional.relu(input, inplace = False )"></a><strong><code>torch.nn.functional.relu(input, inplace = False )</code></strong></h4><img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_15-32.png" class title="2020-09-03_15-32">

<h4 id="torch-nn-functional-sigmoid"><a href="#torch-nn-functional-sigmoid" class="headerlink" title="torch.nn.functional.sigmoid()"></a><strong><code>torch.nn.functional.sigmoid()</code></strong></h4><img src="/.com//09/03/pytorch%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/2020-09-03_15-26.png" class title="2020-09-03_15-26">








      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/03/%E5%9C%A8GPU%E4%B8%8A%E5%8A%A0%E9%80%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/03/%E5%9C%A8GPU%E4%B8%8A%E5%8A%A0%E9%80%9F/" class="post-title-link" itemprop="url">在GPU上加速</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-03 09:36:14" itemprop="dateCreated datePublished" datetime="2020-09-03T09:36:14+08:00">2020-09-03</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:08:26" itemprop="dateModified" datetime="2021-07-25T09:08:26+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="回顾基本训练过程-MINIST"><a href="#回顾基本训练过程-MINIST" class="headerlink" title="回顾基本训练过程(MINIST)"></a>回顾基本训练过程(MINIST)</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><h4 id="导包、设计训练参数"><a href="#导包、设计训练参数" class="headerlink" title="导包、设计训练参数"></a>导包、设计训练参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">import</span>  torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span>  torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span>  torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">batch_size=<span class="number">200</span></span><br><span class="line">learning_rate=<span class="number">0.01</span></span><br><span class="line">epochs=<span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>下载数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                   transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))对数据进行正则化</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">    ])),</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><h4 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">w1, b1 = torch.randn(<span class="number">200</span>, <span class="number">784</span>, requires_grad=<span class="literal">True</span>),\</span><br><span class="line">         torch.zeros(<span class="number">200</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2, b2 = torch.randn(<span class="number">200</span>, <span class="number">200</span>, requires_grad=<span class="literal">True</span>),\</span><br><span class="line">         torch.zeros(<span class="number">200</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w3, b3 = torch.randn(<span class="number">10</span>, <span class="number">200</span>, requires_grad=<span class="literal">True</span>),\</span><br><span class="line">         torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)<span class="comment">#class_size = 10，最后分为10类</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#利用何恺明大佬的初始化方法</span></span><br><span class="line">torch.nn.init.kaiming_normal_(w1)</span><br><span class="line">torch.nn.init.kaiming_normal_(w2)</span><br><span class="line">torch.nn.init.kaiming_normal_(w3)</span><br></pre></td></tr></table></figure>



<h4 id="搭建网络"><a href="#搭建网络" class="headerlink" title="搭建网络"></a>搭建网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fforward</span>():</span></span><br><span class="line">    x = x@w1 + b1</span><br><span class="line">    x =F. relu(x)</span><br><span class="line">    x = x@w2 + b2</span><br><span class="line">    x = F.relu(x)</span><br><span class="line">    x = x@w3+ b3</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h4 id="定义加速器和loss函数"><a href="#定义加速器和loss函数" class="headerlink" title="定义加速器和loss函数"></a>定义加速器和loss函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD([w1,b1,w2,b2,w3,b3],lr = learning_rate)<span class="comment">#传入需要计算梯度的参数和learning_rate</span></span><br><span class="line">criteon = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>

<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">len</span>(epochs):</span><br><span class="line">	<span class="keyword">for</span> batch_idx , (data,target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        data= data.view(-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>)<span class="comment">#view(-1,28*28)中-1表示不确定/不关心的数，只要求dim=0的size是28×28就行</span></span><br><span class="line">        logits = fforward(data)</span><br><span class="line">        </span><br><span class="line">        loss = criteon(logits,target)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()<span class="comment">#所有参数上一轮的梯度信息都清除</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()<span class="comment">#用梯度更新参数</span></span><br><span class="line">        <span class="keyword">if</span>( batch_idx %<span class="number">100</span> ==<span class="number">0</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;train epoch :&#123;&#125;, batch:&#123;&#125; ,loss&#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,batch_idx,loss.item() ))</span><br><span class="line">            <span class="comment">#torch.item() 表示如果多维的torch中只包含一个元素，取出来</span></span><br></pre></td></tr></table></figure>

<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">test_loss</span> = <span class="number">0</span></span><br><span class="line"><span class="title">correct</span> =<span class="number">0</span></span><br><span class="line"><span class="title">for</span> <span class="class"><span class="keyword">data</span>,target in test_loader :</span></span><br><span class="line">	<span class="class"><span class="keyword">data</span> = <span class="keyword">data</span>.view(-1,28*28)</span></span><br><span class="line">	logits = fforward(<span class="class"><span class="keyword">data</span>)#得到预测值</span></span><br><span class="line">	test_loss += criteon(logits,target).item()</span><br><span class="line">	</span><br><span class="line">	pre = logits.argmax(dim=<span class="number">1</span>)</span><br><span class="line">	correct = pred.eq(target.<span class="class"><span class="keyword">data</span>)</span></span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<h2 id="GPU加速"><a href="#GPU加速" class="headerlink" title="GPU加速"></a>GPU加速</h2><p>比较好的博客：</p>
<p><a target="_blank" rel="noopener" href="http://www.feiguyunai.com/index.php/2019/04/30/python-ml-25-pytorch-gpu/">http://www.feiguyunai.com/index.php/2019/04/30/python-ml-25-pytorch-gpu/</a></p>
<p>[自己的实验环境应该是单GPU环境]</p>
<h3 id="创建一个GPU实例"><a href="#创建一个GPU实例" class="headerlink" title="创建一个GPU实例"></a>创建一个GPU实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda: 0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span><span class="string">&#x27;cpu&#x27;</span>)<span class="comment">#首先要先判断是否有可用的GPU</span></span><br><span class="line"><span class="comment">#&#x27;0&#x27;表示GPU的编号</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="把需要计算的量加载到设备上"><a href="#把需要计算的量加载到设备上" class="headerlink" title="把需要计算的量加载到设备上"></a>把需要计算的量加载到设备上</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = MLP().to(deivce) </span><br><span class="line">optimizer = optim.SGD(net.parameters() , lr = learning_rate)</span><br><span class="line">criteon = nn.CrossEntropyLoss().to(device)<span class="comment">#to(device) ,如果数据类型是model，则返回类型是 cpu上数据的引用</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">	<span class="keyword">for</span> batch_idx , (data,target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">		data = data.view(-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">		data, target = data.to(device),target.cuda()<span class="comment">#如果数据类型是 tensor，返回类型是cpu上数据的copy()</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/30/BN_dropout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/30/BN_dropout/" class="post-title-link" itemprop="url">batch normalize</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-30 11:38:23" itemprop="dateCreated datePublished" datetime="2020-08-30T11:38:23+08:00">2020-08-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:24:49" itemprop="dateModified" datetime="2021-07-25T09:24:49+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" itemprop="url" rel="index"><span itemprop="name">基础知识</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h1><p>参考文献：</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Batch_normalization">wiki</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/guoyaohua/p/8724433.html">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></p>
<p>问题背景：</p>
<p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98">梯度消失</a>与<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32154263">梯度爆炸</a>的问题</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_25737169/article/details/78847691">一般解决方法汇总</a></p>
<p>drop_out:这里可以参考吴恩达老师的deep learning课件</p>
<h3 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h3><p>内部协变量移位现象(Internal Covariate shift)</p>
<p>机器学习中有一个比较重要的假设：独立同分布，就是假设训练数据和测试数据是同时满足相同分布；</p>
<p>​    在网络的训练阶段，由于前几层的参数发生变化，因此当前层的输入分布也会相应变化，因此当前层需要不断调整以适应新的分布。对于较深的网络，此问题尤为严重，因为较浅的隐藏层的细微变化将在它们在网络中传播时被放大，从而导致较深的隐藏层发生显着变化。因此，提出了批量标准化的方法，以减少这些不必要的偏移，以加快训练速度并生成更可靠的模型。</p>
<p>​    NP的基本思想就是让每个隐层节点的激活输入分布固定下来（一般是放置在激活层之前）;类似于白化操作：对深层神经网络的每个隐层神经元的激活值做简化版本的白化操作.</p>
<p>​    同时，我们可以发现BN还有其他的用途</p>
<p>​        1.会使网络使用更高的学习速率而不会出现梯度消失或者梯度爆炸</p>
<p>​        2.似乎存在正则化效果从而改善了归一化属性，可以不使用drop out　</p>
<h3 id="本质思想"><a href="#本质思想" class="headerlink" title="本质思想"></a>本质思想</h3><img src="/.com//08/30/BN_dropout/2020-08-30_18-13.png" class title="2020-08-30_18-13">

<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><img src="/.com//08/30/BN_dropout/2020-08-30_18-04.png" class title="2020-08-30_18-04">

<img src="/.com//08/30/BN_dropout/2020-08-30_18-12.png" class title="2020-08-30_18-12">

<img src="/.com//08/30/BN_dropout/2020-08-30_18-06.png" class title="2020-08-30_18-06">

<p>①不仅仅极大提升了训练速度，收敛过程大大加快；</p>
<p>②还能增加分类效果，一种解释是这是类似于 Dropout 的一种防止过拟合的正则化表达方式，所以不用 Dropout 也能达到相当的效果；</p>
<p>③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/29/ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/29/ResNet/" class="post-title-link" itemprop="url">ResNet</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-29 20:22:22" itemprop="dateCreated datePublished" datetime="2020-08-29T20:22:22+08:00">2020-08-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 08:53:00" itemprop="dateModified" datetime="2021-07-25T08:53:00+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><img src="/.com//08/29/ResNet/2020-08-30_10-40.png" class title="2020-08-30_10-40">

<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>在原论文中，作者不断加深网络结构来研究随着网络层次的加深，残差模块的表现</p>
<p>所以存在 ResNet-34, ResNet-50, ResNet-101, ResNet-152</p>
<p>这里只以ResNet-34为主</p>
<img src="/.com//08/29/ResNet/2020-08-29_20-35.png" class title="2020-08-29_20-35">

<p>可以看到整个网络分为两部分Plain Network 和Residual Network</p>
<h3 id="Plain-Network"><a href="#Plain-Network" class="headerlink" title="Plain Network"></a>Plain Network</h3><p>参考最左侧VGG网络的框架，都是采用３×３的卷积核</p>
<p> (i) for the same output feature map size, the layers have the same number of filters; 而且stride=1, padding =1,</p>
<p>(ii) if the feature map size is halved, the number of filters is doubled </p>
<p>so as to preserve the time complexity per layer.[池化层使用步长为２,然后３×３的卷积核]</p>
<p>可以看出来网络比较工整</p>
<h3 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h3><h4 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h4><p>最右侧的网络即是残差网络，在plainNet的基础上增加了Residual模块</p>
<img src="/.com//08/29/ResNet/2020-08-30_07-17.png" class title="2020-08-30_07-17">

<p>实弧线，表示这里shortcuts是正常的恒等映射<br>$$<br>Y = F(X,{w_i}) + X<br>$$<br>虚弧线，表示这里恒等映射(identity mapping / shortcuts)要进行增加维度；原文中提到了两种方式</p>
<p>1.　with extra zero entries padded for increasing dimensions;我理解的是仅仅是用０来填充多余的维度<br>2.　采用 projection shortcut (采用１*1 的卷积核)，公式为,$W_s$用来调整维度和尺寸。</p>
<p>$$<br>Y = F(X,{w_i}) + W_sX<br>$$</p>
<p>对于以上两种方式，stride=2</p>
<h4 id="残差结构的原理"><a href="#残差结构的原理" class="headerlink" title="残差结构的原理"></a>残差结构的原理</h4><p>＝＝自己的数学比较鸡肋，感觉应该是重点理解残差结构（而不仅仅是保证梯度不会很小）</p>
<p>本片论文的introduction中提到了，增加学习网络的深度确实有利于提高神经网络的能，但是会因为一些意料之外的原因导致出现在较深网络层次中准确率降低的情况；</p>
<p>其中比较重要的影响因素是　较深的网络会出现梯度消失和梯度爆炸的问题</p>
<p>1.梯度消失：导致深层的梯度不能传递到浅层,不能更新参数</p>
<p>2.梯度爆炸的：导致网络不稳定</p>
<p>有一些其他的解决方法　比如　normalized initialization, intermediate normalization layers[NP]</p>
<p>本文采用使用　残差结构：在深层网路，训练 F(x)的梯度趋向０，保证不出现梯度下降和梯度爆炸的现象</p>
<p>利用公式，我们可以简单验证一下梯度更新的情况</p>
<img src="/.com//08/29/ResNet/2020-08-30_07-17.png" class title="2020-08-30_07-17">
<p>$$<br>Y = F(X,{ W_i }) +X\<br>F = W_2\sigma(W_1 X) \<br>$$<br>这里忽略了　偏移量$b_i$, $\sigma$ 表示relu函数</p>
<p>实际上我们训练的就是$F(x)$,而且在网络结构的深层我们趋向于将它的梯度训练为０</p>
<p>我们设损失函数为$\Epsilon$</p>
<p>$$<br>\<br>X_{l+1} = X_{l} + F(X_l,{ W_i })\即上一个残差模块的输出作为下一个残差模块的输入\<br>X_{l+2 } = X_{l+1}+F(X_{l+1},{W_I}) = X_l +  F(X_l,{ W_i })+F(X_{l+1},{W_I})\<br>…\<br>X_L = X_l + \sum_{i =l}^{L-1} F(X_i,W_i)\<br> \　X_l表示第l个残差模块的输入，X_L表示第L个残差模块的输入;满足l&lt;L\<br>$$</p>
<p>$$<br>\frac{\partial \Epsilon}{\partial X_l }  = \frac{\partial \Epsilon }{\partial X_L}\times \frac {\part X_L} {X_l}= \frac{\partial \Epsilon }{\partial X_L}(1+\frac{\part}{\part X_l}(\sum_{i=l}^{L-1} F(X_i,W_i)))<br>$$<br>注：这里恒等映射没有经过１×１的卷积运算</p>
<p>可以看出来，较为深层的网络，我们也可以保证梯度不为消失，可以较为无损地传播梯度</p>
<p>其他优秀博主的意见：[可以换个角度思考一下]</p>
<img src="/.com//08/29/ResNet/2020-08-30_10-26.png" class title="2020-08-30_10-26">

<h3 id="残差网络的其他设计"><a href="#残差网络的其他设计" class="headerlink" title="残差网络的其他设计"></a>残差网络的其他设计</h3><p>1.由浅层的网络到深层的网络，将两层（3*3)转换成３层（１×１，３×３，１×１），减少参数，便于训练(１×１卷积，用来调整维度的；第１个降低维度到原来的1/2，第二个再恢复原来的维度)</p>
<img src="/.com//08/29/ResNet/2020-08-30_07-30_1.png" class title="2020-08-30_07-30_1">

<img src="/.com//08/29/ResNet/2020-08-30_07-30.png" class title="2020-08-30_07-30">

<p>２.关于涉及维度变换的残差模块的恒等映射，作者比较了三种实现</p>
<img src="/.com//08/29/ResNet/2020-08-30_10-35.png" class title="2020-08-30_10-35">

<p>比较结果</p>
<img src="/.com//08/29/ResNet/2020-08-30_10-38.png" class title="2020-08-30_10-38">

<p>但是考虑到训练的成本，建议选择方案A，即Zero-padding　shortcut,毕竟重点是残差结构</p>
<h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">（1）使用color augmentation做数据扩增</span><br><span class="line">（2）在每个卷积层之后，激活函数之前使用batch normalization (BN)</span><br><span class="line">（3）SGD作优化，weight decay =0.0001，<span class="attribute">momentum</span>=0.9</span><br><span class="line">（4）learning <span class="attribute">rate</span>=0.1,当错误率停滞时除以10</span><br><span class="line">（5）不使用dropout  </span><br></pre></td></tr></table></figure>

<p>参考一些博客，博主们都提到了训练过程中　BN位置的设置等问题</p>
<p>参考博客：</p>
<p>实验：<a target="_blank" rel="noopener" href="https://juejin.im/entry/6844903564590972936">https://juejin.im/entry/6844903564590972936</a></p>
<p>原理：<a target="_blank" rel="noopener" href="https://www.itread01.com/content/1544868722.html">https://www.itread01.com/content/1544868722.html</a></p>
<p>​        <a target="_blank" rel="noopener" href="https://blog.csdn.net/u014296502/article/details/80438616">https://blog.csdn.net/u014296502/article/details/80438616</a></p>
<p>综述：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31852747">https://zhuanlan.zhihu.com/p/31852747</a></p>
<p>原文地址：</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/27/Faster%20R-CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/27/Faster%20R-CNN/" class="post-title-link" itemprop="url">Faster R-CNN 论文笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-27 23:23:29" itemprop="dateCreated datePublished" datetime="2020-08-27T23:23:29+08:00">2020-08-27</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:24:26" itemprop="dateModified" datetime="2021-07-25T09:24:26+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">目标检测</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>主要<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31426458">参考博文</a></p>
<p><a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">原文地址</a></p>
<p>在原论文的摘要中提出：</p>
<p>在之前SPP-Net 和Fast R-CNN提出依靠region proposal algorithms去预测物体的位置会减少很多计算量，但是这也暴露了区域提议计算的瓶颈</p>
<p>本文关键提出了RPN(region proposal network)，使用卷积层，减少参数的数量，有较快的速度和较高的准确率</p>
<p>RPN ==shares full-image convolutional features== with the detection network, thus enabling nearly ==cost-free== region proposals. </p>
<p>​    An RPN is a fully-convolutional network that simultaneously ==predicts object bounds and objectness scores== at each position. RPNs are trained end-to-end to generate highquality region proposals, which are used by Fast R-CNN for detection.     </p>
<p>softmax交叉熵损失函数：<a target="_blank" rel="noopener" href="https://blog.csdn.net/chaipp0607/article/details/73392175">https://blog.csdn.net/chaipp0607/article/details/73392175</a></p>
<h2 id="网络框架"><a href="#网络框架" class="headerlink" title="网络框架"></a>网络框架</h2><p>![2020-08-28_16-35](Faster R-CNN/2020-08-28_16-35.png)</p>
<p>主要分为四部分：</p>
<ol>
<li><p>Conv layers:　作为一种CNN网络目标检测方法，使用conv+pooling+relu来提取feature maps，被之后的RPN 和全连接层共享</p>
</li>
<li><p>RPN(region proposal network )</p>
<p>​    ==用于生成region proposals==，该层主要工作：</p>
<p>​        a. 通过softmax判断anchors属于positive , negative，</p>
<p>​        b. 再利用bounding box regression 修正anchors获得较为准确的            proposals</p>
</li>
<li><p>roi pooling</p>
<p>​    该层收集输入的feature maps和proposals，综合这些信息提区proposal feature maps，</p>
</li>
<li><p>classification </p>
<p>​    利用 proposal feature maps 计算 proposal 的类别，同时再次 bounding box regression 获得检测框最终的精确位置</p>
</li>
</ol>
<p>下图为python的VGG16模型中的faster_rcnn_test.pt网络结构</p>
<p>![2020-08-28_16-34](Faster R-CNN/2020-08-28_16-34.png)</p>
<h3 id="Conv-layers"><a href="#Conv-layers" class="headerlink" title="Conv layers"></a>Conv layers</h3><p>在Conv layers中</p>
<ol>
<li>所有的conv层都是　kernel_size = 3, pad =1 ,stride =1</li>
<li>所有的pooling层都是　kernel_size =2, pad =0, stride =2</li>
</ol>
<p>导致　Conv layers中conv层不改变输入输出矩阵的大小，只有在pooling层中M×N的矩阵变成(M/2)×(N/2)的大小。所以从Conv layers输出的矩阵的尺寸为M×N</p>
<p>目的是为了在ROI Pooling的输入层中proposal（M×N)与 feature maps尺寸一致</p>
<h3 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h3><p>这一层的主要任务： 获取有效的proposals，完成目标定位</p>
<p>![2020-08-28_17-42](Faster R-CNN/2020-08-28_17-42.png)</p>
<p>![2020-08-28_18-50](Faster R-CNN/2020-08-28_18-50.png)</p>
<p><strong>主要流程：生成 anchors -&gt; softmax 分类器提取 positvie anchors -&gt; bbox reg 回归 positive anchors -&gt; Proposal Layer 生成 proposals</strong></p>
<p><strong>这是主要的原理就是：利用3×３的卷积核的中心作为anchor的中心，在每个点设置9个anchor作为候选区，之后再有cnn来判断anchor是negitive or positive anchor</strong>，目前这里只是二分类,而且后面还有 2 次 bounding box regression 可以修正检测框位置</p>
<h4 id="1-生成anchors"><a href="#1-生成anchors" class="headerlink" title="1.生成anchors"></a>1.生成anchors</h4><p>略(思路是比较简单的,可以参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31426458">博客)</a></p>
<h4 id="2-softmax分类器提取positive-anchors"><a href="#2-softmax分类器提取positive-anchors" class="headerlink" title="2.softmax分类器提取positive anchors"></a>2.softmax分类器提取positive anchors</h4><p>输入：anchors，输出:rpn_cls_score</p>
<h4 id="3-bbox-回归"><a href="#3-bbox-回归" class="headerlink" title="3.bbox 回归"></a>3.bbox 回归</h4><h6 id="bounding-box-regression原理"><a href="#bounding-box-regression原理" class="headerlink" title="bounding box regression原理"></a>bounding box regression原理</h6><p>大致思想：我们目标是像通过　bounding box regression，对anchors进行调整，让他接近GT(但是我们anchor的选取按照固定的方法，没有结合GT的位置信息)；所以我们学习的是anchors和GT之间变换(每张图片的GT是固定的，而且anchors的选取也是固定的)</p>
<p>​    所以本文学习的是预测的窗口(anchor)=&gt;GT窗口之间的一种变换（先平移后进行缩放）(如果相差比较小，即positive可以看作是线性回归模型)</p>
<p>​    并设计相应的loss函数对其进行约束</p>
<p><strong>原理</strong></p>
<p>![2020-08-28_18-29](Faster R-CNN/2020-08-28_18-29.png)</p>
<p>![2020-08-28_18-35](Faster R-CNN/2020-08-28_18-35.png)</p>
<p>![2020-08-28_18-36](Faster R-CNN/2020-08-28_18-36.png)</p>
<p>![2020-08-28_21-22](Faster R-CNN/2020-08-28_21-22.png)</p>
<p>而在原论文中是这样子记录的</p>
<p>![](Faster R-CNN/2020-08-28_21-21.png)</p>
<p>![2020-08-28_20-51](Faster R-CNN/2020-08-28_20-51.png)</p>
<p>==<strong>在bbox-regression 中要训练的参数就是，我们需要预测的box(x,y,w,h)</strong>==</p>
<h4 id="的4-proposal-layer-生成-proposals"><a href="#的4-proposal-layer-生成-proposals" class="headerlink" title="的4. proposal layer 生成 proposals"></a>的4. proposal layer 生成 proposals</h4><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">根据bbox回归得到的[dx(A), dy(A), dw(A) ,dh(A)]对所有的anchors进行修正和微调</span><br><span class="line">按照输入的<span class="keyword">positive</span> softmax scores　由大到小排序anchors，提取前pre_nms_topN，即选择较好的修正后的　<span class="keyword">positive</span> anchors</span><br><span class="line">限定超出图像边界的 <span class="keyword">positive</span> anchors 为图像边界，防止后续 roi pooling 时 proposal 超出图像边界</span><br><span class="line">剔除尺寸非常小的 <span class="keyword">positive</span> anchors</span><br><span class="line">对剩余的 <span class="keyword">positive</span> anchors 进行 NMS（nonmaximum suppression）</span><br></pre></td></tr></table></figure>

<p>注意由于第三步生成的proposal要和原图像进行对比，所以它的尺寸是M*N</p>
<h6 id="NMS：非极大值抑制"><a href="#NMS：非极大值抑制" class="headerlink" title="NMS：非极大值抑制"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37489043">NMS：非极大值抑制</a></h6><p>![2020-08-28_19-07](Faster R-CNN/2020-08-28_19-07.png)</p>
<p>消除冗余的边界框</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">根据置信度得分进行排序</span><br><span class="line">选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除</span><br><span class="line">计算所有边界框的面积</span><br><span class="line">计算置信度最高的边界框与其它候选框的IoU。</span><br><span class="line">删除IoU大于阈值的边界框</span><br><span class="line">重复上述过程，直至边界框列表为空</span><br></pre></td></tr></table></figure>

<p>​    </p>
<h3 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h3><p>是一个简单的SPP-Net, 属于卷积层和连接层之间的过渡层，将大小不一的proposals变成固定大小(之后classifier　模块是利用全连接层进行分类，所以需要固定大小)</p>
<p>存在其他方法：crop，warp,但是他们会破坏图像原有信息</p>
<p>![2020-08-28_21-44](Faster R-CNN/2020-08-28_21-44.png)</p>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>![2020-08-28_19-14](Faster R-CNN/2020-08-28_19-14.png)</p>
<h2 id="loss函数"><a href="#loss函数" class="headerlink" title="loss函数"></a>loss函数</h2><p>在训练过程和bbox回归中都有涉及</p>
<p>![2020-08-28_20-51](Faster R-CNN/2020-08-28_20-51.png)</p>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><h3 id="交替训练过程"><a href="#交替训练过程" class="headerlink" title="交替训练过程"></a>交替训练过程</h3><p>![2020-08-28_19-38](Faster R-CNN/2020-08-28_19-38.png)</p>
<p>![2020-08-28_19-40](Faster R-CNN/2020-08-28_19-40.png)</p>
<p><strong>公共卷积层的不是已经训练好了么？？</strong></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">参数说明：</span><br><span class="line">rpn_cls_prob_reshape:　positive vs negative anchors 分类器结果 </span><br><span class="line"></span><br><span class="line"> rpn_bbox_pred:　对应的 bbox reg 的  变换量[<span class="built_in">dx</span>(A,<span class="built_in">dy</span>(A),<span class="built_in">dw</span>(A),<span class="built_in">dh</span>(A))]</span><br><span class="line"> </span><br><span class="line">im_info:  对于一副任意大小 PxQ 图像，传入 Faster RCNN 前首先 reshape 到固定 MxN，im_info=[M, N, scale_factor] 则保存了此次缩放的所有信息。这样做的是为了方便之后训练网络[历史遗留问题]</span><br></pre></td></tr></table></figure>

<p>![2020-08-28_20-08](Faster R-CNN/2020-08-28_20-08.png)</p>
<p>![2020-08-28_20-26](Faster R-CNN/2020-08-28_20-26.png)</p>
<h4 id="rpn-train1"><a href="#rpn-train1" class="headerlink" title="rpn-train1"></a>rpn-train1</h4><p>![2020-08-28_20-25](Faster R-CNN/2020-08-28_20-25.png)</p>
<p>![2020-08-28_20-29](Faster R-CNN/2020-08-28_20-29.png)</p>
<p>![2020-08-28_20-02](Faster R-CNN/2020-08-28_20-02.png)</p>
<h4 id="RPN-test"><a href="#RPN-test" class="headerlink" title="RPN-test"></a>RPN-test</h4><p>![2020-08-28_20-11](Faster R-CNN/2020-08-28_20-11.png)</p>
<p>![2020-08-28_20-02_1](Faster R-CNN/2020-08-28_20-02_1.png)</p>
<h4 id="faster-rcnn"><a href="#faster-rcnn" class="headerlink" title="faster rcnn"></a>faster rcnn</h4><p>![2020-08-28_20-10](Faster R-CNN/2020-08-28_20-10.png)</p>
<p>![2020-08-28_20-15](Faster R-CNN/2020-08-28_20-15.png)</p>
<p>第二轮训练方式与第一轮大同小异。</p>
<h3 id="端到端方式"><a href="#端到端方式" class="headerlink" title="端到端方式"></a>端到端方式</h3><h2 id="仍然存在的问题"><a href="#仍然存在的问题" class="headerlink" title="仍然存在的问题"></a>仍然存在的问题</h2><p>１。读不懂代码，意味不知道网络需要什么样子的输入以及标注,而且看论文只能看个大概，不是特别清楚得了解每层网络之间的尺寸，维度等等的变换</p>
<p>２.不是特别明白训练的过程</p>
<p>３.不是特别明白　这里关于faster-rcnn　和　rpn　之间的参数共享的问题</p>
<ol start="4">
<li>要训练的参数都有哪些？？　反向传播，参数更新的过程</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/27/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aa.webp">
      <meta itemprop="name" content="Sherry Wang">
      <meta itemprop="description" content="记录点滴成长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Think World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/27/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/" class="post-title-link" itemprop="url">目标检测综述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-27 23:23:17" itemprop="dateCreated datePublished" datetime="2020-08-27T23:23:17+08:00">2020-08-27</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-25 09:13:30" itemprop="dateModified" datetime="2021-07-25T09:13:30+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">目标检测</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="目标检测summary"><a href="#目标检测summary" class="headerlink" title="目标检测summary"></a>目标检测summary</h2><p>本文在 主要描述目标检测的问题以及解决方法</p>
<img src="/.com//08/27/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/20180502184712966.jpg" class width="20180502184712966">

<p>目标检测的任务本质只有两个问题：图像识别，定位</p>
<p>这里着重介绍一下目标检测的评价方式</p>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><img src="/.com//08/27/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/2020-08-30_19-20.png" class title="2020-08-30_19-20">

<img src="/.com//08/27/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/2020-08-30_19-20_1.png" class title="2020-08-30_19-20_1">

<img src="/.com//08/27/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/2020-08-30_19-20_2.png" class title="2020-08-30_19-20_2">

<p><strong>数据集不平衡时， precision和recall等指标就不具备参考性</strong>，所以引出了AP<br>，它的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56961620">相关介绍用例子介绍比较合适</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sherry Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
